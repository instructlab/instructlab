# Troubleshooting

This document is for commonly found problems and their solutions when using `lab`. There is also a section that includes information on fine-tuning and troubleshooting your model to optimize the quality of its responses. 

## `lab` troubleshooting

### Long `lab generate` on MacOS issue
If you notice `lab generate` being very slow for you, several hours or more.
Check [this discussion](https://github.com/ggerganov/llama.cpp/discussions/2182#discussioncomment-7698315)
which is suggesting to tweak the GPU limit, which MacOS assigns. By default it's
around 60%-70% of your total RAM available, which is expressed as 0:
```
$ sudo sysctl iogpu.wired_limit_mb
iogpu.wired_limit_mb = 0
```
You can set it to any number, although it's advisable to leave 4-6GB for MacOS.

On a M1 with 16GB, it was tested that `lab generate` with that limit bumped to
12GB was able to finish in less than an hour.
```
sudo sysctl iogpu.wired_limit_mb=12288
```
Once done, make sure to reset the limit back to 0, which resets it to default.

**Note:** This value will reset to defaults after machine reboot.

## `gh` troubleshooting

This page has some troubleshooting techniques if you hit a github cli `gh` error described below.

### gh error during `lab download`
Some people are hitting a `gh` error while running the `lab download` step.

If you see this `error invoking gh command` there is a `quick fix` and `longer fix`

```
(venv) $ lab download
Make sure the local environment has the `gh` cli: https://cli.github.com
Downloading models from https://github.com/instruct-lab/cli.git@v0.2.0 to models...
Downloading models failed with the following error: error invoking `gh` command: Command '['gh', 'release', 'download', 'v0.2.0',
 '--repo', 'https://github.com/instruct-lab/cli.git', '--dir', 'models', '--pattern', 'ggml-merlinite-7b-0302-Q4_K_M.*']' returned non-zero exit status 4.
it is time to look at your gh settings - and make sure you can run 'gh auth login'
```

#### Quick fix
Run `gh auth login`

```
gh auth login
```

**Note:** On macOS, users can add their SSH keys to their apple-keychain by running:
```
ssh-add --apple-use-keychain ~/.ssh/[your-private-key]
```

#### Longer fix
If after `gh auth login` you are still hitting issue(s) try the following:

Up front
- git uses your SSH public key to allow https git clone (see below)
- gh uses a token

If you need to check/create a new `gh` token
- log in to your `https://github.com/` account
- find `developer settings` bottom of left hand column (sometimes tough to find)
- Go to settings -> Developer Settings -> new personal access (classic) token

Create new token checking off:
- [x] repo
- admin:org [x] read:org

Copy your new token to `mytoken.txt` which gets used below.

More info on `gh_auth_login` is at [gh_auth_login](https://cli.github.com/manual/gh_auth_login)
```
Authenticate against github.com by reading the token from a file
gh auth login --with-token < mytoken.txt
```

#### Raw steps

**NOTE:** logout may not be necessary

```
$ gh auth logout
```

Always refer to [README.md](README.md) for most/latest commands used during Installing lab:

```
gh auth login --with-token < ~/Documents/mytoken.txt
git clone https://github.com/instruct-lab/cli.git
cd cli
mkdir instruct-lab
cd instruct-lab
python3 -m venv venv
source venv/bin/activate
pip install git+ssh://git@github.com/instruct-lab/cli.git
pip install --upgrade pip
```

```
$ lab init
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment:
Path to taxonomy repo [taxonomy]: <ENTER>
`taxonomy` seems to not exists or is empty. Should I clone git@github.com:instruct-lab/taxonomy.git for you? [y/N]: y
Cloning git@github.com:instruct-lab/taxonomy.git...
Path to your model [models/ggml-merlinite-7b-0302-Q4_K_M.gguf]: <ENTER>
Generating `config.yaml` in the current directory...
Initialization completed successfully, you're ready to start using `lab`. Enjoy!
```

```
$ lab download
Make sure the local environment has the `gh` cli: https://cli.github.com
Downloading models from https://github.com/instruct-lab/cli.git@v0.2.0 to models...
```

Ensure a model is downloaded.

```
$ ls models
ggml-merlinite-7b-0302-Q4_K_M.gguf
```

## Model fine tuning and response optimization 

If you are looking to optimize the quality of the outputs generated by the model, there are a number of steps and parameters at various stages of the CLI workflow that you could consider leveraging. Some of these steps are discussed in following sections. 

It is important to note that improved response quality will come at a cost, in the form of either increased compute requirements, or increased time requirement, or both. The described steps will provide you with the best chance of improving the quality of your model's responses, but cannot guarantee an improvement in response quality. 

### Skill compositon

Composing and contributing effective and impactful skills is an iterative process. The typical workflow looks something like this:
- compose skill examples
- run `lab generate` 
- examine generated examples based on supplied skill (found in the `generated` folder)
- If generated examples are not satisfactory in quality:
  - edit the skill examples 
- repeat the process until you are satisfied with the generated data

#### Steps that can be taken in order to improve the skill yaml:
- Increase the number of examples in your skill yaml file. The more examples the model has to go off of, the faster it will be able to generate synthetic data. The generated data will also better if the input contains a wide range of examples
- Improve the quality of provided examples. Review the examples provided to it and see if they can rephrased in a way that they align better with what you are hoping to see the model generate. This will improve chances of the model generating better quality synthetic data in large quantities.


### Data generation 

The data generation step is executed via `lab generate`, and is responsible for generating synthetic data. This is an important step as it forms the basis for what the model will end up learning. 

Note: the data generated from this step is only used within the user's local workflow to train the model and help the user fine tune their skill example. There is a separate process of data generation that is conducted in the backend once a user's skill is actually merged into the taxonomy repository  

#### Steps that can be taken in order to improve the quality of generated data:
- Increase the number of instructions generated by passing the `--num-instructions` flag to `lab generate` as follows:
` lab generate --num-instructions 1000`. This will generate 1000 points of synthetic data based on your provided examples. The greater the number of instructions generated, better the model will be trained (within reasonable limits). 
- Adjust the rouge threshold via `--rouge-threshold`. Rouge threshold is a parameter that determines how accepting we want to be of the synthetic data generated by the model. Each new piece of generated data is scored for how similar it is to the data that has already been generated. The value of rouge threshold ranges from 1.0 to 0.0, where 1.0 indicates maximum leniency (we accept every new generated data point) and 0.0 indicates maximum strictness (we reject every new generated data point). Setting a rouge threshold value closer to 0.0 would force the model to generate data that is different from what it has already generated, leading to a more diverse dataset overall. Rouge threshold can be set as follows:
`lab generate --rouge-threshold 0.25`
- Using a better model via `--model-dir`. Larger models can lead to better data generation. This could mean either using a model with more nodes than the default IBM `merlinite-7b` model, such as the `Mixtral-8x7B-Instruct-v0.1` model, or using an un-quantized version of the IBM `merlinite-7b` model. This step requires users to be familiar with various existing models, and which specific models would suit their needs. It can be used as follows:
`lab generate --model mixtral0-88x7B-INstruct-v0.1`
- Set the number of CPU cores that can be used to generate data via `--num-cpus`. This defaults to 10, but increasing this value could potentially lead to better generated dat. It can be used as follows:
`lab generate --num-cpus 15`

# TO DO: expose to users: (https://github.com/instruct-lab/cli/issues/379)
- Increase the number of examples that can be used by the model. This parameter controls how many of your examples are actually fed to the model to generate synthetic data. The more examples it sees, the better the quality of the synthetic data will be. This parameter is not currently exposed to users


### Training 

The training step can be invokeed via `lab train`. This step trains the model on the synthetic data that was generated. The output of this step is a set of adapter files with the general format `adapters-xxx.npz`, where `xxx` is a number. These adapter files represent a snapshot of the model's trained state and are periodically written to disk. 

#### Steps that can be taken in order to train the model better:
- Increase the number of training iterations via `--iters`. A larger number of iterations usually means a better trained model (diminishing returns might kick in around 300 or so iterations). Increasing the number of iterations comes at the cost of having to wait longer for the training to complete. 
- Pick an adapter file with the lowest validation loss. The training process generates and persists an adapter file periodically. The terminal output will tell you the validation loss that each adapter is associated with. The frequency of adapter file generation will be controlled by `--save-every`. For example:
`lab train --save-every 10` would output an adapter file every 10th iteration 
  

# TO DO: expose to users: (https://github.com/instruct-lab/cli/issues/379)
- Tweak learning rate, batch size etc. There are some other parameters that can be tweaked to affect the training process. These parameters are not currently exposed to the end users. 