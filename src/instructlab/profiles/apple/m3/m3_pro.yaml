chat:
  context: default
  # Sets temperature to 0 if enabled, leading to more deterministic responses
  greedy_mode: false
  # Directory where chat logs are stored
  logs_dir: ~/.local/share/instructlab/chatlogs
  # The maximum number of tokens that can be generated in the chat completion
  max_tokens: null
  # Directory where model to be used for chatting with is stored
  model: ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
  session: null
  # visual mode
  vi_mode: false
  # renders vertical overflow if enabled, displays ellipses otherwise
  visible_overflow: true
evaluate:
  # Base taxonomy branch
  base_branch: null
  # Directory where the model to be evaluated is stored
  base_model: ~/.cache/instructlab/models/instructlab/granite-7b-lab
  # Taxonomy branch containing custom skills/knowledge that should be used for evaluation runs
  branch: null
  # Number of GPUs to use for running evaluation
  gpus: 4
  # MMLU benchmarking settings
  mmlu:
    # batch size for evaluation.
    # Valid values are a positive integer or 'auto' to select the largest batch size that will fit in memory
    batch_size: auto
    # number of question-answer pairs provided in the context preceding the question used for evaluation
    few_shots: 5
  # Settings to run MMLU against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mmlu_branch:
    # Directory where custom MMLU tasks are stored
    tasks_dir: ~/.local/share/instructlab/datasets
  model: null
  # multi-turn benchmarking settings for skills
  mt_bench:
    # Directory where model to be used as judge is stored
    judge_model: ~/.cache/instructlab/models/prometheus-8x7b-v2-0
    max_workers: auto
    # Directory where evaluation results are stored
    output_dir: ~/.local/share/instructlab/internal/eval_data
  # Settings to run MT-Bench against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mt_bench_branch:
    # Path to where base taxonomy is stored
    taxonomy_path: ~/.local/share/instructlab/taxonomy
general:
  debug_level: 0
  log_level: INFO
generate:
  # maximum number of words per chunk
  chunk_word_count: 1000
  # Teacher model that will be used to synthetically generate training data
  model: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  # Number of CPU cores to use for generation
  num_cpus: 10
  # Directory where generated datasets are stored
  output_dir: ~/.local/share/instructlab/datasets
  # Directory where pipeline config files are stored
  pipeline: full
  # The total number of instructions to be generated
  sdg_scale_factor: 30
  # Branch of taxonomy used to calculate diff against
  taxonomy_base: empty
  # Directory where taxonomy is stored and accessed from
  taxonomy_path: ~/.local/share/instructlab/taxonomy
  # Teacher model specific settings
  teacher:
    # Serving backend to use to host the teacher model
    backend: llama-cpp
    # Chat template to supply to the teacher model. Possible values:
    #   - Custom chat template string
    #   - Auto: Uses default for serving backend
    chat_template: 'tokenizer'
    # host and port where teacher model is being served
    host_port: 127.0.0.1:8000
    # Llamacpp serving settings
    llama_cpp:
      # number of model layers to offload to GPU
      # -1 means all
      gpu_layers: -1
      # the family of model being served - used to determine the appropriate chat template
      llm_family: 'mixtral'
      # maximum number of tokens that can be processed by the model
      max_ctx_size: 4096
    # Path to teacher model that will be used to synthetically generate training data
    model_path: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
serve:
  # Serving backend to use to host the model
  backend: llama-cpp
  # Chat template to supply to the served model. Possible values:
  #   - Custom chat template string
  #   - Auto: Uses default for serving backend
  chat_template: auto
  # host and port where the model is being served
  host_port: 127.0.0.1:8000
  # Llamacpp serving settings
  llama_cpp:
    # number of model layers to offload to GPU
    # -1 means all
    gpu_layers: -1
    # the family of model being served - used to determine the appropriate chat template
    llm_family: ''
    # maximum number of tokens that can be processed by the model
    max_ctx_size: 4096
  # Path to model that will be served for inference
  model_path: ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
train:
  pipeline: simple
  # Directory where periodic training checkpoints are stored
  ckpt_output_dir: ~/.local/share/instructlab/checkpoints
  # Directory where the processed training data is stored (post filtering/tokenization/masking)
  data_output_dir: ~/.local/share/instructlab/internal
  # Directory where datasets used for training are stored
  data_path: ~/.local/share/instructlab/datasets
  num_epochs: 1
version: 1.0.0

