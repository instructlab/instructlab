chat:
  context: default
  # Directory where chat logs are stored
  logs_dir: ~/.local/share/instructlab/chatlogs
  # The maximum number of tokens that can be generated in the chat completion
  max_tokens: null
  # Directory where model to be used for chatting with is stored
  model: ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
  session: null
  # visual mode
  vi_mode: false
  # renders vertical overflow if enabled, displays ellipses otherwise
  visible_overflow: true
evaluate:
  # Base taxonomy branch
  base_branch: null
  # Directory where the model to be evaluated is stored
  base_model: ~/.cache/instructlab/models/instructlab/granite-7b-lab
  # Taxonomy branch containing custom skills/knowledge that should be used for evaluation runs
  branch: null
  # Number of GPUs to use for running evaluation
  gpus: 4
  # MMLU benchmarking settings
  mmlu:
    # batch size for evaluation.
    # Valid values are a positive integer or 'auto' to select the largest batch size that will fit in memory
    batch_size: auto
    # number of question-answer pairs provided in the context preceding the question used for evaluation
    few_shots: 5
  # Settings to run MMLU against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mmlu_branch:
    # Directory where custom MMLU tasks are stored
    tasks_dir: ~/.local/share/instructlab/datasets
  model: null
  # multi-turn benchmarking settings for skills
  mt_bench:
    # Directory where model to be used as judge is stored
    judge_model: ~/.cache/instructlab/models/prometheus-8x7b-v2.0
    max_workers: auto
    # Directory where evaluation results are stored
    output_dir: ~/.local/share/instructlab/internal/eval_data/mt_bench
  # Settings to run MT-Bench against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mt_bench_branch:
    # Path to where base taxonomy is stored
    taxonomy_path: ~/.local/share/instructlab/taxonomy
general:
  debug_level: 0
  log_level: INFO
generate:
  # maximum number of words per chunk
  chunk_word_count: 1000
  # Teacher model that will be used to synthetically generate training data
  model: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  # Number of CPU cores to use for generation
  num_cpus: 10
  # Number of batches to send on each core
  batch_size: 8
  # Directory where generated datasets are stored
  output_dir: ~/.local/share/instructlab/datasets
  # Directory where pipeline config files are stored
  pipeline: full
  # The total number of instructions to be generated
  sdg_scale_factor: 30
  # Branch of taxonomy used to calculate diff against
  taxonomy_base: empty
  # Directory where taxonomy is stored and accessed from
  taxonomy_path: ~/.local/share/instructlab/taxonomy
  # Teacher model specific settings
  teacher:
    # Serving backend to use to host the teacher model
    backend: llama-cpp
    # Chat template to supply to the teacher model. Possible values:
    #   - Custom chat template string
    #   - Auto: Uses default for serving backend
    chat_template: 'tokenizer'
    # Llamacpp serving settings
    llama_cpp:
      # number of model layers to offload to GPU
      # -1 means all
      gpu_layers: -1
      # the family of model being served - used to determine the appropriate chat template
      llm_family: ''
      # maximum number of tokens that can be processed by the model
      max_ctx_size: 4096
    # Path to teacher model that will be used to synthetically generate training data
    model_path: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
    # Server configuration including host and port.
    server:
      # host where the teacher is being served
      host: 127.0.0.1
      # port where the teacher is being served
      port: 8000
serve:
  # Serving backend to use to host the model
  backend: llama-cpp
  # Chat template to supply to the served model. Possible values:
  #   - Custom chat template string
  #   - Auto: Uses default for serving backend
  chat_template: auto
  # Llamacpp serving settings
  llama_cpp:
    # number of model layers to offload to GPU
    # -1 means all
    gpu_layers: -1
    # the family of model being served - used to determine the appropriate chat template
    llm_family: ''
    # maximum number of tokens that can be processed by the model
    max_ctx_size: 4096
  # Path to model that will be served for inference
  model_path: ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
  # Server configuration including host and port.
  server:
    # host where the model is being served
    host: 127.0.0.1
    # port where the model is being served
    port: 8000
train:
  # Directory where periodic training checkpoints are stored
  ckpt_output_dir: ~/.local/share/instructlab/checkpoints
  # Directory where the processed training data is stored (post filtering/tokenization/masking)
  data_output_dir: ~/.local/share/instructlab/internal
  # Directory where datasets used for training are stored
  data_path: ~/.local/share/instructlab/datasets
  # The number of samples in a batch that the model should see before its parameters are updated
  effective_batch_size: 64
  # Boolean to indicate if the model being trained is a padding-free transformer model such as Granite
  is_padding_free: false
  # device to use for training
  device: mps
  # Training pipeline to use
  pipeline: full
  # Maximum tokens per gpu for each batch that will be handled in a single step
  # If running into out-of-memory errors, this value can be lowered but not below the `max_seq_len`
  max_batch_len: 5000
  # maximum sequence length to be included in the training set
  # Samples exceeding this length will be dropped
  max_seq_len: 4096
  # Directory where the model to be trained is stored
  model_path: ~/.cache/instructlab/models/instructlab/granite-7b-lab
  # Number of GPUs to use for training
  nproc_per_node: 1
  # number of epochs to run training for
  num_epochs: 8
metadata:
  cpu_info: Apple M4 Max
version: 1.0.0

