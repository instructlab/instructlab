evaluate:
  # Directory where the model to be evaluated is stored
  base_model: ~/.cache/instructlab/models/instructlab/granite-7b-lab
  # MMLU benchmarking settings
  mmlu:
    # batch size for evaluation.
    # Valid values are a positive integer or 'auto' to select the largest batch size that will fit in memory
    batch_size: auto
    # number of question-answer pairs provided in the context preceding the question used for evaluation
    few_shots: 5
  # Settings to run MMLU against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mmlu_branch:
    # Directory where custom MMLU tasks are stored
    tasks_dir: ~/.local/share/instructlab/datasets
  model: ~/.local/share/instructlab/checkpoints/hf_format/samples_56/pytorch_model.gguf
  # multi-turn benchmarking settings for skills
  mt_bench:
    # Directory where model to be used as judge is stored
    judge_model: ~/.cache/instructlab/models/granite-7b-lab.gguf
    # Directory where evaluation results are stored
    output_dir: ~/.local/share/instructlab/internal/eval_data
  # Settings to run MT-Bench against a branch of taxonomy containing
  # custom skills/knowledge used for training
  mt_bench_branch:
    # Path to where base taxonomy is stored
    taxonomy_path: ~/.local/share/instructlab/taxonomy
general:
  debug_level: 0
  log_level: INFO
generate:
  # maximum number of words per chunk
  chunk_word_count: 1000
  # Teacher model that will be used to synthetically generate training data
  model: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  # Number of CPU cores to use for generation
  num_cpus: 10
  # Directory where generated datasets are stored
  output_dir: ~/.local/share/instructlab/datasets
  # Directory where pipeline config files are stored
  pipeline: full
  # The total number of instructions to be generated
  sdg_scale_factor: 30
  # Branch of taxonomy used to calculate diff against
  taxonomy_base: empty
  # Directory where taxonomy is stored and accessed from
  taxonomy_path: ~/.local/share/instructlab/taxonomy
  # Teacher model specific settings
  teacher:
    # Serving backend to use to host the teacher model
    backend: llama-cpp
    # Chat template to supply to the teacher model. Possible values:
    #   - Custom chat template string
    #   - Auto: Uses default for serving backend
    chat_template: 'tokenizer'
    # Llamacpp serving settings
    llama_cpp:
      # number of model layers to offload to GPU
      # -1 means all
      gpu_layers: -1
      # the family of model being served - used to determine the appropriate chat template
      llm_family: ''
      # maximum number of tokens that can be processed by the model
      max_ctx_size: 4096
    # Path to teacher model that will be used to synthetically generate training data
    model_path: ~/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
train:
  ckpt_output_dir: ~/.local/share/instructlab/checkpoints
  # Directory where the processed training data is stored (post filtering/tokenization/masking)
  data_output_dir: ~/.local/share/instructlab/internal
  # Directory where datasets used for training are stored
  data_path: ~/.local/share/instructlab/datasets
  # The number of samples in a batch that the model should see before its parameters are updated
  effective_batch_size: 64
  # Maximum tokens per gpu for each batch that will be handled in a single step
  # If running into out-of-memory errors, this value can be lowered but not below the `max_seq_len`
  max_batch_len: 5000
  # maximum sequence length to be included in the training set
  # Samples exceeding this length will be dropped
  max_seq_len: 4096
  # Directory where the model to be trained is stored
  model_path: ~/.cache/instructlab/models/instructlab/granite-7b-lab
  # Number of GPUs to use for training
  nproc_per_node: 1
  # number of epochs to run training for
  num_epochs: 8
metadata:
  cpu_info: AMD CPU
version: 1.0.0

