# SPDX-License-Identifier: Apache-2.0

boto3>=1.35.96
click>=8.1.7,<9.0.0
click-didyoumean>=0.3.0
datasets>=2.18.0
filelock
gguf>=0.6.0
GitPython>=3.1.42
httpx>=0.25.0
instructlab-eval>=0.5.1
instructlab-quantize>=0.1.0
instructlab-schema>=0.4.2
instructlab-sdg>=0.7.2
# XXX(osilkin): we need to pin this version for now while we improve tokenizer logic
# see: https://github.com/instructlab/training/pull/428
instructlab-training>=0.8.0
llama_cpp_python[server]==0.3.6
mlx>=0.5.1,<0.6.0; sys_platform == 'darwin' and platform_machine == 'arm64'
numpy>=1.26.4,<2.0.0
openai>=1.13.3
peft>=0.9.0
prompt-toolkit>=3.0.38
pydantic>=2.7.4
pydantic_yaml>=1.2.0
PyYAML>=6.0.0
rich>=13.3.1
rouge-score>=0.1.2
ruamel.yaml>=0.17.0
sentencepiece>=0.2.0
# "old" version required for vLLM on CUDA to build
tokenizers>=0.11.1
toml>=0.10.2
# Default version. Can be overridden in extra requirements
torch>=2.3.0,<2.6.0
tqdm>=4.66.2
# temporary cap until https://github.com/instructlab/training/pull/443 is merged and consumed within instructlab
# above PR fixes interactions with newer versions of transformers through the training library
transformers>=4.41.2,<4.51.0
trl>=0.12.2,<0.15.0
wandb>=0.16.4
xdg-base-dirs>=6.0.1
psutil>=6.0.0
huggingface_hub[hf_transfer]>=0.1.8
haystack-ai>=2.8
docling-core[chunking]>=2.10.0
docling>=2.18.0
sentence-transformers>=3.0.0
