{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p63ObBag_gvQ"
      },
      "source": [
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Ah15MlL43_"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured\n",
        "!pip install llmsherpa\n",
        "!pip install datasets\n",
        "!pip install pytesseract\n",
        "!pip install \"unstructured[docx,pptx,pdf]\"\n",
        "!pip install pdf2image\n",
        "!pip install pillow_heif\n",
        "!pip install langchain\n",
        "!pip install pdfminer.six\n",
        "!pip install llama_index\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!python3 -m spacy download en_core_web_md\n",
        "!pip install magika"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install magika"
      ],
      "metadata": {
        "id": "Jv25jBNLqnnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-7df4gymLvT"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!sudo apt install libtesseract-dev\n",
        "!sudo apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9iMzaIpuAwx"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'pg_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1zl4M-MTMAi"
      },
      "source": [
        "# References:\n",
        "* https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
        "* https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chunk_document() demo"
      ],
      "metadata": {
        "id": "7pIpJGH1D7CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "# Standard\n",
        "from functools import cache, wraps\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Mapping, Optional, Union\n",
        "import copy\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import platform\n",
        "import re\n",
        "import subprocess\n",
        "import tempfile\n",
        "\n",
        "# Third Party\n",
        "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "from magika import Magika"
      ],
      "metadata": {
        "id": "1DpTGCigEPuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_YAML_RULES = \"\"\"\\\n",
        "extends: relaxed\n",
        "\n",
        "rules:\n",
        "  line-length:\n",
        "    max: 120\n",
        "\"\"\"\n",
        "\n",
        "DEFAULT_CHUNK_OVERLAP = 100\n",
        "server_ctx_size = 4096\n",
        "chunk_word_count= 4000"
      ],
      "metadata": {
        "id": "LQVkf45HEbfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_words(num_words) -> int:\n",
        "    return int(num_words * 1.3)  # 1 word ~ 1.3 token\n",
        "\n",
        "def num_chars_from_tokens(num_tokens) -> int:\n",
        "    return int(num_tokens * 4)  # 1 token ~ 4 English character"
      ],
      "metadata": {
        "id": "2koh-ZkrEgay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_document_orig(documents: List, server_ctx_size, chunk_word_count) -> List[str]:\n",
        "    \"\"\"\n",
        "    Iterates over the documents and splits them into chunks based on the word count provided by the user.\n",
        "    Args:\n",
        "        documents (dict): List of documents retrieved from git (can also consist of a single document).\n",
        "        server_ctx_size (int): Context window size of server.\n",
        "        chunk_word_count (int): Maximum number of words to chunk a document.\n",
        "    Returns:\n",
        "         List[str]: List of chunked documents.\n",
        "    \"\"\"\n",
        "    no_tokens_per_doc = num_tokens_from_words(chunk_word_count)\n",
        "    if no_tokens_per_doc > int(server_ctx_size - 1024):\n",
        "        raise ValueError(\n",
        "            \"Error: {}\".format(\n",
        "                str(\n",
        "                    f\"Given word count ({chunk_word_count}) per doc will exceed the server context window size ({server_ctx_size})\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    content = []\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
        "        chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "        chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    )\n",
        "\n",
        "    for docs in documents:\n",
        "        temp = text_splitter.create_documents([docs])\n",
        "        content.extend([item.page_content for item in temp])\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "6kDRGJAD5bK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_document(documents: List, server_ctx_size, chunk_word_count) -> List[str]:\n",
        "    \"\"\"\n",
        "    Iterates over the documents and splits them into chunks based on the word count provided by the user.\n",
        "    Args:\n",
        "        documents (dict): List of documents retrieved from git (can also consist of a single document).\n",
        "        server_ctx_size (int): Context window size of server.\n",
        "        chunk_word_count (int): Maximum number of words to chunk a document.\n",
        "    Returns:\n",
        "         List[str]: List of chunked documents.\n",
        "    \"\"\"\n",
        "    no_tokens_per_doc = num_tokens_from_words(chunk_word_count)\n",
        "    if no_tokens_per_doc > int(server_ctx_size - 1024):\n",
        "        raise ValueError(\n",
        "            \"Error: {}\".format(\n",
        "                str(\n",
        "                    f\"Given word count ({chunk_word_count}) per doc will exceed the server context window size ({server_ctx_size})\"\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    content = []\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
        "        chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "        chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    )\n",
        "\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "    ]\n",
        "\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=headers_to_split_on,\n",
        "        strip_headers=False\n",
        "    )\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "        language=Language.MARKDOWN,\n",
        "        chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "        chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    )\n",
        "\n",
        "    # Placeholder for params\n",
        "    content = []\n",
        "    chunk_size = num_chars_from_tokens(no_tokens_per_doc)\n",
        "    chunk_overlap = DEFAULT_CHUNK_OVERLAP\n",
        "    text_splitter = None\n",
        "\n",
        "    # Determine file type for heuristics, default with markdown\n",
        "    for docs in documents:\n",
        "        # Try Except Block if Magika fails\n",
        "        try:\n",
        "            m = Magika()\n",
        "            docs_bytes = docs.encode('utf-8')\n",
        "            res = m.identify_bytes(docs_bytes)\n",
        "            file_type = res.output.ct_label.lower()\n",
        "\n",
        "            # Full list of supported languages of Langchain\n",
        "            supported_types = [e.value for e in Language]\n",
        "\n",
        "            # Checks for file types:\n",
        "            file_type_to_language = {\n",
        "                \"go\": Language.GO,\n",
        "                \"java\": Language.JAVA,\n",
        "                \"javascript\": Language.JS,  # Adjusted to match the previous variable name Language.js\n",
        "                \"php\": Language.PHP,\n",
        "                \"python\": Language.PYTHON,\n",
        "                \"ruby\": Language.RUBY,\n",
        "                \"rust\": Language.RUST,\n",
        "                \"latex\": Language.LATEX,\n",
        "                \"html\": Language.HTML,\n",
        "                \"cs\": Language.CSHARP,\n",
        "                \"c\": Language.C,\n",
        "                \"perl\": Language.PERL,\n",
        "            }\n",
        "\n",
        "            print(file_type)\n",
        "\n",
        "            if file_type in file_type_to_language:\n",
        "                language = file_type_to_language[file_type]\n",
        "                text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "                    language=language,\n",
        "                    chunk_size=chunk_size,\n",
        "                    chunk_overlap=chunk_overlap,\n",
        "                )\n",
        "                temp = text_splitter.create_documents([docs])\n",
        "                content.extend([item.page_content for item in temp])\n",
        "\n",
        "            # Default case set for markdown, we assume most cases: pdf->md\n",
        "            else:\n",
        "                # Falls back to default case\n",
        "                text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "                    language=Language.MARKDOWN,\n",
        "                    chunk_size=chunk_size,\n",
        "                    chunk_overlap=chunk_overlap,\n",
        "                )\n",
        "                # Use regex to remove unnecessary dashes in front of pipe characters in a markdown table.\n",
        "                docs = re.sub(r'-{2,}\\|', '-|', docs)\n",
        "                # Remove unnecessary spaces in front of pipe characters in a markdown table.\n",
        "                docs = re.sub(r'\\  +\\|', ' |', docs)\n",
        "                temp = text_splitter.create_documents([docs])\n",
        "                content.extend([item.page_content for item in temp])\n",
        "\n",
        "        except Exception as e:\n",
        "            content = []\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
        "                chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "                chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "            )\n",
        "\n",
        "            for docs in documents:\n",
        "                temp = text_splitter.create_documents([docs])\n",
        "                content.extend([item.page_content for item in temp])\n",
        "            print(\"Error {}\".format(e))\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "Z2nUitiOD6jm"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def chunk_document_md(documents: List, server_ctx_size, chunk_word_count) -> List[str]:\n",
        "#     \"\"\"\n",
        "#     Iterates over the documents and splits them into chunks based on the word count provided by the user.\n",
        "#     Args:\n",
        "#         documents (dict): List of documents retrieved from git (can also consist of a single document).\n",
        "#         server_ctx_size (int): Context window size of server.\n",
        "#         chunk_word_count (int): Maximum number of words to chunk a document.\n",
        "#     Returns:\n",
        "#          List[str]: List of chunked documents.\n",
        "#     \"\"\"\n",
        "#     no_tokens_per_doc = num_tokens_from_words(chunk_word_count)\n",
        "#     if no_tokens_per_doc > int(server_ctx_size - 1024):\n",
        "#         raise ValueError(\n",
        "#             \"Error: {}\".format(\n",
        "#                 str(\n",
        "#                     f\"Given word count ({chunk_word_count}) per doc will exceed the server context window size ({server_ctx_size})\"\n",
        "#                 )\n",
        "#             )\n",
        "#         )\n",
        "#     content = []\n",
        "#     text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "#         language=Language.MARKDOWN,\n",
        "#         chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "#         chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "#     )\n",
        "\n",
        "#     for docs in documents:\n",
        "#         temp = text_splitter.create_documents([docs])\n",
        "#         content.extend([item.page_content for item in temp])\n",
        "\n",
        "#     return content"
      ],
      "metadata": {
        "id": "L56DvQNTJJBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\n",
        "with open(\"/content/redbook-example.md\", 'r', encoding='utf-8') as file:\n",
        "    document = file.read()\n",
        "# documents = [document[:10000], document[20000:30000], document[-10000:]]\n",
        "documents = [document]"
      ],
      "metadata": {
        "id": "55AKskgfuxcv"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output with original chunking method"
      ],
      "metadata": {
        "id": "RqApJANxRPE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = chunk_document_orig(documents, 4096, 1024)\n",
        "print(len(res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRs9q8FmRQ91",
        "outputId": "ec048d88-074e-4eb9-a1c5-5b59767d55cf"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in res:\n",
        "    i=0\n",
        "    print(\"************************* Chunk **********************************\")\n",
        "    print(\"*************************** {} *********************************\".format(len(r)))\n",
        "    print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoLAIcVZTQcM",
        "outputId": "486e8a31-3c9e-4a3a-b8b8-0d0dd38f54dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Chunk **********************************\n",
            "*************************** 603 *********************************\n",
            "Front cover\n",
            "\n",
            "## Accelerating IBM watsonx.data with IBM Fusion HCI\n",
            "\n",
            "IBM Redbooks\n",
            "\n",
            "## Accelerating IBM watsonx.data with IBM Fusion HCI\n",
            "\n",
            "March 2024\n",
            "\n",
            "Note: Before using this information and the product it supports, read the information in 'Notices' on page v.\n",
            "\n",
            "## First Edition (March 2024)\n",
            "\n",
            "This edition applies to Version 2, Release 7, Modification x of IBM Fusion HCI\n",
            "\n",
            "## ' Copyright International Business Machines Corporation 2024. All rights reserved.\n",
            "\n",
            "Note to U.S. Government Users Restricted Rights--Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
            "\n",
            "## Contents\n",
            "************************* Chunk **********************************\n",
            "*************************** 4997 *********************************\n",
            "| Notices                                                                                                                                                                                                                 | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .v   |\n",
            "|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| Trademarks                                                                                                                                                                                                              | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi         |\n",
            "| Preface                                                                                                                                                                                                                 | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii  |\n",
            "| Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii                                                                         |                                                                                                                                        |\n",
            "| Now you can become a published author, too!                                                                                                                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix                                                               |\n",
            "| Comments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix                                                                                      |                                                                                                                                        |\n",
            "| Stay connected to IBM Redbooks                                                                                                                                                                                          | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x                                           |\n",
            "| Chapter 1. Solution overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                              | 1                                                                                                                                      |\n",
            "| 1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                  | 2                                                                                                                                      |\n",
            "| 1.2 Benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                | 4                                                                                                                                      |\n",
            "| 1.3 Architecture, components, and functional characteristics . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                            | 5                                                                                                                                      |\n",
            "| 1.3.1 Integrated solution architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                   | 5                                                                                                                                      |\n",
            "| 1.3.2 Solution component architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                       | 6                                                                                                                                      |\n",
            "************************* Chunk **********************************\n",
            "*************************** 4997 *********************************\n",
            "| Chapter 2. Sizing and planning                                                                                                                                                                                          | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                             |\n",
            "| 2.1 Sizing guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                       | 12                                                                                                                                     |\n",
            "| 2.1.1 Licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12                                                                                      |                                                                                                                                        |\n",
            "| 2.1.2 IBM Fusion HCI with IBM watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                              | 12                                                                                                                                     |\n",
            "| 2.2 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                    | 15                                                                                                                                     |\n",
            "| 2.2.1 Network access to object storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                        | 15                                                                                                                                     |\n",
            "| 2.2.2                                                                                                                                                                                                                   |                                                                                                                                        |\n",
            "| AFM                                                                                                                                                                                                                     | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15               |\n",
            "| 2.3.1 Data sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                            | 16                                                                                                                                     |\n",
            "| Chapter 3. Implementation                                                                                                                                                                                               | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17                                     |\n",
            "| 3.1 IBM Fusion HCI installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                             | 18                                                                                                                                     |\n",
            "| 3.2 Installing Red Hat OpenShift Data Foundation and configuring the Multicloud Object  Gateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 18                                                                                                                                     |\n",
            "| 3.2.1 Configuring Advanced File Management nodes . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                                    | 19                                                                                                                                     |\n",
            "| 3.2.2 Creating the static PV and PVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                        | 21                                                                                                                                     |\n",
            "************************* Chunk **********************************\n",
            "*************************** 4997 *********************************\n",
            "| 3.2.3 Performance tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                 | 27                                                                                                                                     |\n",
            "| 3.3 Installing IBM watsonx.data on IBM Fusion HCI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                           | 29                                                                                                                                     |\n",
            "| Chapter 4. Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                           | 31                                                                                                                                     |\n",
            "| Chapter 5. Backup and restore of IBM Cloud Pak for Data                                                                                                                                                                 | . . . . . . . . . . . . . . . . . . . . . . 33                                                                                         |\n",
            "| 5.1 Considerations and requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                   | 34                                                                                                                                     |\n",
            "| 5.2 Getting the prerequisites ready . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                               | 34                                                                                                                                     |\n",
            "| 5.2.1 Installing the Cloud Pak Backup and Restore service                                                                                                                                                               |                                                                                                                                        |\n",
            "| . . . . . . . . . . . . . . . . . . . . .                                                                                                                                                                               | 34                                                                                                                                     |\n",
            "| 5.2.2 Installing the cpdbr service on the target cluster. . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.3 Backup policies for Cloud Pak for Data applications. . . . . . . . . . . . . . . . . . . . . . .      | 35                                                                                                                                     |\n",
            "| 5.2.4 Creating and assigning a backup policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                            | 36 37                                                                                                                                  |\n",
            "| 5.3 Backing up the source cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                               | 39                                                                                                                                     |\n",
            "| 5.4 Restoring to an alternative cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                                                |                                                                                                                                        |\n",
            "|                                                                                                                                                                                                                         | 39                                                                                                                                     |\n",
            "| Related publications IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                 | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 45                        |\n",
            "************************* Chunk **********************************\n",
            "*************************** 5320 *********************************\n",
            "| Online resources                                                                                                                    |   45 |\n",
            "|-------------------------------------------------------------------------------------------------------------------------------------|------|\n",
            "| Help from IBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . |   46 |\n",
            "\n",
            "## Notices\n",
            "\n",
            "This information was developed for products and services offered in the US. This material might be available from IBM in other languages. However, you may be required to own a copy of the product or product version in that language in order to access it.\n",
            "\n",
            "IBM may not offer the products, services, or features discussed in this document in other countries. Consult your local IBM representative for information on the products and services currently available in your area. Any reference to an IBM product, program, or service is not intended to state or imply that only that IBM product, program, or service may be used. Any functionally equivalent product, program, or service that does not infringe any IBM intellectual property right may be used instead. However, it is the user's responsibility to evaluate and verify the operation of any non-IBM product, program, or service.\n",
            "\n",
            "IBM may have patents or pending patent applications covering subject matter described in this document. The furnishing of this document does not grant you any license to these patents. You can send license inquiries, in writing, to:\n",
            "\n",
            "IBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US\n",
            "\n",
            "INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION 'AS IS' WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in certain transactions, therefore, this statement may not apply to you.\n",
            "\n",
            "This information could include technical inaccuracies or typographical errors. Changes are periodically made to the information herein; these changes will be incorporated in new editions of the publication. IBM may make improvements and/or changes in the product(s) and/or the program(s) described in this publication at any time without notice.\n",
            "\n",
            "Any references in this information to non-IBM websites are provided for convenience only and do not in any manner serve as an endorsement of those websites. The materials at those websites are not part of the materials for this IBM product and use of those websites is at your own risk.\n",
            "\n",
            "IBM may use or distribute any of the information you provide in any way it believes appropriate without incurring any obligation to you.\n",
            "\n",
            "The performance data and client examples cited are presented for illustrative purposes only. Actual performance results may vary depending on specific configurations and operating conditions.\n",
            "\n",
            "Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements or other publicly available sources. IBM has not tested those products and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products.\n",
            "\n",
            "Statements regarding IBM's future direction or intent are subject to change or withdrawal without notice, and represent goals and objectives only.\n",
            "\n",
            "This information contains examples of data and reports used in daily business operations. To illustrate them as completely as possible, the examples include the names of individuals, companies, brands, and products. All of these names are fictitious and any similarity to actual people or business enterprises is entirely coincidental.\n",
            "\n",
            "## COPYRIGHT LICENSE:\n",
            "\n",
            "This information contains sample application programs in source language, which illustrate programming techniques on various operating platforms. You may copy, modify, and distribute these sample programs in any form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample programs are written. These examples have not been thoroughly tested under all conditions. IBM, therefore, cannot guarantee or imply reliability, serviceability, or function of these programs. The sample programs are provided 'AS IS', without warranty of any kind. IBM shall not be liable for any damages arising out of your use of the sample programs.\n",
            "\n",
            "## Trademarks\n",
            "\n",
            "IBM, the IBM logo, and ibm.com are trademarks or registered trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at 'Copyright and trademark information' at https://www.ibm.com/legal/copytrade.shtml\n",
            "\n",
            "The following terms are trademarks or registered trademarks of International Business Machines Corporation, and might also be trademarks or registered trademarks in other countries.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4943 *********************************\n",
            "| Db2fiDS800                    | Mfi             | Redbooksfi    |\n",
            "|-------------------------------|-----------------|---------------|\n",
            "| 0fi fi XIVfi Th               | IBM Cloudfi     | Redbooks (log |\n",
            "| Enterprise Storage ServerfiIB | IBM Cloud Pakfi |               |\n",
            "\n",
            "e following terms are trademarks of other companies:\n",
            "\n",
            "Red Hat, Ceph, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the United States and other countries.\n",
            "\n",
            "Other company, product, or service names may be trademarks or service marks of others.\n",
            "\n",
            "## Preface\n",
            "\n",
            "Organizations that are expanding from AI pilot projects to full-scale production systems typically need a set of tools for building and deploying foundation models, a container-based application platform, software-defined storage, and hardware on which to run it all. This IBM Redpaper publication describes the IBMfi solution for running IBM watsonx.data on premises, with IBM Fusion HCI providing an appliance-based hosting platform, and IBM Storage Ceph providing cloud-scale object storage.\n",
            "\n",
            "This publication shows how to set up the Storage Acceleration feature, so IBM watsonx.data queries can benefit from a shareable on-premises high-performance cache acceleration. The Storage Acceleration feature is available only on an IBM Fusion HCI.\n",
            "\n",
            "This paper is targeted toward technical professionals, consultants, technical support staff, IT Architects, and IT specialists who are responsible for delivering data lakehouse solutions optimized for data, analytics, and AI workloads.\n",
            "\n",
            "This paper was produced by a team of specialists from around the world working with the IBM Redbooks, Tucson Center.\n",
            "\n",
            "Larry Coyne is a Project Leader at the IBM International Technical Support Organization, Tucson, Arizona, center. He has over 35 years of IBM experience, with 23 years in IBM storage software management. He holds degrees in Software Engineering from the University of Texas at El Paso and Project Management from George Washington University. His areas of expertise include client relationship management, quality assurance, development management, and support management for IBM storage management software.\n",
            "\n",
            "Paulina Acevedo is a System Test Architect for the Application and Resiliency Fusion Team. Paulina has been with IBM for more than 17 years and has held several different positions within the Systems organization. She is a certified Project Manager and has been the System Test Project manager for several IBM Storage products.\n",
            "\n",
            "Eduardo Daniel Ibarra Alvarez is a Systems Test Engineer for the Application and Resiliency Fusion Team. Eduardo has a strong automation and testing background.\n",
            "\n",
            "Gabriela Valencia Castillo is a System Test Engineer for the Application and Resiliency Fusion Team. Gabriela has extensive experience in functional and non-functional testing, as well as test plan creation, design scenarios, and creation of test cases.\n",
            "\n",
            "Kenneth Hartsoe is the documentation Content Strategist for IBM Storage Ceph and IBM Fusion Data Foundation, as well as providing content strategy collaboration across multiple solutions within IBM Storage. Kenneth has more than twenty years experience within the storage documentation area, both as a senior technical writer and content strategist, including several years as the storage content strategist at Red Hat.\n",
            "\n",
            "Mike Kieran is a product marketer currently focusing on Storage for IBM watsonx.data. He has more than a decade of storage marketing experience at Pure Storage, NetApp, and Nimble Storage. Mike is the author of four books on digital color and the recipient of an Emmy for science and technology writing. He is an avid stargazer and astrophotographer.\n",
            "\n",
            "## Authors\n",
            "\n",
            "Alberto Larios is a System Test Engineer for the Application and Resiliency Fusion Team. Alberto collaborates with his team as a systems associate and tests Cloud Pak products on IBM Storage. His expertise lies on the data science field with knowledge on machine learning and modelling.\n",
            "\n",
            "Savitha H N is a System Test Engineer for the Application and Resiliency Fusion Team. Savitha has been with IBM from past one year and is a systems associate with Cloud Pak. She holds a Devops engineer certification, and specialist in Q/A testing by executing scenarios to Ensures the product is robust and failure scenarios are considered and refactored.\n",
            "\n",
            "Khanh Ngo is a leader in the IBM Storage CTO office specializing in Data and AI integration with IBM Storage products including the optimization of IBM watsonx.data.\n",
            "\n",
            "AshaRani G R is a System Test Engineer for the Application and Resiliency Fusion Team. Asha possesses extensive expertise encompassing server hardware, management software, and storage solutions within SAN and DAS domains. She has expertise in bringing up compute nodes for Infrastructure as a Service and performing end-to-end system testing. Additionally, she has a strong background in virtualization technology.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5215 *********************************\n",
            "Shyamala Rajagopalan is the senior lead technical content and information architect for IBM Fusion product. She has over 24 years of industry experience, with a significant track record of successfully delivering end-to-end IT documentation solutions to global clients across diverse industries. Her area of contribution includes digital transformation, legacy modernization, integration platforms (middleware, EAI), semiconductor, Cloud, and storage systems. She has performed diverse roles in the information development domain, which includes Information Architect, Technical Writing Manager, Lead Content Developer, Senior technical writer, and Competency Area Mentor.\n",
            "\n",
            "Ben Randall is the User Experience Architect for IBM Fusion and works on product development and design. He has worked in the enterprise storage industry for 21 years, focusing on technologies such as disaster recovery, backup and restore, container native storage, software defined storage, high performance computing, and SAN monitoring.\n",
            "\n",
            "Hemalatha B T is a System Test lead for Application and Resiliency Fusion Team. Hema has been with IBM for over 15 years and was associated with Power System Performance before moving to the functional/system test area working for products like IBM Cloud Pakfi for Systems, IBM Fusion. An enthusiastic quality assurance person who thrives to ensure high quality and perfectly functioning systems are delivered to customers.\n",
            "\n",
            "Todd Tosseth is a Software Engineer for IBM in Tucson, Arizona. Joining IBM in 2001, he has worked as a test and development engineer on several IBM storage products, such as IBM DS8000fi, IBM Storage Scale, and IBM Enterprise Storage Serverfi. He is working on IBM Cloud Pak as a system test engineer, with an emphasis on Cloud Pak storage integration.\n",
            "\n",
            "Jayson Tsingine is an Advisory Software Engineer in the IBM Systems Storage Group based in Tucson, Arizona. Jayson has worked in numerous test roles since he joined IBM in 2003, providing test support for storage products, including IBM XIVfi, DS8000, FlashSystems, Hydra, Spectrum Scale, Spectrum NAS, and Cloud Pak Solutions. He holds a BS degree in Computer Science from the University of Arizona.\n",
            "\n",
            "Israel Vizcarra is a test specialist that works for the Cloud Pak Storage Test Team. He graduated as a Mechatronics Engineer and he has 8 years of experience in the Quality Assurance area for the storage organization. Israel has actively participated in different roles from Functional Verification, System level, and Automation Testing.\n",
            "\n",
            "Zhi Yong Xue is an Architect of Fusion Storage in China. He has 15 years of experience in software design and development as a developer and architect at IBM. He holds a Bachelor's degree in Exploration Technology and Engineering from the University of Petroleum. His areas of expertise include Storage and Cloud computing.\n",
            "\n",
            "Thanks to the following people for their contributions to this project:\n",
            "\n",
            "## Patrik Hysky IBM Redbooksfi, IBM Infrastructure\n",
            "\n",
            "Pramod Thekkepat Achutha, Tara Astigarraga, Scott Colbeck, Karli Collins, Marcel Hergaarden, Ted Hoover, Lisa Huston, Kedar Karmarkar, Moushumi Kalita, Joshua Kim, Ana Lilia Sanchez Llamas, Ajay Lunawat, Boda Devi Manikanta, Kevin Shen, Bill Stoddard, Chris Tan, Thiha Than, Garth Tschetter, Hans Uhlig IBM Infrastructure\n",
            "\n",
            "David Wohlford IBM CHQ, Marketing\n",
            "\n",
            "## Now you can become a published author, too!\n",
            "\n",
            "Here's an opportunity to spotlight your skills, grow your career, and become a published author-all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase product acceptance and customer satisfaction, as you expand your network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\n",
            "\n",
            "Find out more about the residency program, browse the residency index, and apply online at: ibm.com /redbooks/residencies.html\n",
            "\n",
            "## Comments welcome\n",
            "\n",
            "Your comments are important to us!\n",
            "\n",
            "We want our papers to be as helpful as possible. Send us your comments about this paper or other IBM Redbooks publications in one of the following ways:\n",
            "\n",
            "Use the online Contact us review Redbooks form found at: ibm.com /redbooks\n",
            "\n",
            "Send your comments in an email to:\n",
            "\n",
            "redbooks@us.ibm.com\n",
            "\n",
            "Mail your comments to:\n",
            "\n",
            "IBM Corporation, IBM Redbooks Dept. HYTD Mail Station P099 2455 South Road Poughkeepsie, NY 12601-5400\n",
            "\n",
            "## Stay connected to IBM Redbooks\n",
            "\n",
            "Find us on LinkedIn:\n",
            "\n",
            "https://www.linkedin.com/groups/2130806\n",
            "\n",
            "Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks weekly newsletter:\n",
            "\n",
            "https://www.redbooks.ibm.com/subscribe\n",
            "\n",
            "Stay current on recent Redbooks publications with RSS Feeds:\n",
            "\n",
            "https://www.redbooks.ibm.com/rss.html\n",
            "\n",
            "Chapter 1.\n",
            "\n",
            "## Solution overview\n",
            "\n",
            "Organizations that are expanding from AI pilot projects to full-scale production systems typically need the following components:\n",
            "\n",
            "A set of tools for building and deploying foundation models\n",
            "\n",
            "A container-based application platform\n",
            "\n",
            "Software-defined storage\n",
            "\n",
            "Hardware on which to run it\n",
            "************************* Chunk **********************************\n",
            "*************************** 5219 *********************************\n",
            "A container-based application platform\n",
            "\n",
            "Software-defined storage\n",
            "\n",
            "Hardware on which to run it\n",
            "\n",
            "This publication describes the IBM solution for running IBM watsonx.data on premises, with IBM Fusion HCI providing an appliance-based hosting platform, and IBM Storage Ceph providing cloud-scale object storage. This publication shows how to set up the Storage Acceleration feature, which is only available on IBM Fusion HCI, so IBM watsonx.data queries can benefit from a shareable on-premises, high-performance cache acceleration.\n",
            "\n",
            "This paper is targeted toward technical professionals including consultants, technical support staff, IT Architects, and IT specialists who are responsible for delivering optimized for data, analytics, and AI workloads.\n",
            "\n",
            "This chapter includes an overview covering the background of data lakes and how the IBM solution of IBM watsonx.data, IBM Storage Ceph, and IBM Fusion HCI accelerated infrastructure works to improve on-premises performance and improves cost efficiency. The architecture of the solution and components are also described.\n",
            "\n",
            "1\n",
            "\n",
            "## 1.1 Overview\n",
            "\n",
            "This section describes the evolution of data lakes, the emergence of data lakehouses, and IBM watsonx.data lakehouse, IBM Storage Ceph, and the IBM Fusion HCI accelerated infrastructure solution.\n",
            "\n",
            "## From data warehouse to data lake\n",
            "\n",
            "During the past 20 years, large organizations have changed the way they aggregate data for analytics and business intelligence (BI) purposes. The original approach was to build a single monolithic database, or data warehouse, and then analyze specific subsets of the data through an extract, transform, load (ETL) process based on queries by using structured query language (SQL).\n",
            "\n",
            "Data warehouses are often used for repeatable reporting and analysis workloads such as monthly sales reports, tracking of sales per region, and website traffic. But building and maintaining a data warehouse is a costly, time-consuming process, and data warehouses work only with structured data.\n",
            "\n",
            "Moving data warehouses to the cloud doesn't solve the problem. Sometimes, it makes them even more expensive, and they're still not well suited to machine learning or AI applications.\n",
            "\n",
            "These limitations led to the concept of the data lake, which is a centralized repository that can store massive volumes of data in its original form so that it's consolidated, integrated, secure, and accessible. Data lakes are designed to accommodate all types of data from many different sources:\n",
            "\n",
            "Structured data, such as database tables and Excel sheets\n",
            "\n",
            "Semi-structured data, such as herbages and XML files\n",
            "\n",
            "Unstructured data, such as images, video, audio, and social media posts\n",
            "\n",
            "Because data lakes are massively scalable and can handle all types of data, they are ideal for real-time analytics, predictive analytics, and machine learning or AI. They are also typically less costly than data warehouses.\n",
            "\n",
            "## Data lakehouse architecture\n",
            "\n",
            "The data lakehouse is an emerging architecture that offers the flexibility of a data lake with the performance and structure of a data warehouse. Lakehouse solutions typically provide a high-performance query engine over low-cost object storage along with a metadata governance layer. Data lakehouses are based around open-standard object storage and enable multiple analytics and AI workloads to operate simultaneously on top of the data lake without requiring that the data be duplicated and converted.\n",
            "\n",
            "A key benefit of data lakehouses is that they address the needs of both traditional data warehouse analysts who curate and publish data for business intelligence and reporting purposes; and of data scientists and engineers who run more complex data analysis and processing workloads.\n",
            "\n",
            "IBM watsonx.data, shown in Figure 1-1, is built on an open lakehouse architecture, supported by querying, governance, and open data formats for accessing and sharing data.\n",
            "\n",
            "Figure 1-1 IBM watsonx.data provides an ideal platform for building and scaling AI applications\n",
            "\n",
            "## IBM watsonx.data, IBM Storage Ceph, and the IBM Fusion HCI accelerated infrastructure solution\n",
            "\n",
            "Administrators of today's modern data lakehouses are required to think about storage optimizations as a top priority and a two-tiered approach. The first tier is an on-premises high-performance acceleration layer, which provides superior storage bandwidth with a cost-effective caching approach for the hybrid cloud object storage. The second tier is the low-cost persistent storage for your on-premises storage needs. With the combination of IBM Fusion HCI as your first tier solution and IBM Storage Ceph as your second tier solution, an organization can improve query performance with Storage Acceleration, significant cost advantage, and superior data management capabilities. IBM watsonx.data can take advantage of both of these tiers when using the IBM Fusion HCI and IBM Storage Ceph.\n",
            "\n",
            "## 1.2 Benefits\n",
            "\n",
            "IBM Fusion HCI is a hosting platform for IBM watsonx.data and provides the following benefits and features:\n",
            "\n",
            "Hosting platform for IBM watsonx products, starting with IBM watsonx.data:\n",
            "\n",
            "-Provides an automated deployment of Red Hat OpenShift on top of resilient compute, network, and storage in an appliance form-factor.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5267 *********************************\n",
            "-Provides all the storage classes that are needed by IBM Cloud Pak for Data (CP4D) and IBM watsonx.data\n",
            "\n",
            "Storage acceleration feature for Tier 1 data caching to accelerate IBM watsonx.data query performance to 5-15x improvement:\n",
            "\n",
            "-Connects to multiple object buckets\n",
            "\n",
            "-Uses intelligent caching to accelerate data access including automatic eviction\n",
            "\n",
            "-High-performance persistent object cache with low-capacity requirements:\n",
            "\n",
            "Cache once concept for faster performance\n",
            "\n",
            "Shareable across all engines and projects and namespaces\n",
            "\n",
            "Cache available to all nodes\n",
            "\n",
            "Multi-protocol (including virtualization) support\n",
            "\n",
            "Supports IBM Cloud Object Storage, Amazon Web Services, Seagate Lyve Cloud, Google Cloud Platform\n",
            "\n",
            "watsonx platform in a box:\n",
            "\n",
            "-Install efforts of a few days\n",
            "\n",
            "-Support for a maximum of 2 dedicated GPU nodes with optimizations\n",
            "\n",
            "-Support for a maximum of 2 dedicated gateway nodes for data access services\n",
            "\n",
            "-Scalable by adding nodes and disk capacity\n",
            "\n",
            "Shared run-time platform:\n",
            "\n",
            "-Multiple solutions in a box:\n",
            "\n",
            "IBM Db2fi Warehouse\n",
            "\n",
            "watsonx.data\n",
            "\n",
            "-Shared resources across multiple engines:\n",
            "\n",
            "Presto\n",
            "\n",
            "S p a r k\n",
            "\n",
            "-Compute-storage nodes provide high core-to-memory ratio. A C05 node with 64 cores and 2 TB memory yields a 1:32 core-to-memory ratio.\n",
            "\n",
            "Global Data Platform:\n",
            "\n",
            "-Data access services provides better performance across multiple parallel paths with single source of truth.\n",
            "\n",
            "-Data virtualization, collaboration and orchestration services for a true global namespace and data sharing across geo-distributed locations.\n",
            "\n",
            "-Supports compression at storage class level for space savings for various open data formats.\n",
            "\n",
            "-Encryption ensures both secure storage and secure deletion of data (at file system level).\n",
            "\n",
            "Local S3 object storage\n",
            "\n",
            "IBM Storage Ceph as an external cloud-scale S3 object store\n",
            "\n",
            "Ability to integrate GPUs into the IBM watsonx solution\n",
            "\n",
            "It is worth noting that the Storage Acceleration feature providing the data caching for improved query performance is very different from your traditional local caching. The IBM Fusion HCI has a global data platform which allows for a cache only once concept to achieve faster performance and transparency. After an object has been cached, it is available and shareable to every engine with IBM watsonx.data across all nodes within the cluster. The Storage Acceleration provides a persistent data cache for all engines. Newly provisioned engines also begin with a warm or hot cache.\n",
            "\n",
            "## 1.3 Architecture, components, and functional characteristics\n",
            "\n",
            "This section provides an architecture overview of IBM watsonx.data with IBM Fusion HCI and the IBM technologies integrated within the solution.\n",
            "\n",
            "## 1.3.1 Integrated solution architecture\n",
            "\n",
            "This integrated solution, as shown in Figure 1-2, consists of IBM watsonx.data deployed on Red Hat OpenShift hosted by the IBM Fusion HCI. IBM watsonx.data is connected to accelerated buckets hosted in either the public cloud, which includes IBM Cloud, Amazon S3, and Google Cloud Storage, or on-premises infrastructure such as IBM Storage Ceph. By connecting IBM Fusion HCI to external object buckets, high-performance object access is delivered by intelligent caching that is provided by IBM Fusion HCI's storage infrastructure. IBM Fusion HCI exposes the accelerated buckets to IBM watsonx.data for attachment to a query engine (Presto, Spark). Persistent cache is immediately available for newly provisioned engines.\n",
            "\n",
            "Figure 1-2 IBM watsonx.data Storage Acceleration hosted on IBM Fusion HCI\n",
            "\n",
            "## 1.3.2 Solution component architectures\n",
            "\n",
            "This section describes the architectures of the solution components.\n",
            "\n",
            "## IBM watsonx.data\n",
            "\n",
            "IBM watsonx.data is an open, hybrid, and governed data lakehouse optimized for all data and AI workloads. It combines the high performance and usability of a data warehouse with the flexibility and scalability of data lakes. IBM watsonx.data is a unique solution that allows co-existence of open source technologies and proprietary products. It offers a single point of entry where you can store the data or attach data sources for managing and analyzing structured, semi-structured, and unstructured enterprise data, which enables access to all data across cloud and on-premises environments.\n",
            "\n",
            "The following components as shown in Figure 1-3 provide the foundation of IBM watsonx.data:\n",
            "\n",
            "Open table formats, such as Apache Iceberg provide structure and deliver the reliability of SQL with big data. They allow different engines to access the same data at the same time, and enable data sharing across multiple repositories including data warehouses and data lakes.\n",
            "\n",
            "Query engines access data in an open table format. IBM watsonx.data query engines are fully modular and can be dynamically scaled to meet workload demands and concurrency.\n",
            "\n",
            "The technical metadata service enables the query engine to know the location, format, and read capabilities of the data.\n",
            "\n",
            "Data catalogs assist with finding the correct data and deliver semantic information for policies and rules.\n",
            "\n",
            "The policy engine enables users to define and enforce data protection.\n",
            "\n",
            "Figure 1-3 IBM watsonx.data architecture\n",
            "\n",
            "The IBM watsonx.data software stack is built on IBM Fusion HCI and IBM Storage Ceph to provide high-performance infrastructure and storage.\n",
            "\n",
            "## IBM Fusion HCI\n",
            "************************* Chunk **********************************\n",
            "*************************** 5246 *********************************\n",
            "## IBM Fusion HCI\n",
            "\n",
            "IBM Fusion HCI is a hyper-converged appliance that delivers all of the infrastructure needed to run Red Hat OpenShift on bare metal, which eliminates the complexity of designing, deploying, and maintaining an on-premises architecture for IBM watsonx.data. See Figure 1-4 on page 8. The appliance is delivered as a rack with all components mounted, cabled, and tested. It provides all the infrastructure resources that are required to host the Red Hat OpenShift cluster, such as storage nodes, compute nodes, and network switches.\n",
            "\n",
            "S3 object storage, IBM Storage Ceph, IBM Storage Scale, and NAS file arrays are available in a single namespace. Access by applications is unaffected by the type of storage behind the namespace. Intelligent global data caching enables accessing remote data at local file system speeds. IBM Fusion HCI uses a dedicated network for Red Hat OpenShift traffic and a dedicated, high-performance network for the storage cluster, and provides scalability for expanding workloads. Online migration of data from remote storage systems to the IBM Fusion HCI file system is included.\n",
            "\n",
            "Figure 1-4 IBM Fusion HCI architecture\n",
            "\n",
            "## IBM Storage Ceph\n",
            "\n",
            "IBM Storage Ceph is a software-defined storage platform that is based on an open source development model and can be deployed on industry-standard x86 hardware. It provides non-disruptive, horizontal scaling of object, block, and file storage to thousands of clients accessing exabytes of data. It is ideal for modern AI frameworks that require data lake capabilities.\n",
            "\n",
            "IBM Storage Ceph provides an external S3 object store for IBM watsonx.data. This S3 object store can be the main S3 object store for IBM watsonx.data, or an additional S3 object store with other on-premises or public-cloud object stores. The IBM Storage Ceph object storage interface, the Ceph Object Gateway, is compatible with a large subset of the Amazon S3 RESTful API. See Figure 1-5.\n",
            "\n",
            "Figure 1-5 Ceph Object Gateway architecture\n",
            "\n",
            "Multiple Red Hat OpenShift clusters can share storage from the same Ceph S3 object store.\n",
            "\n",
            "## IBM Storage Scale Erasure Code Edition Active File Management\n",
            "\n",
            "IBM Fusion HCI uses IBM Storage Scale Erasure Coded Edition (ECE) as an underlying storage platform. IBM Fusion ECE is a high-performance parallel file system that is used in High Performance Computing (HCP) and maximizes storage I/O within a clustered compute environment. This high-performance storage layer provides storage for IBM Cloud Pak for Data and IBM watsonx.data internal services and serves as a cache for storage accelerated buckets.\n",
            "\n",
            "This solution achieves storage acceleration by using Storage Scale's Active File Management (AFM) technology to connect to existing object storage buckets. These buckets reside in an on-premises object storage solution, such as IBM Storage Ceph or IBM Cloud Object Storage or in a public cloud provider such as AWS, Azure, or IBM Cloudfi. AFM is a high-speed cache for buckets it is attached to, allowing for data access that is significantly faster than I/O on the buckets directly. See Figure 1-6.\n",
            "\n",
            "Figure 1-6 Storage acceleration with Active File Management\n",
            "\n",
            "## Multicloud Object Gateway\n",
            "\n",
            "Multicloud Object Gateway (MCG) is a lightweight object storage service for Red Hat OpenShift. Although MCG can function as an object storage provider using storage from the IBM Fusion HCI appliance, in this solution, MCG functions as a gateway between IBM watsonx.data and storage accelerated buckets provided by AFM. MCG connects directly to filesets in the Storage Scale ECE file system that map back to object buckets attached to AFM. When IBM watsonx.data reads from buckets, the reads pass through MCG to the AFM cache for the bucket. A cache hit results in a high-performance read. With a cache miss, the object is fetched from the external bucket hosting the data. See Figure 1-7.\n",
            "\n",
            "Figure 1-7 Multicloud Object Gateway and Storage Acceleration\n",
            "\n",
            "Chapter 2.\n",
            "\n",
            "## Sizing and planning\n",
            "\n",
            "This chapter describes sizing guidelines for the licensed components and highlights several planning activities that are related to the solution in this publication.\n",
            "\n",
            "## 2.1 Sizing guidelines\n",
            "\n",
            "This section provides sample sizing configurations for the licensed components.\n",
            "\n",
            "## 2.1.1 Licensing\n",
            "\n",
            "The following list highlights the software and hardware licensing for IBM Fusion HCI:\n",
            "\n",
            "watsonx is licensed by available cores not total cores:\n",
            "\n",
            "-A Fusion HCI 32 core worker node has 20 available cores. Watsonx needs 20 VPC $^{1}$of entitlement per Fusion HCI 32 core server.\n",
            "\n",
            "-A Fusion HCI 64 core worker node has 52 available cores. watsonx needs 52 VPC of entitlement per Fusion HCI 64 core server.\n",
            "\n",
            "-The other 12 cores in each server are reserved for Red Hat OpenShift and Fusion.\n",
            "\n",
            "Fusion HCI CPUs can support SMT$^{2}$=2:\n",
            "\n",
            "-A Fusion HCI 32 core worker node has 40 available vCPU.\n",
            "\n",
            "-A Fusion HCI 64 core worker node has 104 available vCPU.\n",
            "\n",
            "Fusion HCI 9155 Expert Care is required:\n",
            "\n",
            "-Expert care provides Fusion hardware support starting at beginning of year 1.\n",
            "\n",
            "-Expert care provides Fusion software support starting at beginning of year 1.\n",
            "\n",
            "Strategies for managing excess Fusion HCI hardware capacity:\n",
            "\n",
            "-License watsonx to a sub-capacity of the Fusion HCI cluster size.\n",
            "************************* Chunk **********************************\n",
            "*************************** 3744 *********************************\n",
            "-License watsonx to a sub-capacity of the Fusion HCI cluster size.\n",
            "\n",
            "-Use excess cluster capacity for other Cloud Paks and workloads. If segregation of workload is required workloads can be isolated in separate namespaces.\n",
            "\n",
            "Learn more about container licensing here.\n",
            "\n",
            "## 2.1.2 IBM Fusion HCI with IBM watsonx.data\n",
            "\n",
            "This section provides guidance to plan for IBM Fusion HCI configurations using watsonx.data to meet your requirements. There are two types of configurations: Standard and Performance:\n",
            "\n",
            "Standard configuration use the following:\n",
            "\n",
            "-32 core worker nodes, each with 512 GB of memory.\n",
            "\n",
            "-Fusion HCI 32-core servers have 20 cores (40 vCPU) that are available for workloads after subtracting overhead for Red Hat OpenShift and Fusion software.\n",
            "\n",
            "Performance configurations use the following:\n",
            "\n",
            "-64 core worker nodes each with 2048 GB of memory.\n",
            "\n",
            "-Fusion HCI 64-core servers have 52 cores (104 vCPU) that are available for workloads after subtracting overhead for Red Hat OpenShift and Fusion software.\n",
            "\n",
            "## Detailed configurations follow:\n",
            "\n",
            "'Fusion HCI Standard configurations' on page 13\n",
            "\n",
            "'Fusion HCI Performance configurations' on page 14\n",
            "\n",
            "'Multi Rack Performance and Standard configurations' on page 14\n",
            "\n",
            "## Fusion HCI Standard configurations\n",
            "\n",
            "Fusion HCI Standard configurations are shown in Table 2-1.\n",
            "\n",
            "Table 2-1 Standard configuration details\n",
            "\n",
            "| Fusion HCI                | Fusion HCI                   | E03 3    | E04 3    | E05 3    | E06 3    | E07 3    | E08 3    | E09 3    | E10 3    | E11 3    |\n",
            "|---------------------------|------------------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n",
            "|                           | Control Nodes                | E03 3    | E04 3    | E05 3    | E06 3    | E07 3    | E08 3    | E09 3    | E10 3    | E11 3    |\n",
            "|                           | Worker nodes                 | 3        | 4        | 5        | 6        | 7        | 8        | 9        | 10       | 11       |\n",
            "|                           | Active NVMe drives           | 12       | 14       | 16       | 18       | 20       | 22       | 24       | 26       | 28       |\n",
            "| Fusion hardware           | Query accelerator  nodes     | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional |\n",
            "| M/T 9155                  | Total worker cores           | 96       | 128      | 160      | 192      | 224      | 256      | 288)     | 320      | 352      |\n",
            "|                           | Available cores a            | 60       | 80       | 100      | 120      | 140      | 160      | 180      | 200      | 220      |\n",
            "|                           | Available memory$^{b}$  (GB) | 1,296    | 1,728    | 2,160    | 2,592    | 3,024    | 3,456    | 3,888    | 4,320    | 4,752    |\n",
            "|                           | Usable NVMe$^{c}$  (TB)      | 59       | 69       | 79       | 89       | 99       | 109      | 119      | 129      | 139      |\n",
            "| Fusion  software 5771-PP7 | VPCs to license              | 96       | 128      | 160      | 192      | 224      | 256      | 288      | 320      | 352      |\n",
            "| watsonx D0F4SZX           | VPCs$^{d}$ to license        | 60       | 80       | 100      | 120      | 140      | 160      | 180      | 200      | 220      |\n",
            "\n",
            "a. SMT=1.\n",
            "\n",
            "b. This is a double memory configuration with 16 GB RAM per core. You can reduce cost by configuring with 8 GB RAM per core.\n",
            "\n",
            "c. Decimal TB. The formula is 7.68 TB x number of drives x 0.65. The 0.65 approximates 4+2p erasure coding overhead.\n",
            "\n",
            "d. watsonx.data is licensed per usable / available VPC not raw cores.\n",
            "\n",
            "## Fusion HCI Performance configurations\n",
            "\n",
            "Fusion HCI Performance configurations are shown in Table 2-2.\n",
            "\n",
            "Table 2-2 Performance configuration details\n",
            "************************* Chunk **********************************\n",
            "*************************** 3644 *********************************\n",
            "Table 2-2 Performance configuration details\n",
            "\n",
            "| Fusion HCI                | Fusion HCI                   | P03 3    | P04 3    | P05 3    | P06 3    | P07 3    | P08 3    | P09 3    | P10 3    | P11 3    |\n",
            "|---------------------------|------------------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n",
            "|                           | Control Nodes                | P03 3    | P04 3    | P05 3    | P06 3    | P07 3    | P08 3    | P09 3    | P10 3    | P11 3    |\n",
            "|                           | Worker nodes                 | 3        | 4        | 5        | 6        | 7        | 8        | 9        | 10       | 11       |\n",
            "|                           | Active NVMe drives           | 12       | 14       | 16       | 18       | 20       | 22       | 24       | 26       | 28       |\n",
            "| Fusion hardware           | Query accelerator  nodes     | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional |\n",
            "| M/T 9155                  | Total worker cores           | 192      | 256      | 320      | 384      | 448      | 512      | 576      | 640      | 704      |\n",
            "|                           | Available cores a            | 156      | 208      | 260      | 312      | 364      | 416      | 468      | 520      | 572      |\n",
            "|                           | Available memory$^{b}$  (GB) | 5.904    | 7,872    | 9,840    | 11,808   | 13,776   | 15,744   | 17,712   | 19,680   | 21,648   |\n",
            "|                           | Usable NVMe$^{c}$  (TB)      | 59       | 69       | 79       | 89       | 99       | 109      | 119      | 129      | 139      |\n",
            "| Fusion  software 5771-PP7 | VPCs to license              | 192      | 256      | 320      | 384      | 448      | 512      | 576      | 640      | 704      |\n",
            "| watsonx D0F4SZX           | VPCs$^{d}$ to license        | 156      | 208      | 260      | 312      | 364      | 416      | 468      | 520      | 572      |\n",
            "\n",
            "b. This is a double memory configuration with 16 GB RAM per core. You can reduce cost by configuring with 8 GB RAM per core.\n",
            "\n",
            "c. Decimal TB. The formula is 7.68 TB x number of drives x 0.65. The 0.65 approximates 4+2p erasure coding overhead.\n",
            "\n",
            "d. watsonx.data is licensed per usable / available VPC not raw cores.\n",
            "\n",
            "## Multi Rack Performance and Standard configurations\n",
            "\n",
            "Table 2-3 shows the Multi Rack Performance configurations.\n",
            "\n",
            "Table 2-3 Multi Rack Performance configurations\n",
            "\n",
            "| Size            | 64-core worker  nodes   | Available vCPU  (SMT=1)   | Available vCPU  (SMT=2)   | Total Memory in  GB   |\n",
            "|-----------------|-------------------------|---------------------------|---------------------------|-----------------------|\n",
            "| Two racks       | Two racks               | Two racks                 | Two racks                 | Two racks             |\n",
            "| P11 + P03 a     | 17                      | 884                       | 1768                      | 33,456                |\n",
            "| P11 + P11       | 25                      | 1300                      | 2600                      | 49,200                |\n",
            "| Three racks     | Three racks             | Three racks               | Three racks               | Three racks           |\n",
            "| P11+P11+P03 a   | 31                      | 1612                      | 3224                      | 61,008                |\n",
            "| P11 + P11 + P11 | 39                      | 2028                      | 4056                      | 76,752                |\n",
            "\n",
            "a. The 3 control nodes in rack 2 and rack 3 are converted to worker nodes.\n",
            "\n",
            "Table 2-4 shows the Multi Rack Standard configurations.\n",
            "\n",
            "## 2.2.2 AFM\n",
            "\n",
            "Table 2-4 Multi rack Standard configuration\n",
            "************************* Chunk **********************************\n",
            "*************************** 4918 *********************************\n",
            "## 2.2.2 AFM\n",
            "\n",
            "Table 2-4 Multi rack Standard configuration\n",
            "\n",
            "| Size            | 32-core worker  nodes   | Available vCPU  (SMT=1)   | Available vCPU  (SMT=2)   | Total Memory in  GB   |\n",
            "|-----------------|-------------------------|---------------------------|---------------------------|-----------------------|\n",
            "| Two racks       | Two racks               | Two racks                 | Two racks                 | Two racks             |\n",
            "| E11 + E03 a     | 17                      | 340                       | 680                       | 7,344                 |\n",
            "| E11 + E05       | 19                      | 380                       | 760                       | 8,208                 |\n",
            "| E11 + E07       | 21                      | 420                       | 840                       | 9,072                 |\n",
            "| E11 + E09       | 23                      | 460                       | 920                       | 9.936                 |\n",
            "| E11 + E11       | 25                      | 500                       | 1000                      | 10,800                |\n",
            "| Three racks     | Three racks             | Three racks               | Three racks               | Three racks           |\n",
            "| E11+E11+E03 a   | 31                      | 620                       | 1240                      | 13,392                |\n",
            "| E11+E11+E05     | 33                      | 660                       | 1320                      | 14,256                |\n",
            "| E11 + E11 + E07 | 35                      | 700                       | 1400                      | 15,120                |\n",
            "| E11 + E11 + E09 | 37                      | 740                       | 1480                      | 15,984                |\n",
            "| E11 + E11 + E11 | 39                      | 780                       | 1560                      | 16,848                |\n",
            "\n",
            "## 2.2 Planning\n",
            "\n",
            "Planning tasks help ensure that the IBM Fusion HCI is accurately integrated with IBM watsonx.data and configured properly for your operations.\n",
            "\n",
            "For more information, see Planning and prerequisites.\n",
            "\n",
            "## 2.2.1 Network access to object storage\n",
            "\n",
            "The IBM Fusion HCI connects to the data center network during the initial appliance setup. The appliance includes two high-speed switches that are connected to the core network through one port channel. This connection acts as the gateway between the IBM Fusion appliance and the network. It enables administration of the appliance and Red Hat OpenShift and is also used for network traffic in and out of the cluster. Network resources and applicable configuration settings are applied during this setup phase.\n",
            "\n",
            "For more information, see Network planning.\n",
            "\n",
            "AFM creates associations between clusters and the data source. It provides a single, global namespace across sites to automate the flow of data. AFM is enabled on the fileset that connects to the remote S3 endpoint to access the cache.\n",
            "\n",
            "The cache fileset is served by the AFM node, which functions as a gateway. As a gateway, the AFM node owns the fileset and communicates regarding data transfers.\n",
            "\n",
            "Make sure the IBM Fusion HCI has AFM nodes installed and configured as part of the Red Hat OpenShift cluster. AFM nodes must be installed and configured before you begin any storage acceleration operations. In addition, determine how large the tier 1 cache must be for storage acceleration. Ensure there is sufficient usable storage for that cache, in addition to the storage that is used for the Cloud Pak for Data and IBM watsonx.data PVCs.\n",
            "\n",
            "For more information, see Planning for AFM and Sharing Data.\n",
            "\n",
            "## 2.3 Customer use cases\n",
            "\n",
            "IBM watsonx provides the crucial data analytics and AI capabilities that all large organizations require. The strength of IBM Fusion HCI for IBM watsonx.data is the appliance-like experience where Red Hat OpenShift cluster, compute, storage, and network is all in a single box. It allows for a shorter time to value by having everything you need, so you can immediately start performing queries by using IBM watsonx.data.\n",
            "\n",
            "Consider the following key use cases for IBM watsonx.data:\n",
            "\n",
            "AI and machine learning (ML) at scale\n",
            "\n",
            "Build, train, tune, deploy, and monitor trusted AI and ML models for mission-critical workloads with governed data in IBM watsonx.data and ensure compliance with lineage and reproducibility of data used for AI.\n",
            "\n",
            "Real-time analytical and business intelligence Combine data from existing sources with new data to unlock new, faster insights without the cost and complexity of duplicating and moving data across different environments.\n",
            "\n",
            "Streamline data engineering\n",
            "\n",
            "Reduce data pipelines, simplify data transformation, and enrich data for consumption using SQL, Python, or an AI-infused conversational interface.\n",
            "\n",
            "Responsible data sharing Enable self-service access for more users to more data while ensuring security and compliance through centralized governance and local automated policy enforcement.\n",
            "\n",
            "## 2.3.1 Data sharing\n",
            "************************* Chunk **********************************\n",
            "*************************** 5206 *********************************\n",
            "## 2.3.1 Data sharing\n",
            "\n",
            "IBM Db2 Warehouse has the option to write and read to and from a cloud bucket using open formats such as parquet and iceberg. This allows for seamless integrating and sharing of data between IBM Db2 Warehouse and IBM watsonx.data without the need for deduplication or additional extract, transform, load operations. This might reduce your costs for storage that is used by IBM Db2 Warehouse and offload some of the workloads to IBM watsonx.data.\n",
            "\n",
            "Chapter 3.\n",
            "\n",
            "## Implementation\n",
            "\n",
            "Implementation involves the combination of IBM Fusion HCI and IBM Storage Ceph, which provide all the infrastructure for a stand-alone data lakehouse. The implementation includes installation of IBM Fusion HCI, Multicloud Object Gateway (MCG) configuration, Active File Management (AFM) configuration, performance tuning, and the installation of IBM watsonx.data.\n",
            "\n",
            "3\n",
            "\n",
            "## 3.1 IBM Fusion HCI installation\n",
            "\n",
            "If you already have an IBM Fusion HCI installation, ensure that your Red Hat OpenShift Container Platform is at Version 4.12.7.\n",
            "\n",
            "For the procedure to install IBM Fusion HCI 2.7.x, see Deploying IBM Fusion HCI.\n",
            "\n",
            "## 3.2 Installing Red Hat OpenShift Data Foundation and configuring the Multicloud Object Gateway\n",
            "\n",
            "The Multicloud Object Gateway (MCG) provides an object endpoint to which IBM watsonx.data and other workloads can connect to access multiple buckets, including Storage Acceleration buckets. The MCG is provided by the Red Hat OpenShift Data Foundation operator.\n",
            "\n",
            "Install the Red Hat OpenShift Data Foundation operator into Red Hat OpenShift Container Platform:\n",
            "\n",
            "1. Log in to your Red Hat OpenShift Container Platform.\n",
            "\n",
            "2. Go to OperatorHub and search for Data Foundation operator.\n",
            "\n",
            "3. Type Data Foundation in the Filter by keyword field to find the Data Foundation operator.\n",
            "\n",
            "4. Click Install. See Figure 3-1.\n",
            "\n",
            "5. Enter ibm-storage-fusion-cp-sc StorageClass that is configured by default in IBM Fusion HCI, and click Install.\n",
            "\n",
            "6. Click Install.\n",
            "\n",
            "Figure 3-1 Installing Red Hat OpenShift Data Foundation\n",
            "\n",
            "Create StorageSystem for Red Hat OpenShift Data Foundation:\n",
            "\n",
            "1. Ensure that you select the Installation Mode as A specific namespace on the cluster and click Install.\n",
            "\n",
            "2. Click Create StorageSystem for Red Hat OpenShift Data Foundation.\n",
            "\n",
            "3. In the Deployment type field, select MultiCloud Object Gateway. For more information about MCG deployment, see Deploy stand-alone Multicloud Object Gateway.\n",
            "\n",
            "Figure 3-2 Deployment type selection in Create StorageSystem\n",
            "\n",
            "## 3.2.1 Configuring Advanced File Management nodes\n",
            "\n",
            "Configure Storage Acceleration on IBM Fusion HCI to connect your remote object bucket to the Storage Scale Advanced File Management (AFM) accelerator and then expose an accelerated bucket through the MCG. This workflow has two steps:\n",
            "\n",
            "1. Create a Storage Scale AFM fileset that is connected to the remote S3 endpoint.\n",
            "\n",
            "2. Connect the MCG to the local Scale AFM fileset.\n",
            "\n",
            "After you complete the configuration, you can access the AFM cached remote S3 data.\n",
            "\n",
            "## Prerequisites\n",
            "\n",
            "This list describes the tools and environment prerequisites:\n",
            "\n",
            "Ensure that the Fusion HCI rack has AFM nodes installed, and ensure they are configured to be part of the Red Hat OpenShift cluster.\n",
            "\n",
            "The oc command is used to issue commands to the Red Hat OpenShift Container Platform cluster.\n",
            "\n",
            "Noobaa is installed as a part of MCG. It is used to access the Red Hat OpenShift Data Foundation MCG, and the AWS/S3 CLI is used to access the noobaa API endpoint. If the installation is not available, see noobaa-operator.\n",
            "\n",
            "Check whether you can log in to the Red Hat OpenShift Container Platform cluster.\n",
            "\n",
            "Retrieve the values for bucket name, access key, secret key, and S3 endpoint for the object bucket that you want to accelerate. The remote S3 can be IBM Storage Ceph for an on-premises environment. The S3 can be a cloud provider, such as IBM Cloud Object Storage (COS) or AWS S3. For more information, see IBM Storage Ceph and IBM Redpaper: IBM Storage Ceph Solutions Guide, REDP-5715.\n",
            "\n",
            "## Configuring the AFM nodes\n",
            "\n",
            "Before you configure the AFM nodes, collect the following information:\n",
            "\n",
            "Endpoint as REMOTE_S3_ENDPOINT\n",
            "\n",
            "Access Key as REMOTE_S3_ACCESS_KEY\n",
            "\n",
            "Access Secret as REMOTE_S3_ACCESS_SECRET\n",
            "\n",
            "Bucket Name as REMOTE_S3_BUCKET\n",
            "\n",
            "## Createing an AFM fileset and connecting to a remote S3 bucket\n",
            "\n",
            "1. Get the Scale core pod name on AFM node:\n",
            "\n",
            "AFM_NODE_POD_NAME=$(oc get node-l scale.spectrum.ibm.com/role=afm-o json | jq-r '.items[0].metadata.name' | awk-F '.' '{print $1}')\n",
            "\n",
            "2. Go into the Scale core pod:\n",
            "\n",
            "oc exec-it $AFM_NODE_POD_NAME bash-n ibm-spectrum-scale\n",
            "\n",
            "3. Input REMOTE_S3_ENDPOINT, REMOTE_S3_BUCKET, REMOTE_S3_ACCESS_KEY, and REMOTE_S3_ACCESS_SECRET :\n",
            "\n",
            "-REMOTE_S3_ENDPOINT=http://s3.us-south.cloud-object-storage.appdomain.cloud\n",
            "\n",
            "-REMOTE_S3_BUCKET=afm-s3test\n",
            "\n",
            "-REMOTE_S3_ACCESS_KEY=e48acxxxx750\n",
            "\n",
            "-REMOTE_S3_ACCESS_SECRET=8ffc2xxxxx1e22d\n",
            "\n",
            "4. Create an access key:\n",
            "\n",
            "mmafmcoskeys $REMOTE_S3_BUCKET set $REMOTE_S3_ACCESS_KEY $REMOTE_S3_ACCESS_SECRET\n",
            "\n",
            "5. Input the AFM fileset, endpoint, bucket, and afm_mode.\n",
            "\n",
            "6. Create an AFM fileset, endpoint, bucket, and afm_mode : For example, create an AFM fileset, and the AFM node is in mode *lu*. See Example 3-1.\n",
            "\n",
            "## Example 3-1 Creating an AFM fileset\n",
            "************************* Chunk **********************************\n",
            "*************************** 5312 *********************************\n",
            "## Example 3-1 Creating an AFM fileset\n",
            "\n",
            "FILE_SYSTEM=ibmspectrum-fs fileset=afm-cos-s3-fileset AFM_MODE=lu mmafmcosconfig $FILE_SYSTEM $fileset--endpoint $REMOTE_S3_ENDPOINT--object-fs--bucket $REMOTE_S3_BUCKET--cleanup--debug--mode $AFM_MODE--tmpdir.noobaa% mmchfileset $FILE_SYSTEM $fileset-p afmPrefetchThreshold=100 mmafmcosctl $FILE_SYSTEM $fileset /mnt/${FILE_SYSTEM}/${fileset} download--all--metadata\n",
            "\n",
            "The following list describes the available modes and their purposes:\n",
            "\n",
            "Independent Writer (IW) is for changes made from the cache and server. This option must be configured when you are setting up both read and write cache. As you set the accelerator on top of the object bucket, the accelerator works both as a read and write cache for the object. It is the default setting.\n",
            "\n",
            "Local Update (LU) is for changes that are made on only the server. In this mode, you can use it for testing of your model. You do not want the changes you are making to go to the backend object bucket. After the test is complete, you can change to the IW mode.\n",
            "\n",
            "Single Writer (SW) is for changes made only from cache. In this mode, only the cache fileset does all the writing and the cache does not check home for file or directory updates.\n",
            "\n",
            "## Performance tuning\n",
            "\n",
            "Do configuration settings for performance tuning. For more information about tuning configuration, see 3.2.3, 'Performance tuning' on page 27.\n",
            "\n",
            "## Evicting cache data manually\n",
            "\n",
            "You can evict cache manually, or you can control eviction by defining quotas.\n",
            "\n",
            "Perform the following steps to evict cache manually:\n",
            "\n",
            "Evict all cache data manually:\n",
            "\n",
            "mmafmcosctl fs1 fileset1 /gpfs/fs1/new1 evict-all\n",
            "\n",
            "Evict all cache data and metadata: mmafmcosctl fs1 fileset1 /gpfs/fs1/new1 evict-all--metadata\n",
            "\n",
            "Manual eviction after quota limit is set. Evict data by using a criteria:\n",
            "\n",
            "mmafmctl fs1 evict-j fileset1--order LRU mmafmctl fs1 evict-j fileset1--order Size\n",
            "\n",
            "## Evicting cache by using quota enabled eviction\n",
            "\n",
            "Eviction can also be automatically controlled by using quotas. After the soft quota is exceeded, AFM automatically evicts the files based on LUR fashion. If a policy is not set for eviction, after the limit is reached, then the requests will fail with no space return error code. For more information, see Enabling quotas.\n",
            "\n",
            "In this example, set the soft quota to be 1 TB and the hard quota to be 2 TB. Adjust the values based on the PV/PVC size that you plan to create.\n",
            "\n",
            "mmsetquota $FILE_SYSTEM:$fileset--block 1000G:2000G\n",
            "\n",
            "Use either mlsquota or mmrepquota to view your quotas.\n",
            "\n",
            "## 3.2.2 Creating the static PV and PVC\n",
            "\n",
            "The steps in this section describe how to create the static PV and PVC.\n",
            "\n",
            "## Defining the variables\n",
            "\n",
            "Assign the scale cluster ID to the variable CLUSTER_ID : CLUSTER_ID=$(oc exec $AFM_NODE_POD_NAME-n ibm-spectrum-scale 'mmlscluster ' | grep 'GPFS cluster id: ' | awk '{print $4}')\n",
            "\n",
            "Assign the file system ID to the variable FILESYSTEM_ID :\n",
            "\n",
            "FILESYSTEM_ID=$(oc exec $AFM_NODE_POD_NAME-n ibm-spectrum-scale--bash-c 'mmlsfs ibmspectrum-fs--uid' | grep 'uid ' | awk '{print $2}')\n",
            "\n",
            "## Updating and applying the YAML template\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Update the YAML template for PV name, capacity, volumeHandle and PVC name, as shown in Example 3-2.\n",
            "\n",
            "## Example 3-2 Updating the YAML template\n",
            "\n",
            "apiVersion: v1 kind: PersistentVolume metadata: name: {{PV_Name}} spec: accessModes:-ReadWriteMany capacity: storage: {{Capacity}} csi: driver: spectrumscale.csi.ibm.com volumeHandle: 0;2;{{CLUSTER_ID}};{{FILESYSTEM_ID}};;{{fileset}};/mnt/ibmspectrum-fs/{{fileset}} persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem--apiVersion: v1 kind: PersistentVolumeClaim metadata: name: {{PVC_Name}} namespace: openshift-storage spec: accessModes:-ReadWriteMany resources: requests: storage: {{Capacity}} storageClassName: ' ' volumeMode: Filesystem volumeName: {{PV_Name}}\n",
            "\n",
            "Example 3-3 is an example YAML file is an example using PV name, capacity, volumeHandle and PVC name. You will need to use your naming conventions for your organization.\n",
            "\n",
            "## Example 3-3 Example of updating the YAML template\n",
            "\n",
            "Example yaml as file pv_pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: afm-cos-s3-remote-pv spec: accessModes:-ReadWriteMany capacity:\n",
            "\n",
            "storage: 1000Gi csi: driver: spectrumscale.csi.ibm.com volumeHandle: 0;2;6734170828145876673;1180A8C0:646EEE59;;afm-cos-s3-fileset;/mnt/ibmspectrum-fs/ afm-cos-s3-fileset persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem--apiVersion: v1 kind: PersistentVolumeClaim metadata: name: afm-cos-s3-remote-pvc namespace: openshift-storage spec: accessModes:-ReadWriteMany resources: requests: storage: 1000Gi storageClassName: ' ' volumeMode: Filesystem volumeName: afm-cos-s3-remote-pv\n",
            "\n",
            "2. Apply the yaml to create static PV and PVC. The following command is an example:\n",
            "\n",
            "oc apply-f pv_pvc.yaml\n",
            "\n",
            "## Configuring the MCG bucket\n",
            "\n",
            "Configure an MCG bucket that uses the PVC to access the AFM cache.\n",
            "\n",
            "As a prerequisite, install ODF operator and create an ODF cluster with MCG only mode.\n",
            "\n",
            "1. Create a NamespaceStore with the PVC, as shown in Example 3-4.\n",
            "\n",
            "## Example 3-4 Creating a NamespaceStore with the PVC\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: NamespaceStore metadata: name: {{NSS_Name}} namespace: openshift-storage spec: nsfs: fsBackend: GPFS pvcName: {{PVC_Name}} subPath: ' ' type: nsfs\n",
            "************************* Chunk **********************************\n",
            "*************************** 4147 *********************************\n",
            "2. Create a bucket class with the NamespaceStore, as shown in Example 3-5.\n",
            "\n",
            "## Example 3-5 Creating a bucket class with the NamespaceStore\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: name: {{Bucket_Class_Name}} namespace: openshift-storage spec: namespacePolicy: single: resource: {{NSS_Name}} type: Single\n",
            "\n",
            "3. Create a Noobaa Account for the NamespaceStore, as shown in Example 3-6.\n",
            "\n",
            "Example 3-6 Creating a Noobaa Account for the NamespaceStore\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: NooBaaAccount metadata: name: {{Noobaa_Account_Name}} namespace: openshift-storage spec: allow_bucket_creation: true default_resource: {{NSS_Name}} nsfs_account_config: gid: 0 new_buckets_path: / nsfs_only: true uid: 0\n",
            "\n",
            "4. Create an ObjectBucketClaim with the BucketClass as shown in Example 3-7.\n",
            "\n",
            "Example 3-7 Creating an ObjectBucketClaim with the BucketClass\n",
            "\n",
            "apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: {{Object_Bucket_Claim_Name}} namespace: openshift-storage spec: additionalConfig: bucketclass: {{Bucket_Class_Name}} bucketName: {{Bucket_Name}} storageClassName: openshift-storage.noobaa.io\n",
            "\n",
            "Example 3-8 is an example of creating an ObjectBucketClaim with the BucketClass. You will need to use your organizations naming conventions.\n",
            "\n",
            "Example 3-8 Example showing YAML creating an ObjectBucketClaim with the BucketClass\n",
            "\n",
            "Example YAML: apiVersion: noobaa.io/v1alpha1 kind: NamespaceStore metadata: name: afm-cos-s3-nss\n",
            "\n",
            "namespace: openshift-storage spec: nsfs: fsBackend: GPFS pvcName: afm-cos-s3-remote-pvc subPath: data type: nsfs--apiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: name: afm-cos-s3-bc namespace: openshift-storage spec: namespacePolicy: single: resource: afm-cos-s3-nss type: Single--apiVersion: noobaa.io/v1alpha1 kind: NooBaaAccount metadata: name: afm-cos-s3-acc namespace: openshift-storage spec: allow_bucket_creation: true default_resource: afm-cos-s3-nss nsfs_account_config: gid: 0In e new_buckets_path: / nsfs_only: true uid: 0--apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: afm-cos-s3-obc namespace: openshift-storage spec: additionalConfig: bucketclass: afm-cos-s3-bc bucketName: afm-cos-s3-nss-bc storageClassName: openshift-storage.noobaa.io\n",
            "\n",
            "5. Apply the yaml file from Example 3-8 on page 24 to create resources.\n",
            "\n",
            "## Updating the bucket access policy\n",
            "\n",
            "You need the noobaa admin account to update the bucket access policy.\n",
            "\n",
            "1. Get AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output: noobaa status--show-secrets\n",
            "\n",
            "2. Create the policy.json file with the content in Example 3-9.\n",
            "\n",
            "Example 3-9 Creating the policy.json file\n",
            "\n",
            "{ 'Version ': '2012-10-17 ', 'Statement ':[{ 'Sid ': 'id-1 ', 'Effect ': 'Allow ', 'Principal ': '* ', 'Action ':['s3:* '], 'Resource ':['arn:aws:s3:::* '] }] }\n",
            "\n",
            "3. Add a bucket policy (Example 3-10).\n",
            "\n",
            "Replace the bucket name with the bucket name that you defined in the previous steps.\n",
            "\n",
            "## Example 3-10 Adding a bucket policy\n",
            "\n",
            "ObjectBucketClaim.Spec.bucketName field BUCKET_NAME=afm-cos-s3-nss-bc S3_ENDPOINT=https://$(oc get route s3-n openshift-storage-o json | jq-r '.status.ingress[0].host')\n",
            "\n",
            "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY aws--endpoint-url=$S3_ENDPOINT--no-verify-ssl s3api put-bucket-policy--bucket $BUCKET_NAME--policy file://policy.json\n",
            "\n",
            "4. Verify the bucket:\n",
            "\n",
            "BUCKET_NAME=afm-cos-s3-nss-bc\n",
            "\n",
            "a. Get the noobaa account. The noobaa account name that was defined in the previous steps.\n",
            "\n",
            "b. Get the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output:\n",
            "\n",
            "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY aws--endpoint $S3_ENDPOINT\n",
            "\n",
            "NoobaaAccount.metadata.name NOOBAA_ACCOUNT_NAME=afm-cos-s3-acc noobaa account status $NOOBAA_ACCOUNT_NAME--show-secrets b. Get the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output: AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID--no-verify-ssl s3api list-objects--bucket $BUCKET_NAME\n",
            "\n",
            "## 3.2.3 Performance tuning\n",
            "\n",
            "You defined a storage accelerated bucket that can be accessed by workloads such as watsonx.data. The following list describes the default parameters that can be changed or disabled to improve performance:\n",
            "************************* Chunk **********************************\n",
            "*************************** 5288 *********************************\n",
            "| afmFileLookupRefreshInterval   | Defines the frequency of revalidation that is triggered by a  look-up operation on a file such as  ls  or  stat , from the  IBM Fusion HCI. AFM sends a message to the external  object bucket to determine that the metadata of the file is                                                                                                                                                                |\n",
            "|--------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| afmFileOpenRefreshInterval     | IBM Fusion HCI. Defines the frequency of revalidations that are triggered by  the read and write operations on a file from the IBM Fusion  HCI. AFM sends a message to the external object bucket  to determine if the metadata of the file was modified since  it was last revalidated.                                                                                                                    |\n",
            "| afmDirLookupRefreshInterval    | Defines the frequency of revalidation that is triggered by a  look-up operation such as  ls  or  stat  on a directory from  the IBM Fusion HCI. AFM sends a message to the external  object bucket to find out whether the metadata of that  directory is modified since it was last revalidated. If so, the  latest metadata information at the external object bucket is                                  |\n",
            "| afmDirOpenRefreshInterval      | Defines the frequency of revalidations that are triggered by  the read and update operations on a directory from the  IBM Fusion HCI. AFM sends a message to the external  object bucket to find whether the metadata of that directory  is modified since it was last revalidated.                                                                                                                         |\n",
            "| afmObjectFastReaddir           | Improves the objects download and readdir performance,  when the  afmObjectFastReaddir  parameter value is set to  yes  at the fileset level. Extended attributes and ACLs are  not fetched from a cloud object storage when this  parameter is enabled. Also, deleted objects on a cloud  object storage system are not reflected immediately on a  cache when this parameter is enabled. You can use this |\n",
            "| afmParallelReadChunkSize       | Defines the minimum chunk size of the read that needs to  be distributed among the gateway nodes during parallel  reads. A zero (0) value disables the parallel reads across  multiple gateways. The parallel reads are routed through a  single gateway node.                                                                                                                                              |\n",
            "| afmObjectFastReaddir           | Improves the objects download and readdir performance,  afmObjectFastReaddir  parameter value is set to  'yes' at the fileset level. Extended attributes and ACLs are                                                                                                                                                                                                                                       |\n",
            "|                                | when the  not fetched from a cloud object storage when this  parameter is enabled. Also, deleted objects on a cloud  object storage system are not reflected immediately on a                                                                                                                                                                                                                               |\n",
            "\n",
            "## Data not changing on the server\n",
            "\n",
            "When you configure an accelerator or caching for an object bucket, there might not be any changes to the data on the object bucket.\n",
            "\n",
            "Changes to the applications go to the object bucket through the accelerator. If there is no possibility of anybody making updates to the object bucket without the accelerator, then you can disable the refresh intervals.\n",
            "\n",
            "For example, a Ceph object bucket exists, and the accelerator is placed on top of it. Applications in turn run on top of the accelerator.\n",
            "\n",
            "If no changes are happening on the object bucket outside the accelerator path, then use the following configuration parameters:\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmFileLookupRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmFileOpenRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmDirLookupRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmDirOpenRefreshInterval=disable\n",
            "\n",
            "## Data might change on the server\n",
            "\n",
            "When the data can change on the server, then you can disable the file lookup interval without disabling the directory lookup. You can you can increase the revalidation time from the default of 60 seconds.\n",
            "\n",
            "Enter the following commands to alter the frequency of revalidation:\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmObjectFastReaddir=yes mmchfileset $FILE_SYSTEM $fileset-p afmFileLookupRefreshInterval=disable\n",
            "************************* Chunk **********************************\n",
            "*************************** 5207 *********************************\n",
            "## Size of the download from each Gateway\n",
            "\n",
            "AFM can read from multiple gateways at the same time. If the amount of date that is read is greater than a defined number, then parallel reads begin. For example, if 12 MB is the chunk size, then each gateway reads 12 MB and then pass the data to the main gateway to process the data.\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmParallelReadChunkSize=12M\n",
            "\n",
            "## How it improves performance\n",
            "\n",
            "Parallel read data transfer improves the overall data read transfer performance of an AFM to cloud object storage fileset by using multiple gateway nodes.\n",
            "\n",
            "## Multiple gateways\n",
            "\n",
            "For filesets with mode LU or RO, you can use multiple gateways for better performance. Configure more gateways with the following commands:\n",
            "\n",
            "mmchnode-gateway-N node mm chfileset fs fileset-name-p afmGateway=all\n",
            "\n",
            "To activate afmGateway=all, stop and restart the fileset by using the following commands:\n",
            "\n",
            "mmafmctl perffs stop-j db2wh-db2u-perf-test-2-cos mmchfileset perffs db2wh-db2u-perf-test-2-cos-p afmGateway=all mmafmctl perffs start-j db2wh-db2u-perf-test-2-cos\n",
            "\n",
            "For the other modes, such as IW and SW, use the mmafmconfig command:\n",
            "\n",
            "mmafmconfig {add | update} MapName--export-map ExportServerMap [--no-server-resolution]\n",
            "\n",
            "The following command is a sample of the mmafmconfig command:\n",
            "\n",
            "mmafmconfig add mymap--export-map 169.46.118.100/fin37.ibm.com,10.242.33.16/fin38.ibm.com\n",
            "\n",
            "The 2nd IP address in each pair is the gateway, and fin37.ibm.com and fin38.ibm.com are the addresses of the gateways.\n",
            "\n",
            "## Populating the metadata cache\n",
            "\n",
            "If you want to pre-fetch your metadata cache, you have several options.\n",
            "\n",
            "The mmchfileset command retrieves the name entries the fastest from listv2 with minimum attributes. You can use the ls command to view the metadata. Use the following format for the command:\n",
            "\n",
            "mmchfileset device filesetname-p afmObjectFastReaddir=yes\n",
            "\n",
            "The mmafmcosctl command is not as fast as the mmchfileset command. The command bypasses the gateway and retrieves the names and full attributes:\n",
            "\n",
            "mmafmcosctl device filesetName path download--metadata--outband\n",
            "\n",
            "The mmafmcosctl is slower than the other commands. It uses the gateway and retrieves the names and full attributes:\n",
            "\n",
            "mmafmcosctl device filesetame path download--metadata\n",
            "\n",
            "## 3.3 Installing IBM watsonx.data on IBM Fusion HCI\n",
            "\n",
            "Complete the following steps to install IBM watsonx.data.\n",
            "\n",
            "1. Install IBM watsonx.data. For the procedure to install, see Installing wastsonx.data.\n",
            "\n",
            "2. To configure IBM watsonx.data for IBM Fusion HCI storage, create a storage class with the appropriate settings for use with IBM watsonx.data. For the actual procedure to configure, see Setting up IBM Storage Scale storage.\n",
            "\n",
            "3. Create a watsonx.data instance. The watsonx.data operator is installed one time on the cluster and shared by many instances of watsonx.data on the cluster.\n",
            "\n",
            "## Creating an accelerated bucket\n",
            "\n",
            "As a prerequisite, create an AFM fileset and attach it to a bucket. Use the following procedure to create an accelerated bucket and connect to an existing externally managed object storage (Multicloud Gateway):\n",
            "\n",
            "1. Log in to watsonx.data console.\n",
            "\n",
            "2. From the navigation menu, select Infrastructure Manager.\n",
            "\n",
            "3. To define and connect a bucket, click Add component and select Add bucket.\n",
            "\n",
            "4. In the Add bucket window, provide the following details to connect to the accelerated bucket provided by MCG:\n",
            "\n",
            "Note: Refer to the values that you set during the accelerated bucket creation for the Bucket name, endpoint, access key, and secret key.\n",
            "\n",
            "-Bucket type. Select Ceph as the value for bucket type from list.\n",
            "\n",
            "-Bucket name. Enter the name of your existing bucket.\n",
            "\n",
            "-Endpoint. Enter the endpoint URL.\n",
            "\n",
            "-Access key. Enter your access key.\n",
            "\n",
            "-Secret key. Enter your secret key.\n",
            "\n",
            "-Activation. Activate the bucket immediately or activate it later.\n",
            "\n",
            "-Catalog type. Select the catalog type from the list.\n",
            "\n",
            "-Catalog name. Enter the name of the catalog. The catalog is automatically associated with your bucket.\n",
            "\n",
            "To add a bucket-catalog pair, see Adding a bucket-catalog pair.\n",
            "\n",
            "Chapter 4.\n",
            "\n",
            "## Monitoring\n",
            "\n",
            "The IBM Fusion HCI is like an Red Hat OpenShift cluster in a box. With the available default Red Hat OpenShift monitoring options, you can monitor the watsonx.data project or namespace. In addition, the IBM Fusion HCI has its own monitoring and logging capabilities to view different dashboards for storage, networking, and compute. For more information, see Monitoring and logging.\n",
            "\n",
            "However, for monitoring watsonx.data specifically, use the built-in Presto engine web interface for monitoring and managing queries. This web interface is accessible by an Red Hat OpenShift route to the coordinator pod. For more information, see Exposing secure route to Presto server.\n",
            "\n",
            "After the route has been exposed, you can open a web browser to the route's URL. The main page has a list of queries and includes information such as unique query ID, query text, query state, percentage completed, username, and source from which this query originated. The currently running queries are at the top of the page, followed by the most recently completed or failed queries.\n",
            "\n",
            "Figure 4-1 shows an example of the main page.\n",
            "\n",
            "Figure 4-1 An example of the main page\n",
            "************************* Chunk **********************************\n",
            "*************************** 5276 *********************************\n",
            "Figure 4-1 shows an example of the main page.\n",
            "\n",
            "Figure 4-1 An example of the main page\n",
            "\n",
            "For more detailed information about a query, click the query ID link. The query detail page has a summary section, graphical representation of various stages of the query and a list of tasks. Each task ID can be clicked to get more information about that task. For example, when you click the task ID, you see a page similar to Figure 4-2.\n",
            "\n",
            "Figure 4-2 An example of the detailed view of an example task ID\n",
            "\n",
            "Chapter 5.\n",
            "\n",
            "## Backup and restore of IBM Cloud Pak for Data\n",
            "\n",
            "This chapter describes backing up and restoring Cloud Pak for Data with IBM watsonx.data on IBM Fusion. It covers the non-disruptive backup of Cloud Pak for Data on a Fusion HCI and restore to an alternative Fusion HCI. The process includes setting up a backup location, creating backup policies on the Fusion hub system, and applying the policies on to the Cloud Pak for Data application that is deployed.\n",
            "\n",
            "## 5.1 Considerations and requirements\n",
            "\n",
            "The example includes two IBM Fusion HCI racks running Version 2.6.1 that are connected as hub and spoke. Supported versions of Cloud Pak for Data and watsonx.data service are installed on the hub system as the source for backup.\n",
            "\n",
            "The following software and configurations are required when backing up your Cloud Pak for Data environment:\n",
            "\n",
            "IBM Fusion HCI 2.6.1 or later.\n",
            "\n",
            "Cloud Pak for Data 4.7.1 or later.\n",
            "\n",
            "Red Hat OpenShift Container Platform versions must be at the same major version on both source and target clusters. For example, IBM Fusion HCI 2.6.1 supports OpenShift Container Platform 4.10 and 4.12 and both source and target clusters must be at the same major version.\n",
            "\n",
            "Cloud Pak for Data and its services at the same release level.\n",
            "\n",
            "A supported version of Cloud Pak for Data is installed in private topology, and each Cloud Pak for Data instance includes the following components:\n",
            "\n",
            "-Two namespaces:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "cpd-instance\n",
            "\n",
            "-One shared cluster components: ibm-scheduler (optional scheduling service)\n",
            "\n",
            "With IBM Fusion 2.6.1, Backup & Restore should be configured across two Fusion HCI clusters with one of the clusters acting as the hub. The hub controls the backup and restore flow with one or more clusters that are connected to the hub as spokes. This setup allows for backups that are taken in one cluster to be restored in a different cluster.\n",
            "\n",
            "Cloud Pak for Data along with IBM watsonx.data can be installed on either the hub cluster or the spoke cluster.\n",
            "\n",
            "## 5.2 Getting the prerequisites ready\n",
            "\n",
            "Install more components before backing up the Cloud Pak for Data environment.\n",
            "\n",
            "## 5.2.1 Installing the Cloud Pak Backup and Restore service\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Install the cpdbr-oadp service in the Cloud Pak for Data operators and Cloud Pak for Data shared namespaces of the cluster components, which include Cloud Pak for Data Scheduling service (if installed). Ignore the Scheduling service namespace if it is not installed. The cpdbr-oadp service must be installed on both the Hub and Spoke clusters.\n",
            "\n",
            "To install the service, prepare your Hub and Spoke clusters to use cpd-cli. For more information, see Cloud Pak for Data command-line interface CPD.\n",
            "\n",
            "2. Install the cpdbr-oadp service in the following namespaces:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "cpd-scheduler\n",
            "\n",
            "For more information, see Installing the cpdbr service for IBM Fusion integration.\n",
            "\n",
            "3. On the source cluster, install the cpdbr-oadp service by issuing the following command:\n",
            "\n",
            "./cpd-cli oadp install--component=cpdbr-tenant--tenant-operator-namespace=<cpd-operator_ns>--cpdbr-hooks-image-prefix=quay.io/cpdsre--cpd-scheduler-namespace=cpd-scheduler--log-level=debug--verbose\n",
            "\n",
            "4. After installation is done, verify that the cpdbr pod is deployed by running the following command:\n",
            "\n",
            "oc get pods-A| grep cpdbr The following line is an example of the expected output: cpd-operator cpdbr-tenant-service-6dcc49464c-rr9jh\n",
            "\n",
            "The installation of cpdbr-oadp also installs, generates, and applies the required recipes in the respective Cloud Pak for Data cpd-operator and cpd-scheduler namespaces (if installed). To verify, issue the command oc get frcpe-A, as shown in Example 5-1.\n",
            "\n",
            "## Example 5-1 Example output\n",
            "\n",
            "| oc get frcpe -A        | oc get frcpe -A            | oc get frcpe -A   |\n",
            "|------------------------|----------------------------|-------------------|\n",
            "| NAMESPACE              | NAME                       | AGE               |\n",
            "| cpd-operator           | ibmcpd-tenant              | 2m6s              |\n",
            "| ibm-spectrum-fusion-ns | fusion-control-plane 3d23h |                   |\n",
            "| ibm-spectrum-fusion-ns | fusion-cr-backup           | 10d               |\n",
            "\n",
            "## 5.2.2 Installing the cpdbr service on the target cluster\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Install the cpdbr-tenant service on the target cluster, as shown in Example 5-2.\n",
            "\n",
            "Example 5-2 Installing cpdbr-tenant service on the target cluster\n",
            "\n",
            "$ cpd-cli oadp install--component=cpdbr-tenant--tenant-operator-namespace=cpd-operator processing request... cpd tenant operator namespace: cpd-operator clusterrole/cpdbr-tenant-service-clusterrole created clusterrolebinding/cpdbr-tenant-service-crb created role/cpdbr-tenant-service-role created in namespace kube-public rolebinding/cpdbr-tenant-service-rb created in namespace kube-public\n",
            "************************* Chunk **********************************\n",
            "*************************** 3560 *********************************\n",
            "2. After the installation is done, verify that ClusterRoleBinding was created, as shown in Example 5-3.\n",
            "\n",
            "## Example 5-3 Verifying that ClusterRoleBinding was created\n",
            "\n",
            "$ oc get clusterrolebinding cpdbr-tenant-service-crb NAME ROLE AGE cpdbr-tenant-service-crb ClusterRole/cpdbr-tenant-service-clusterrole 37s $ oc get clusterrole cpdbr-tenant-service-clusterrole NAME CREATED AT cpdbr-tenant-service-clusterrole 2023-08-25T17:45:55Z $ oc get clusterrolebinding | grep cpdbr cpdbr-tenant-service-crb\n",
            "\n",
            "## 5.2.3 Backup policies for Cloud Pak for Data applications\n",
            "\n",
            "Before you create and apply backup policies to the Cloud Pak for Data applications, you must create an S3 compliant backup location in the Fusion UI of the hub system. The hub system is used to store the backups and is the source when the data is restored.\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. To create a backup location, on IBM Fusion UI, select Backup & restore  Locations to add a backup location. In this setup, which is shown in Figure 5-1, we added an object storage from IBM Cloud as a backup location.\n",
            "\n",
            "Figure 5-1 Adding a backup location\n",
            "\n",
            "2. After creating the backup location, run the following command to list the backup locations that were created:\n",
            "\n",
            "$ oc get fbsl-n ibm-spectrum-fusion-ns NAME PROVIDER\n",
            "\n",
            "ibm-cloudd\n",
            "\n",
            "isf-backup-restore\n",
            "\n",
            "PHASE\n",
            "\n",
            "STORAGETYPE\n",
            "\n",
            "ibm\n",
            "\n",
            "Connected\n",
            "\n",
            "## 5.2.4 Creating and assigning a backup policy\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. On IBM Fusion UI, select Backup & restore  Policies to add a backup policy, as shown in Figure 5-2. A Backup Policy specifies how frequently backups are taken, where backups are stored, and how long backups are retained.\n",
            "\n",
            "Figure 5-2 Adding a backup policy\n",
            "\n",
            "To list backup policies from the CLI, run the oc get backuppolicies command:\n",
            "\n",
            "$oc get backuppolicies-n ibm-spectrum-fusion-ns NAME PROVIDER BACKUPSTORAGELOCATION SCHEDULE RETENTION RETENTIONUNIT cpd-oper-policy isf-backup-restore ibm-cloud 00 1 * * * 30 days\n",
            "\n",
            "2. Assign the backup policy to Cloud Pak for Data applications. From the IBM Fusion UI, select Backed up applications, and then open the Protect apps menu and select the cluster where Cloud Pak for Data is deployed. Select the following applications:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "ibm-scheduler (if installed)\n",
            "\n",
            "Figure 5-3 Verifying that the recipes are now associated to the corresponding policy assignments\n",
            "\n",
            "3. Examine the policies that are assigned by running the command in Example 5-4.\n",
            "\n",
            "## Example 5-4 Checking the policies that are assigned\n",
            "\n",
            "$ oc get policyassignments.data-protection.isf.ibm.com-n ibm-spectrum-fusion-ns | grep cpd-oper\n",
            "\n",
            "NAME PROVIDER APPLICATION BACKUPPOLICY RECIPE RECIPENAMESPACE PHASE LASTBACKUPTIMESTAMP CAPACITY\n",
            "\n",
            "cpd-operator-cpd-oper-policy-apps isf-backup-restorecpd-operator cpd-oper-policyibmcpd-tenant cpd-operator\n",
            "\n",
            "Assigned <no value>\n",
            "\n",
            "The recipes are not yet associated to the policy assignment. The recipes must be manually patched into the policies except for cpd-tenant, which is assigned automatically. If cpd-scheduler is assigned, the recipe must be patched. Example 5-5 shows patching cpd-scheduler.\n",
            "\n",
            "## Example 5-5 Patching the recipes into the policy assignment\n",
            "\n",
            "$ oc-n ibm-spectrum-fusion-ns patch policyassignment <cpd-scheduler-policy-assignment>--type merge-p '{ 'spec ':{ 'recipe ':{ 'name ': 'ibmcpd-scheduler ', 'namespace ': 'cpd-scheduler ', 'apiVersion ': 'spp-data-protection.isf.ibm.com/v1alpha1 '}}}'\n",
            "\n",
            "4. Check the policy assignments again. The recipes are now attached to the backup policy assignment in Example 5-6.\n",
            "\n",
            "## Example 5-6 Checking the policy assignments again\n",
            "************************* Chunk **********************************\n",
            "*************************** 5286 *********************************\n",
            "## Example 5-6 Checking the policy assignments again\n",
            "\n",
            "| $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper   |\n",
            "|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------|\n",
            "| NAME                                                                                               |                                                                                                    | Provider                                                                                           | APPLICATION                                                                                        | BACKUPPOLICY                                                                                       | RECIPE                                                                                             |\n",
            "| RECIPENAMESPACE                                                                                    | PHASE                                                                                              | LASTBACKUPTIMESTAMP                                                                                | CAPAC                                                                                              |                                                                                                    |                                                                                                    |\n",
            "| cpd-operator-cpd-oper-policy-apps                                                                  | cpd-operator-cpd-oper-policy-apps                                                                  | isf-backup-restore                                                                                 | cpd-operator                                                                                       | cpd-oper-policy                                                                                    | cpd-oper-policy                                                                                    |\n",
            "| ibmcpd-tenant                                                                                      | cpd-operator                                                                                       | Assigned                                                                                           | <no value>                                                                                         | <no value>                                                                                         | <no value>                                                                                         |\n",
            "\n",
            "## 5.3 Backing up the source cluster\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Take a backup on the hub by selecting Backed up applications  cpd-operator. Then, select Backup now  Backup.\n",
            "\n",
            "Important: The first backup of cpd-operator prepares follow-on backups to be valid for restore. Do not restore from the first backup. After the first backup is complete, take a second backup. The second backup, and all later backups, may be used for the restore.\n",
            "\n",
            "2. After all the backups are finished, go to the Backed up applications page to confirm that the backups finished successfully.\n",
            "\n",
            "## 5.4 Restoring to an alternative cluster\n",
            "\n",
            "Before restoring to an alternative cluster, ensure that the target cluster is prepared for Cloud Pak for Data and watsonx.data installation. For more information, see Preparing your cluster.\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. When the alternative cluster is ready, edit guardian-configmap in the ibm-backup-restore project by increasing the restoreDatamoverTimeout parameter value to 240 minutes.\n",
            "\n",
            "2. Then, select Backup & restore  Topology and verify that the hub and spoke are connected and in a healthy state.\n",
            "\n",
            "The next step is to install the certificate manager and the IBM License Service. For more information, see Installing shared cluster components for IBM Cloud Pak for Data.\n",
            "\n",
            "3. Change the logging level from default INFO to DEBUG by running the following command:\n",
            "\n",
            "oc patch cm-n ibm-backup-restore guardian-configmap-p='{ 'data ':{ 'logLevel ': 'DEBUG '}}'\n",
            "\n",
            "4. Next, restore cpd-operator by selecting Backed up applications  cpd-operator  Restore.\n",
            "************************* Chunk **********************************\n",
            "*************************** 1358 *********************************\n",
            "4. Next, restore cpd-operator by selecting Backed up applications  cpd-operator  Restore.\n",
            "\n",
            "5. Under Select a destination, click Choose a different cluster to restore the application in and then select the target cluster. Click Next.\n",
            "\n",
            "6. In the next window, select the backup that you want to use and then click Next.\n",
            "\n",
            "7. After the job is finished, verify the completion details by going to the Jobs section under Backup & restore.\n",
            "\n",
            "8. Repeat these steps for 'ibm-scheduler' (if installed). After the restore is finished for 'cpd-operator', and 'ibm-scheduler', confirm that the restore was successful by logging in to the Cloud Pak for Data user interface. Inspect the watsonx.data instance and all the previous data under the Instances page, as shown in Figure 5-4.\n",
            "\n",
            "Figure 5-4 Verification of the restored watsonx.data instance in the Cloud Pak for Data user interface\n",
            "\n",
            "Figure 5-5 shows that the watsonx.data test engine was restored successfully.\n",
            "\n",
            "Figure 5-5 The watsonx.data test engine was restored successfully\n",
            "\n",
            "Figure 5-6 shows the data that is restored from the 'Data manager' view.\n",
            "\n",
            "Figure 5-6 Data was restored from the 'Data manager' view\n",
            "\n",
            "9. Verify the restore by running oc get pods-n cpd-instance and ensuring that all pods are in a good state, as shown in Example 5-7.\n",
            "\n",
            "Example 5-7 Verifying the restore by running oc get pods-n cpd-instance\n",
            "************************* Chunk **********************************\n",
            "*************************** 5277 *********************************\n",
            "| $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE                  | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE   | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE   | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE   | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE   | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE   |\n",
            "|--------------------------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|-----------------------------------------------------------|\n",
            "| NAME                                                                     |                                                           |                                                           |                                                           |                                                           |                                                           |\n",
            "| create-secrets-job-nwt8q                                                 |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 8m35s                                                     |\n",
            "| ibm-lh-lakehouse-hive-metastore-696f8fb6dd-8ss85 1/1 Running 3 (16m ago) |                                                           |                                                           |                                                           |                                                           | 21m                                                       |\n",
            "| ibm-lh-lakehouse-minio-ff8f7b77f-h4n5r                                   |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| ibm-lh-lakehouse-presto-01-presto-0                                      |                                                           | 1/1 Running 2 (16m ago)                                   |                                                           |                                                           | 21m                                                       |\n",
            "| ibm-lh-lakehouse-presto543-presto-0                                      |                                                           | 1/1 Running 2 (16m ago)                                   |                                                           |                                                           | 21m                                                       |\n",
            "| ibm-lh-postgres-edb-2                                                    |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 26m                                                       |\n",
            "| ibm-lh-postgres-edb-3                                                    |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 25m                                                       |\n",
            "| ibm-lh-postgres-edb-4                                                    |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 24m                                                       |\n",
            "| ibm-lh-postgres-setup-job-8q6zf                                          |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 10m                                                       |\n",
            "| ibm-nginx-6995f698fd-9s9vv                                               |                                                           | 2/2 Running 0                                             |                                                           |                                                           | 19m                                                       |\n",
            "| ibm-nginx-6995f698fd-sgvq6                                               |                                                           | 2/2 Running 0                                             |                                                           |                                                           | 19m                                                       |\n",
            "************************* Chunk **********************************\n",
            "*************************** 5277 *********************************\n",
            "| ibm-nginx-tester-55588dd7b-pnjvx                                         |                                                           | 2/2 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| lhconsole-api-85f77cc57d-k4wk9                                           |                                                           | 1/1 Running 5 (18m ago)                                   |                                                           |                                                           | 21m                                                       |\n",
            "| lhconsole-api-85f77cc57d-vmzkv                                           |                                                           | 1/1 Running 5 (18m ago)                                   |                                                           |                                                           | 21m                                                       |\n",
            "| lhconsole-nodeclient-6bb7475775-7x4js                                    |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| lhconsole-ui-7c7dbb98d8-t8kpj                                            |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| usermgmt-6bf557c77c-2fsgg                                                |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 18m                                                       |\n",
            "| usermgmt-6bf557c77c-s5ql6                                                |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 18m                                                       |\n",
            "| usermgmt-ensure-tables-job-6vpp2                                         |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 7m20s                                                     |\n",
            "| zen-audit-67944bcc74-v2445                                               |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| zen-core-5f7786c596-bmr9n                                                |                                                           | 2/2 Running 3 (18m ago)                                   |                                                           |                                                           | 19m                                                       |\n",
            "| zen-core-5f7786c596-hzb8v                                                |                                                           | 2/2 Running 3 (18m ago)                                   |                                                           |                                                           | 19m                                                       |\n",
            "| zen-core-api-58f7f7664d-2thq8                                            |                                                           | 2/2 Running 0                                             |                                                           |                                                           | 19m                                                       |\n",
            "| zen-core-api-58f7f7664d-97fzz                                            |                                                           | 2/2 Running 0                                             |                                                           |                                                           | 19m                                                       |\n",
            "| zen-core-create-tables-job-x7qb7                                         |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 6m50s                                                     |\n",
            "************************* Chunk **********************************\n",
            "*************************** 4900 *********************************\n",
            "| zen-core-pre-requisite-job-qh5sg                                         |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 5m7s                                                      |\n",
            "| zen-metastore-edb-2                                                      |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 26m                                                       |\n",
            "| zen-metastore-edb-3                                                      |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 25m                                                       |\n",
            "| zen-minio-0                                                              |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| zen-minio-1                                                              |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| zen-minio-2                                                              |                                                           | 1/1 Running 0                                             |                                                           |                                                           | 21m                                                       |\n",
            "| zen-minio-create-buckets-job-ccmdv                                       |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 8m44s                                                     |\n",
            "| zen-pre-requisite-job-lxghr                                              |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 5m51s                                                     |\n",
            "| zen-validate-metastore-edb-connection-job-m979g 0/1 Completed 0          |                                                           |                                                           |                                                           |                                                           | 7m44s                                                     |\n",
            "| zen-watchdog-7f5dcd6789-zkcrl                                            |                                                           | 1/1 Running 5 (13m ago)                                   |                                                           |                                                           | 18m                                                       |\n",
            "| zen-watchdog-create-tables-job-kgz7q                                     |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 6m36s 3m33s                                               |\n",
            "| zen-watchdog-post-requisite-job-cgchv                                    |                                                           | 0/1 Completed 0                                           |                                                           |                                                           |                                                           |\n",
            "| zen-watchdog-pre-requisite-job-x6pn5                                     |                                                           | 0/1 Completed 0                                           |                                                           |                                                           | 3m51s                                                     |\n",
            "************************* Chunk **********************************\n",
            "*************************** 176 *********************************\n",
            "10.Run oc get pvc-n cpd-instance to ensure that each pvc is in a good state and bound, as shown in Example 5-8.\n",
            "\n",
            "Example 5-8 Ensuring that each pvc is in a good state and bound\n",
            "************************* Chunk **********************************\n",
            "*************************** 5151 *********************************\n",
            "|                                                                               | $ oc get pvc -n cpd-instance                                                  |                                                                               | STATUS VOLUME                                                                 |\n",
            "|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n",
            "| NAME CAPACITY                                                                 | ACCESS MODES STORAGECLASS                                                     |                                                                               | AGE                                                                           |\n",
            "| export-zen-minio-0                                                            | export-zen-minio-0 Bound                                                      |                                                                               | pvc-0b5e56d4-8dff-42fd-9dc9-bbf903365bfd                                      |\n",
            "| 10Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 21m                                                  |\n",
            "| export-zen-minio-1                                                            | export-zen-minio-1 Bound                                                      |                                                                               | pvc-3729cd73-6950-4fb3-ace6-d86389b50f5d                                      |\n",
            "| 10Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 21m                                                  |\n",
            "| export-zen-minio-2                                                            | export-zen-minio-2 Bound                                                      |                                                                               | pvc-b92377fb-a2d7-4973-983e-d04988fda54c                                      |\n",
            "| 10Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 21m                                                  |\n",
            "| ibm-lh-lakehouse-minio-pvc                                                    | ibm-lh-lakehouse-minio-pvc                                                    |                                                                               | Bound pvc-608134b5-0c6c-41b9-9537-ff5e0878851d                                |\n",
            "| 488284Mi                                                                      | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 43m                                                  |\n",
            "| ibm-lh-postgres-edb-2                                                         | ibm-lh-postgres-edb-2 Bound                                                   |                                                                               | pvc-adf13e39-14af-4b55-866f-27e3184a6157                                      |\n",
            "| 9540Mi                                                                        | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 43m                                                  |\n",
            "| ibm-lh-postgres-edb-3                                                         | Bound                                                                         |                                                                               | pvc-cf179bc5-38e0-49f9-bc16-b535d31b0f00                                      |\n",
            "| 9765625Ki                                                                     | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 26m                                                  |\n",
            "| ibm-lh-postgres-edb-4                                                         | ibm-lh-postgres-edb-4 Bound                                                   |                                                                               | pvc-be3b1749-7f31-487b-b60e-aa4f733adb65                                      |\n",
            "************************* Chunk **********************************\n",
            "*************************** 2253 *********************************\n",
            "| 9765625Ki                                                                     | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 25m                                                  |\n",
            "| ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be |\n",
            "| 20Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 43m                                                  |\n",
            "| zen-metastore-edb-2                                                           |                                                                               |                                                                               | Bound pvc-88f1dd8e-3534-4ee3-82b1-3ef2b5904f25                                |\n",
            "| 10Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 43m                                                  |\n",
            "| zen-metastore-edb-3                                                           |                                                                               |                                                                               | Bound pvc-7d9498fc-3c17-456d-b45d-90576b1d8b0d                                |\n",
            "| 10Gi                                                                          | RWO                                                                           |                                                                               | ibm-storage-fusion-cp-sc 26m                                                  |\n",
            "************************* Chunk **********************************\n",
            "*************************** 2871 *********************************\n",
            "11.Run oc get catalogsource-n cpd-operator and oc get pods-n cpd-operator to ensure that the Cloud Pak for Data operators are in a good state, as shown in Example 5-9.\n",
            "\n",
            "Example 5-9 Ensuring that the Cloud Pak for Data operator is in a good state\n",
            "\n",
            "|                                                                     | $ oc get catalogsource -n cpd-operator             |\n",
            "|---------------------------------------------------------------------|----------------------------------------------------|\n",
            "| NAME                                                                | DISPLAY                                            |\n",
            "| TYPE PUBLISHER                                                      | AGE                                                |\n",
            "| cloud-native-postgresql-catalog                                     |                                                    |\n",
            "|                                                                     | ibm-cloud-native-postgresql-4.14.0+20230616.111503 |\n",
            "| grpc IBM                                                            | 39m                                                |\n",
            "| ibm-watsonx-data-catalog                                            |                                                    |\n",
            "| ibm-watsonx-data-1.0.2+20230816.142123.1192-linux-amd64 grpc IBM    |                                                    |\n",
            "| IBM                                                                 | opencloud-operators ibm-cp-common-services-4.1.0   |\n",
            "| grpc                                                                | 37m                                                |\n",
            "| $ oc get pods -n cpd-operator                                       |                                                    |\n",
            "| NAME READY STATUS                                                   |                                                    |\n",
            "| 28347d5b35b4a7e67ebbadc34bae6a27cf624ee1ec0388b16aa215aa76mjpbk 0/1 |                                                    |\n",
            "| Completed                                                           | 0 37m                                              |\n",
            "| 457c18b305fb59f54375ecc9faa6f530db9ca6c2f2adbf6a4c2d831673shjq5 0/1 | 36m                                                |\n",
            "| Completed                                                           | 0                                                  |\n",
            "| 82a84c3f4c0679f4c09e1261e671982f2405935dd8a66d50738940740dclj48 0/1 | 31m                                                |\n",
            "| 8ee3408bcdd092e94f0be116278a44a1007c5fa49b13bcdbfbc1c6f002r7p57 0/1 |                                                    |\n",
            "|                                                                     | 36m                                                |\n",
            "| Completed                                                           | 0                                                  |\n",
            "************************* Chunk **********************************\n",
            "*************************** 5279 *********************************\n",
            "| 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k   | 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k   | 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k   | 0/1         |\n",
            "|-------------------------------------------------------------------|-------------------------------------------------------------------|-------------------------------------------------------------------|-------------|\n",
            "| Completed 0                                                       |                                                                   | 35m                                                               |             |\n",
            "| b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc   | b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc   | b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc   | 0/1         |\n",
            "| Completed 0                                                       | Completed 0                                                       | 37m                                                               |             |\n",
            "| cloud-native-postgresql-catalog-nqjn4 1/1 Running                 | cloud-native-postgresql-catalog-nqjn4 1/1 Running                 | cloud-native-postgresql-catalog-nqjn4 1/1 Running                 |             |\n",
            "| 0                                                                 |                                                                   | 40m                                                               |             |\n",
            "| cpd-platform-hp4w6                                                | cpd-platform-hp4w6                                                | cpd-platform-hp4w6                                                | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 39m                                                             |             |\n",
            "| cpd-platform-operator-manager-6bc68dc8d-7xbjz                     | cpd-platform-operator-manager-6bc68dc8d-7xbjz                     | cpd-platform-operator-manager-6bc68dc8d-7xbjz                     | 1/1 Running |\n",
            "| 0                                                                 |                                                                   | 15m                                                               |             |\n",
            "| cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running                 | cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running                 | cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running                 |             |\n",
            "|                                                                   |                                                                   | 0 41m                                                             |             |\n",
            "| create-postgres-license-config-m6ljg                              | create-postgres-license-config-m6ljg                              | create-postgres-license-config-m6ljg                              | 0/1         |\n",
            "| Completed 0                                                       |                                                                   | 32m                                                               |             |\n",
            "|                                                                   |                                                                   |                                                                   | 0/1         |\n",
            "| create-postgres-license-config-xdc4s Completed 0 30m              | create-postgres-license-config-xdc4s Completed 0 30m              | create-postgres-license-config-xdc4s Completed 0 30m              |             |\n",
            "| e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk   | e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk   | e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk   | 0/1         |\n",
            "| Completed                                                         | Completed                                                         |                                                                   |             |\n",
            "| 0 33m ibm-common-service-operator-5f688bffdb-jzppd                | 0 33m ibm-common-service-operator-5f688bffdb-jzppd                | 0 33m ibm-common-service-operator-5f688bffdb-jzppd                | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 15m                                                             |             |\n",
            "|                                                                   | ibm-lakehouse-controller-manager-6c54bdbb6f-c6stq                 | ibm-lakehouse-controller-manager-6c54bdbb6f-c6stq                 | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 15m                                                             |             |\n",
            "|                                                                   | ibm-namespace-scope-operator-66f4878bff-w9bt5                     | ibm-namespace-scope-operator-66f4878bff-w9bt5                     | 1/1 Running |\n",
            "************************* Chunk **********************************\n",
            "*************************** 3959 *********************************\n",
            "|                                                                   |                                                                   | 0 37m                                                             |             |\n",
            "| ibm-watsonx-data-catalog-fvl6d                                    | ibm-watsonx-data-catalog-fvl6d                                    | ibm-watsonx-data-catalog-fvl6d                                    | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 38m                                                             |             |\n",
            "|                                                                   | ibm-zen-operator-5646fffdf6-bb95f                                 | ibm-zen-operator-5646fffdf6-bb95f                                 | 1/1 Running |\n",
            "| 0                                                                 |                                                                   | 15m                                                               |             |\n",
            "| meta-api-deploy-7bcbf6c896-nkwl7                                  | meta-api-deploy-7bcbf6c896-nkwl7                                  | meta-api-deploy-7bcbf6c896-nkwl7                                  | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 30m                                                             |             |\n",
            "| opencloud-operators-8nw56                                         | opencloud-operators-8nw56                                         | opencloud-operators-8nw56                                         | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 37m                                                             |             |\n",
            "| operand-deployment-lifecycle-manager-5f94f78-9rwln                | operand-deployment-lifecycle-manager-5f94f78-9rwln                | operand-deployment-lifecycle-manager-5f94f78-9rwln                | 1/1 Running |\n",
            "| 0                                                                 |                                                                   | 15m                                                               |             |\n",
            "| postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5    | postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5    | postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5    | 1/1 Running |\n",
            "|                                                                   |                                                                   | 0 15m                                                             | 0/1         |\n",
            "| pre-zen-operand-config-job-lbj9l                                  | pre-zen-operand-config-job-lbj9l                                  | pre-zen-operand-config-job-lbj9l                                  |             |\n",
            "| Completed 0                                                       |                                                                   | 29m                                                               |             |\n",
            "| pre-zen-operand-config-job-n6xzr 0                                | pre-zen-operand-config-job-n6xzr 0                                | pre-zen-operand-config-job-n6xzr 0                                | 0/1         |\n",
            "| Completed setup-job-ft4lg                                         |                                                                   | 30m                                                               | 0/1         |\n",
            "| Completed 0 15m                                                   | Completed 0 15m                                                   | Completed 0 15m                                                   |             |\n",
            "************************* Chunk **********************************\n",
            "*************************** 3190 *********************************\n",
            "To access the catalogs in IBM watsonx.data, the IBM watsonx.data service must be restarted by running the following command:\n",
            "\n",
            "oc rollout restart sts,deploy-l 'component in (ibm-lh-presto-coordinator,ibm-lh-presto,ibm-lh-hive-metastore)'-n cpd-instance\n",
            "\n",
            "The command and its output are shown in Example 5-10.\n",
            "\n",
            "## Example 5-10 Restarting IBM watsonx.data service\n",
            "\n",
            "$ oc rollout restart sts,deploy-l 'component in (ibm-lh-presto-coordinator,ibm-lh-presto,ibm-lh-hive-metastore)'-n cpd-instance Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto-coordinator ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto-coordinator restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto-worker ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto-worker restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-hive-metastore ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') deployment.apps/ibm-lh-lakehouse-hive-metastore restarted\n",
            "\n",
            "After the command finishes, verify that the watsonx.data service was successfully restarted by running the following command:\n",
            "\n",
            "oc get deploy,sts-n cpd-instance\n",
            "\n",
            "## Related publications\n",
            "\n",
            "The publications listed in this section are considered particularly suitable for a more detailed discussion of the topics covered in this paper.\n",
            "\n",
            "## IBM Redbooks\n",
            "\n",
            "The following IBM Redbooks publications provide additional information about the topic in this document. Note that some publications referenced in this list might be available in softcopy only. For the current online list of Fusion Redbooks select here.\n",
            "\n",
            "IBM Storage Fusion HCI System: Metro Sync Disaster Recovery Use Case, REDP-5708\n",
            "\n",
            "IBM Storage Fusion Multicloud Object Gateway, REDP-5718\n",
            "\n",
            "IBM Storage Fusion Product Guide, REDP-5688\n",
            "\n",
            "You can search for, view, download or order these documents and other Redbooks, Redpapers, web docs, drafts, and additional materials, at the following website:\n",
            "\n",
            "ibm.com /redbooks\n",
            "\n",
            "## Other publications\n",
            "\n",
            "These publications are also relevant as further information sources:\n",
            "\n",
            "## Online resources\n",
            "\n",
            "IBM Documentation for IBM Fusion 2.7.x https://www.ibm.com/docs/en/sfhs/2.7.x\n",
            "\n",
            "IBM Documentation for IBM watsonx.data https://cloud.ibm.com/docs/watsonxdata\n",
            "\n",
            "IBM Fusion\n",
            "\n",
            "https://www.ibm.com/products/storage-fusion\n",
            "\n",
            "IBM watsonx.data https://www.ibm.com/products/watsonx-data\n",
            "\n",
            "IBM watsonx.data together with IBM Storage Fusion HCI System (video)\n",
            "\n",
            "## Help from IBM\n",
            "\n",
            "IBM Support and downloads ibm.com /support IBM Global Services ibm.com /services\n",
            "\n",
            "Back cover\n",
            "\n",
            "REDP-5720-00\n",
            "\n",
            "ISBN 0738461458\n",
            "\n",
            "Printed in U.S.A.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output with newer chunking method"
      ],
      "metadata": {
        "id": "MyD7c2e2RKKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = chunk_document(documents, 4096, 1024)\n",
        "print(len(res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-RnmqzgTSVU",
        "outputId": "cbac7ae1-597b-4223-d494-00c00388f03e"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "markdown\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in res:\n",
        "    i=0\n",
        "    print(\"************************* Chunk **********************************\")\n",
        "    print(\"*************************** {} *********************************\".format(len(r)))\n",
        "    print(r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YClyMrZSFoMV",
        "outputId": "911a79e7-6c1d-447d-e73c-1ad4e3ddf42a"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Chunk **********************************\n",
            "*************************** 590 *********************************\n",
            "Front cover\n",
            "\n",
            "## Accelerating IBM watsonx.data with IBM Fusion HCI\n",
            "\n",
            "IBM Redbooks\n",
            "\n",
            "## Accelerating IBM watsonx.data with IBM Fusion HCI\n",
            "\n",
            "March 2024\n",
            "\n",
            "Note: Before using this information and the product it supports, read the information in 'Notices' on page v.\n",
            "\n",
            "## First Edition (March 2024)\n",
            "\n",
            "This edition applies to Version 2, Release 7, Modification x of IBM Fusion HCI\n",
            "\n",
            "## ' Copyright International Business Machines Corporation 2024. All rights reserved.\n",
            "\n",
            "Note to U.S. Government Users Restricted Rights--Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5306 *********************************\n",
            "## Contents\n",
            "\n",
            "| Notices | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .v |\n",
            "|-|-|\n",
            "| Trademarks | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi |\n",
            "| Preface | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii |\n",
            "| Authors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii | |\n",
            "| Now you can become a published author, too! | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix |\n",
            "| Comments welcome. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix | |\n",
            "| Stay connected to IBM Redbooks | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x |\n",
            "| Chapter 1. Solution overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 1 |\n",
            "| 1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 2 |\n",
            "| 1.2 Benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 4 |\n",
            "| 1.3 Architecture, components, and functional characteristics . . . . . . . . . . . . . . . . . . . . . . . . | 5 |\n",
            "| 1.3.1 Integrated solution architecture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 5 |\n",
            "| 1.3.2 Solution component architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 6 |\n",
            "| Chapter 2. Sizing and planning | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 |\n",
            "| 2.1 Sizing guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 12 |\n",
            "| 2.1.1 Licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 | |\n",
            "| 2.1.2 IBM Fusion HCI with IBM watsonx.data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 12 |\n",
            "| 2.2 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 15 |\n",
            "| 2.2.1 Network access to object storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 15 |\n",
            "| 2.2.2 | |\n",
            "| AFM | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 |\n",
            "| 2.3.1 Data sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 16 |\n",
            "| Chapter 3. Implementation | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 |\n",
            "| 3.1 IBM Fusion HCI installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 18 |\n",
            "| 3.2 Installing Red Hat OpenShift Data Foundation and configuring the Multicloud Object  Gateway . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 18 |\n",
            "| 3.2.1 Configuring Advanced File Management nodes . . . . . . . . . . . . . . . . . . . . . . . . . . | 19 |\n",
            "| 3.2.2 Creating the static PV and PVC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 21 |\n",
            "| 3.2.3 Performance tuning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 27 |\n",
            "| 3.3 Installing IBM watsonx.data on IBM Fusion HCI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 29 |\n",
            "| Chapter 4. Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 31 |\n",
            "| Chapter 5. Backup and restore of IBM Cloud Pak for Data | . . . . . . . . . . . . . . . . . . . . . . 33 |\n",
            "| 5.1 Considerations and requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 34 |\n",
            "| 5.2 Getting the prerequisites ready . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 34 |\n",
            "| 5.2.1 Installing the Cloud Pak Backup and Restore service | |\n",
            "| . . . . . . . . . . . . . . . . . . . . . | 34 |\n",
            "| 5.2.2 Installing the cpdbr service on the target cluster. . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.3 Backup policies for Cloud Pak for Data applications. . . . . . . . . . . . . . . . . . . . . . . | 35 |\n",
            "| 5.2.4 Creating and assigning a backup policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 36 37 |\n",
            "| 5.3 Backing up the source cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | 39 |\n",
            "| 5.4 Restoring to an alternative cluster. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | |\n",
            "| | 39 |\n",
            "| Related publications IBM Redbooks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . | . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 45 |\n",
            "************************* Chunk **********************************\n",
            "*************************** 176 *********************************\n",
            "| Online resources |   45 |\n",
            "|-|-|\n",
            "| Help from IBM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . |   46 |\n",
            "************************* Chunk **********************************\n",
            "*************************** 4306 *********************************\n",
            "## Notices\n",
            "\n",
            "This information was developed for products and services offered in the US. This material might be available from IBM in other languages. However, you may be required to own a copy of the product or product version in that language in order to access it.\n",
            "\n",
            "IBM may not offer the products, services, or features discussed in this document in other countries. Consult your local IBM representative for information on the products and services currently available in your area. Any reference to an IBM product, program, or service is not intended to state or imply that only that IBM product, program, or service may be used. Any functionally equivalent product, program, or service that does not infringe any IBM intellectual property right may be used instead. However, it is the user's responsibility to evaluate and verify the operation of any non-IBM product, program, or service.\n",
            "\n",
            "IBM may have patents or pending patent applications covering subject matter described in this document. The furnishing of this document does not grant you any license to these patents. You can send license inquiries, in writing, to:\n",
            "\n",
            "IBM Director of Licensing, IBM Corporation, North Castle Drive, MD-NC119, Armonk, NY 10504-1785, US\n",
            "\n",
            "INTERNATIONAL BUSINESS MACHINES CORPORATION PROVIDES THIS PUBLICATION 'AS IS' WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE. Some jurisdictions do not allow disclaimer of express or implied warranties in certain transactions, therefore, this statement may not apply to you.\n",
            "\n",
            "This information could include technical inaccuracies or typographical errors. Changes are periodically made to the information herein; these changes will be incorporated in new editions of the publication. IBM may make improvements and/or changes in the product(s) and/or the program(s) described in this publication at any time without notice.\n",
            "\n",
            "Any references in this information to non-IBM websites are provided for convenience only and do not in any manner serve as an endorsement of those websites. The materials at those websites are not part of the materials for this IBM product and use of those websites is at your own risk.\n",
            "\n",
            "IBM may use or distribute any of the information you provide in any way it believes appropriate without incurring any obligation to you.\n",
            "\n",
            "The performance data and client examples cited are presented for illustrative purposes only. Actual performance results may vary depending on specific configurations and operating conditions.\n",
            "\n",
            "Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements or other publicly available sources. IBM has not tested those products and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products.\n",
            "\n",
            "Statements regarding IBM's future direction or intent are subject to change or withdrawal without notice, and represent goals and objectives only.\n",
            "\n",
            "This information contains examples of data and reports used in daily business operations. To illustrate them as completely as possible, the examples include the names of individuals, companies, brands, and products. All of these names are fictitious and any similarity to actual people or business enterprises is entirely coincidental.\n",
            "\n",
            "## COPYRIGHT LICENSE:\n",
            "\n",
            "This information contains sample application programs in source language, which illustrate programming techniques on various operating platforms. You may copy, modify, and distribute these sample programs in any form without payment to IBM, for the purposes of developing, using, marketing or distributing application programs conforming to the application programming interface for the operating platform for which the sample programs are written. These examples have not been thoroughly tested under all conditions. IBM, therefore, cannot guarantee or imply reliability, serviceability, or function of these programs. The sample programs are provided 'AS IS', without warranty of any kind. IBM shall not be liable for any damages arising out of your use of the sample programs.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4160 *********************************\n",
            "## Trademarks\n",
            "\n",
            "IBM, the IBM logo, and ibm.com are trademarks or registered trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the web at 'Copyright and trademark information' at https://www.ibm.com/legal/copytrade.shtml\n",
            "\n",
            "The following terms are trademarks or registered trademarks of International Business Machines Corporation, and might also be trademarks or registered trademarks in other countries.\n",
            "\n",
            "| Db2fiDS800 | Mfi | Redbooksfi |\n",
            "|-|-|-|\n",
            "| 0fi fi XIVfi Th | IBM Cloudfi | Redbooks (log |\n",
            "| Enterprise Storage ServerfiIB | IBM Cloud Pakfi | |\n",
            "\n",
            "e following terms are trademarks of other companies:\n",
            "\n",
            "Red Hat, Ceph, OpenShift, are trademarks or registered trademarks of Red Hat, Inc. or its subsidiaries in the United States and other countries.\n",
            "\n",
            "Other company, product, or service names may be trademarks or service marks of others.\n",
            "\n",
            "## Preface\n",
            "\n",
            "Organizations that are expanding from AI pilot projects to full-scale production systems typically need a set of tools for building and deploying foundation models, a container-based application platform, software-defined storage, and hardware on which to run it all. This IBM Redpaper publication describes the IBMfi solution for running IBM watsonx.data on premises, with IBM Fusion HCI providing an appliance-based hosting platform, and IBM Storage Ceph providing cloud-scale object storage.\n",
            "\n",
            "This publication shows how to set up the Storage Acceleration feature, so IBM watsonx.data queries can benefit from a shareable on-premises high-performance cache acceleration. The Storage Acceleration feature is available only on an IBM Fusion HCI.\n",
            "\n",
            "This paper is targeted toward technical professionals, consultants, technical support staff, IT Architects, and IT specialists who are responsible for delivering data lakehouse solutions optimized for data, analytics, and AI workloads.\n",
            "\n",
            "This paper was produced by a team of specialists from around the world working with the IBM Redbooks, Tucson Center.\n",
            "\n",
            "Larry Coyne is a Project Leader at the IBM International Technical Support Organization, Tucson, Arizona, center. He has over 35 years of IBM experience, with 23 years in IBM storage software management. He holds degrees in Software Engineering from the University of Texas at El Paso and Project Management from George Washington University. His areas of expertise include client relationship management, quality assurance, development management, and support management for IBM storage management software.\n",
            "\n",
            "Paulina Acevedo is a System Test Architect for the Application and Resiliency Fusion Team. Paulina has been with IBM for more than 17 years and has held several different positions within the Systems organization. She is a certified Project Manager and has been the System Test Project manager for several IBM Storage products.\n",
            "\n",
            "Eduardo Daniel Ibarra Alvarez is a Systems Test Engineer for the Application and Resiliency Fusion Team. Eduardo has a strong automation and testing background.\n",
            "\n",
            "Gabriela Valencia Castillo is a System Test Engineer for the Application and Resiliency Fusion Team. Gabriela has extensive experience in functional and non-functional testing, as well as test plan creation, design scenarios, and creation of test cases.\n",
            "\n",
            "Kenneth Hartsoe is the documentation Content Strategist for IBM Storage Ceph and IBM Fusion Data Foundation, as well as providing content strategy collaboration across multiple solutions within IBM Storage. Kenneth has more than twenty years experience within the storage documentation area, both as a senior technical writer and content strategist, including several years as the storage content strategist at Red Hat.\n",
            "\n",
            "Mike Kieran is a product marketer currently focusing on Storage for IBM watsonx.data. He has more than a decade of storage marketing experience at Pure Storage, NetApp, and Nimble Storage. Mike is the author of four books on digital color and the recipient of an Emmy for science and technology writing. He is an avid stargazer and astrophotographer.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5320 *********************************\n",
            "## Authors\n",
            "\n",
            "Alberto Larios is a System Test Engineer for the Application and Resiliency Fusion Team. Alberto collaborates with his team as a systems associate and tests Cloud Pak products on IBM Storage. His expertise lies on the data science field with knowledge on machine learning and modelling.\n",
            "\n",
            "Savitha H N is a System Test Engineer for the Application and Resiliency Fusion Team. Savitha has been with IBM from past one year and is a systems associate with Cloud Pak. She holds a Devops engineer certification, and specialist in Q/A testing by executing scenarios to Ensures the product is robust and failure scenarios are considered and refactored.\n",
            "\n",
            "Khanh Ngo is a leader in the IBM Storage CTO office specializing in Data and AI integration with IBM Storage products including the optimization of IBM watsonx.data.\n",
            "\n",
            "AshaRani G R is a System Test Engineer for the Application and Resiliency Fusion Team. Asha possesses extensive expertise encompassing server hardware, management software, and storage solutions within SAN and DAS domains. She has expertise in bringing up compute nodes for Infrastructure as a Service and performing end-to-end system testing. Additionally, she has a strong background in virtualization technology.\n",
            "\n",
            "Shyamala Rajagopalan is the senior lead technical content and information architect for IBM Fusion product. She has over 24 years of industry experience, with a significant track record of successfully delivering end-to-end IT documentation solutions to global clients across diverse industries. Her area of contribution includes digital transformation, legacy modernization, integration platforms (middleware, EAI), semiconductor, Cloud, and storage systems. She has performed diverse roles in the information development domain, which includes Information Architect, Technical Writing Manager, Lead Content Developer, Senior technical writer, and Competency Area Mentor.\n",
            "\n",
            "Ben Randall is the User Experience Architect for IBM Fusion and works on product development and design. He has worked in the enterprise storage industry for 21 years, focusing on technologies such as disaster recovery, backup and restore, container native storage, software defined storage, high performance computing, and SAN monitoring.\n",
            "\n",
            "Hemalatha B T is a System Test lead for Application and Resiliency Fusion Team. Hema has been with IBM for over 15 years and was associated with Power System Performance before moving to the functional/system test area working for products like IBM Cloud Pakfi for Systems, IBM Fusion. An enthusiastic quality assurance person who thrives to ensure high quality and perfectly functioning systems are delivered to customers.\n",
            "\n",
            "Todd Tosseth is a Software Engineer for IBM in Tucson, Arizona. Joining IBM in 2001, he has worked as a test and development engineer on several IBM storage products, such as IBM DS8000fi, IBM Storage Scale, and IBM Enterprise Storage Serverfi. He is working on IBM Cloud Pak as a system test engineer, with an emphasis on Cloud Pak storage integration.\n",
            "\n",
            "Jayson Tsingine is an Advisory Software Engineer in the IBM Systems Storage Group based in Tucson, Arizona. Jayson has worked in numerous test roles since he joined IBM in 2003, providing test support for storage products, including IBM XIVfi, DS8000, FlashSystems, Hydra, Spectrum Scale, Spectrum NAS, and Cloud Pak Solutions. He holds a BS degree in Computer Science from the University of Arizona.\n",
            "\n",
            "Israel Vizcarra is a test specialist that works for the Cloud Pak Storage Test Team. He graduated as a Mechatronics Engineer and he has 8 years of experience in the Quality Assurance area for the storage organization. Israel has actively participated in different roles from Functional Verification, System level, and Automation Testing.\n",
            "\n",
            "Zhi Yong Xue is an Architect of Fusion Storage in China. He has 15 years of experience in software design and development as a developer and architect at IBM. He holds a Bachelor's degree in Exploration Technology and Engineering from the University of Petroleum. His areas of expertise include Storage and Cloud computing.\n",
            "\n",
            "Thanks to the following people for their contributions to this project:\n",
            "\n",
            "## Patrik Hysky IBM Redbooksfi, IBM Infrastructure\n",
            "\n",
            "Pramod Thekkepat Achutha, Tara Astigarraga, Scott Colbeck, Karli Collins, Marcel Hergaarden, Ted Hoover, Lisa Huston, Kedar Karmarkar, Moushumi Kalita, Joshua Kim, Ana Lilia Sanchez Llamas, Ajay Lunawat, Boda Devi Manikanta, Kevin Shen, Bill Stoddard, Chris Tan, Thiha Than, Garth Tschetter, Hans Uhlig IBM Infrastructure\n",
            "\n",
            "David Wohlford IBM CHQ, Marketing\n",
            "\n",
            "## Now you can become a published author, too!\n",
            "\n",
            "Here's an opportunity to spotlight your skills, grow your career, and become a published author-all at the same time! Join an IBM Redbooks residency project and help write a book in your area of expertise, while honing your experience using leading-edge technologies. Your efforts will help to increase product acceptance and customer satisfaction, as you expand your network of technical contacts and relationships. Residencies run from two to six weeks in length, and you can participate either in person or as a remote resident working from your home base.\n",
            "\n",
            "Find out more about the residency program, browse the residency index, and apply online at: ibm.com /redbooks/residencies.html\n",
            "************************* Chunk **********************************\n",
            "*************************** 5031 *********************************\n",
            "## Comments welcome\n",
            "\n",
            "Your comments are important to us!\n",
            "\n",
            "We want our papers to be as helpful as possible. Send us your comments about this paper or other IBM Redbooks publications in one of the following ways:\n",
            "\n",
            "Use the online Contact us review Redbooks form found at: ibm.com /redbooks\n",
            "\n",
            "Send your comments in an email to:\n",
            "\n",
            "redbooks@us.ibm.com\n",
            "\n",
            "Mail your comments to:\n",
            "\n",
            "IBM Corporation, IBM Redbooks Dept. HYTD Mail Station P099 2455 South Road Poughkeepsie, NY 12601-5400\n",
            "\n",
            "## Stay connected to IBM Redbooks\n",
            "\n",
            "Find us on LinkedIn:\n",
            "\n",
            "https://www.linkedin.com/groups/2130806\n",
            "\n",
            "Explore new Redbooks publications, residencies, and workshops with the IBM Redbooks weekly newsletter:\n",
            "\n",
            "https://www.redbooks.ibm.com/subscribe\n",
            "\n",
            "Stay current on recent Redbooks publications with RSS Feeds:\n",
            "\n",
            "https://www.redbooks.ibm.com/rss.html\n",
            "\n",
            "Chapter 1.\n",
            "\n",
            "## Solution overview\n",
            "\n",
            "Organizations that are expanding from AI pilot projects to full-scale production systems typically need the following components:\n",
            "\n",
            "A set of tools for building and deploying foundation models\n",
            "\n",
            "A container-based application platform\n",
            "\n",
            "Software-defined storage\n",
            "\n",
            "Hardware on which to run it\n",
            "\n",
            "This publication describes the IBM solution for running IBM watsonx.data on premises, with IBM Fusion HCI providing an appliance-based hosting platform, and IBM Storage Ceph providing cloud-scale object storage. This publication shows how to set up the Storage Acceleration feature, which is only available on IBM Fusion HCI, so IBM watsonx.data queries can benefit from a shareable on-premises, high-performance cache acceleration.\n",
            "\n",
            "This paper is targeted toward technical professionals including consultants, technical support staff, IT Architects, and IT specialists who are responsible for delivering optimized for data, analytics, and AI workloads.\n",
            "\n",
            "This chapter includes an overview covering the background of data lakes and how the IBM solution of IBM watsonx.data, IBM Storage Ceph, and IBM Fusion HCI accelerated infrastructure works to improve on-premises performance and improves cost efficiency. The architecture of the solution and components are also described.\n",
            "\n",
            "1\n",
            "\n",
            "## 1.1 Overview\n",
            "\n",
            "This section describes the evolution of data lakes, the emergence of data lakehouses, and IBM watsonx.data lakehouse, IBM Storage Ceph, and the IBM Fusion HCI accelerated infrastructure solution.\n",
            "\n",
            "## From data warehouse to data lake\n",
            "\n",
            "During the past 20 years, large organizations have changed the way they aggregate data for analytics and business intelligence (BI) purposes. The original approach was to build a single monolithic database, or data warehouse, and then analyze specific subsets of the data through an extract, transform, load (ETL) process based on queries by using structured query language (SQL).\n",
            "\n",
            "Data warehouses are often used for repeatable reporting and analysis workloads such as monthly sales reports, tracking of sales per region, and website traffic. But building and maintaining a data warehouse is a costly, time-consuming process, and data warehouses work only with structured data.\n",
            "\n",
            "Moving data warehouses to the cloud doesn't solve the problem. Sometimes, it makes them even more expensive, and they're still not well suited to machine learning or AI applications.\n",
            "\n",
            "These limitations led to the concept of the data lake, which is a centralized repository that can store massive volumes of data in its original form so that it's consolidated, integrated, secure, and accessible. Data lakes are designed to accommodate all types of data from many different sources:\n",
            "\n",
            "Structured data, such as database tables and Excel sheets\n",
            "\n",
            "Semi-structured data, such as herbages and XML files\n",
            "\n",
            "Unstructured data, such as images, video, audio, and social media posts\n",
            "\n",
            "Because data lakes are massively scalable and can handle all types of data, they are ideal for real-time analytics, predictive analytics, and machine learning or AI. They are also typically less costly than data warehouses.\n",
            "\n",
            "## Data lakehouse architecture\n",
            "\n",
            "The data lakehouse is an emerging architecture that offers the flexibility of a data lake with the performance and structure of a data warehouse. Lakehouse solutions typically provide a high-performance query engine over low-cost object storage along with a metadata governance layer. Data lakehouses are based around open-standard object storage and enable multiple analytics and AI workloads to operate simultaneously on top of the data lake without requiring that the data be duplicated and converted.\n",
            "\n",
            "A key benefit of data lakehouses is that they address the needs of both traditional data warehouse analysts who curate and publish data for business intelligence and reporting purposes; and of data scientists and engineers who run more complex data analysis and processing workloads.\n",
            "\n",
            "IBM watsonx.data, shown in Figure 1-1, is built on an open lakehouse architecture, supported by querying, governance, and open data formats for accessing and sharing data.\n",
            "\n",
            "Figure 1-1 IBM watsonx.data provides an ideal platform for building and scaling AI applications\n",
            "************************* Chunk **********************************\n",
            "*************************** 4831 *********************************\n",
            "## IBM watsonx.data, IBM Storage Ceph, and the IBM Fusion HCI accelerated infrastructure solution\n",
            "\n",
            "Administrators of today's modern data lakehouses are required to think about storage optimizations as a top priority and a two-tiered approach. The first tier is an on-premises high-performance acceleration layer, which provides superior storage bandwidth with a cost-effective caching approach for the hybrid cloud object storage. The second tier is the low-cost persistent storage for your on-premises storage needs. With the combination of IBM Fusion HCI as your first tier solution and IBM Storage Ceph as your second tier solution, an organization can improve query performance with Storage Acceleration, significant cost advantage, and superior data management capabilities. IBM watsonx.data can take advantage of both of these tiers when using the IBM Fusion HCI and IBM Storage Ceph.\n",
            "\n",
            "## 1.2 Benefits\n",
            "\n",
            "IBM Fusion HCI is a hosting platform for IBM watsonx.data and provides the following benefits and features:\n",
            "\n",
            "Hosting platform for IBM watsonx products, starting with IBM watsonx.data:\n",
            "\n",
            "-Provides an automated deployment of Red Hat OpenShift on top of resilient compute, network, and storage in an appliance form-factor.\n",
            "\n",
            "-Provides all the storage classes that are needed by IBM Cloud Pak for Data (CP4D) and IBM watsonx.data\n",
            "\n",
            "Storage acceleration feature for Tier 1 data caching to accelerate IBM watsonx.data query performance to 5-15x improvement:\n",
            "\n",
            "-Connects to multiple object buckets\n",
            "\n",
            "-Uses intelligent caching to accelerate data access including automatic eviction\n",
            "\n",
            "-High-performance persistent object cache with low-capacity requirements:\n",
            "\n",
            "Cache once concept for faster performance\n",
            "\n",
            "Shareable across all engines and projects and namespaces\n",
            "\n",
            "Cache available to all nodes\n",
            "\n",
            "Multi-protocol (including virtualization) support\n",
            "\n",
            "Supports IBM Cloud Object Storage, Amazon Web Services, Seagate Lyve Cloud, Google Cloud Platform\n",
            "\n",
            "watsonx platform in a box:\n",
            "\n",
            "-Install efforts of a few days\n",
            "\n",
            "-Support for a maximum of 2 dedicated GPU nodes with optimizations\n",
            "\n",
            "-Support for a maximum of 2 dedicated gateway nodes for data access services\n",
            "\n",
            "-Scalable by adding nodes and disk capacity\n",
            "\n",
            "Shared run-time platform:\n",
            "\n",
            "-Multiple solutions in a box:\n",
            "\n",
            "IBM Db2fi Warehouse\n",
            "\n",
            "watsonx.data\n",
            "\n",
            "-Shared resources across multiple engines:\n",
            "\n",
            "Presto\n",
            "\n",
            "S p a r k\n",
            "\n",
            "-Compute-storage nodes provide high core-to-memory ratio. A C05 node with 64 cores and 2 TB memory yields a 1:32 core-to-memory ratio.\n",
            "\n",
            "Global Data Platform:\n",
            "\n",
            "-Data access services provides better performance across multiple parallel paths with single source of truth.\n",
            "\n",
            "-Data virtualization, collaboration and orchestration services for a true global namespace and data sharing across geo-distributed locations.\n",
            "\n",
            "-Supports compression at storage class level for space savings for various open data formats.\n",
            "\n",
            "-Encryption ensures both secure storage and secure deletion of data (at file system level).\n",
            "\n",
            "Local S3 object storage\n",
            "\n",
            "IBM Storage Ceph as an external cloud-scale S3 object store\n",
            "\n",
            "Ability to integrate GPUs into the IBM watsonx solution\n",
            "\n",
            "It is worth noting that the Storage Acceleration feature providing the data caching for improved query performance is very different from your traditional local caching. The IBM Fusion HCI has a global data platform which allows for a cache only once concept to achieve faster performance and transparency. After an object has been cached, it is available and shareable to every engine with IBM watsonx.data across all nodes within the cluster. The Storage Acceleration provides a persistent data cache for all engines. Newly provisioned engines also begin with a warm or hot cache.\n",
            "\n",
            "## 1.3 Architecture, components, and functional characteristics\n",
            "\n",
            "This section provides an architecture overview of IBM watsonx.data with IBM Fusion HCI and the IBM technologies integrated within the solution.\n",
            "\n",
            "## 1.3.1 Integrated solution architecture\n",
            "\n",
            "This integrated solution, as shown in Figure 1-2, consists of IBM watsonx.data deployed on Red Hat OpenShift hosted by the IBM Fusion HCI. IBM watsonx.data is connected to accelerated buckets hosted in either the public cloud, which includes IBM Cloud, Amazon S3, and Google Cloud Storage, or on-premises infrastructure such as IBM Storage Ceph. By connecting IBM Fusion HCI to external object buckets, high-performance object access is delivered by intelligent caching that is provided by IBM Fusion HCI's storage infrastructure. IBM Fusion HCI exposes the accelerated buckets to IBM watsonx.data for attachment to a query engine (Presto, Spark). Persistent cache is immediately available for newly provisioned engines.\n",
            "\n",
            "Figure 1-2 IBM watsonx.data Storage Acceleration hosted on IBM Fusion HCI\n",
            "\n",
            "## 1.3.2 Solution component architectures\n",
            "\n",
            "This section describes the architectures of the solution components.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4803 *********************************\n",
            "## IBM watsonx.data\n",
            "\n",
            "IBM watsonx.data is an open, hybrid, and governed data lakehouse optimized for all data and AI workloads. It combines the high performance and usability of a data warehouse with the flexibility and scalability of data lakes. IBM watsonx.data is a unique solution that allows co-existence of open source technologies and proprietary products. It offers a single point of entry where you can store the data or attach data sources for managing and analyzing structured, semi-structured, and unstructured enterprise data, which enables access to all data across cloud and on-premises environments.\n",
            "\n",
            "The following components as shown in Figure 1-3 provide the foundation of IBM watsonx.data:\n",
            "\n",
            "Open table formats, such as Apache Iceberg provide structure and deliver the reliability of SQL with big data. They allow different engines to access the same data at the same time, and enable data sharing across multiple repositories including data warehouses and data lakes.\n",
            "\n",
            "Query engines access data in an open table format. IBM watsonx.data query engines are fully modular and can be dynamically scaled to meet workload demands and concurrency.\n",
            "\n",
            "The technical metadata service enables the query engine to know the location, format, and read capabilities of the data.\n",
            "\n",
            "Data catalogs assist with finding the correct data and deliver semantic information for policies and rules.\n",
            "\n",
            "The policy engine enables users to define and enforce data protection.\n",
            "\n",
            "Figure 1-3 IBM watsonx.data architecture\n",
            "\n",
            "The IBM watsonx.data software stack is built on IBM Fusion HCI and IBM Storage Ceph to provide high-performance infrastructure and storage.\n",
            "\n",
            "## IBM Fusion HCI\n",
            "\n",
            "IBM Fusion HCI is a hyper-converged appliance that delivers all of the infrastructure needed to run Red Hat OpenShift on bare metal, which eliminates the complexity of designing, deploying, and maintaining an on-premises architecture for IBM watsonx.data. See Figure 1-4 on page 8. The appliance is delivered as a rack with all components mounted, cabled, and tested. It provides all the infrastructure resources that are required to host the Red Hat OpenShift cluster, such as storage nodes, compute nodes, and network switches.\n",
            "\n",
            "S3 object storage, IBM Storage Ceph, IBM Storage Scale, and NAS file arrays are available in a single namespace. Access by applications is unaffected by the type of storage behind the namespace. Intelligent global data caching enables accessing remote data at local file system speeds. IBM Fusion HCI uses a dedicated network for Red Hat OpenShift traffic and a dedicated, high-performance network for the storage cluster, and provides scalability for expanding workloads. Online migration of data from remote storage systems to the IBM Fusion HCI file system is included.\n",
            "\n",
            "Figure 1-4 IBM Fusion HCI architecture\n",
            "\n",
            "## IBM Storage Ceph\n",
            "\n",
            "IBM Storage Ceph is a software-defined storage platform that is based on an open source development model and can be deployed on industry-standard x86 hardware. It provides non-disruptive, horizontal scaling of object, block, and file storage to thousands of clients accessing exabytes of data. It is ideal for modern AI frameworks that require data lake capabilities.\n",
            "\n",
            "IBM Storage Ceph provides an external S3 object store for IBM watsonx.data. This S3 object store can be the main S3 object store for IBM watsonx.data, or an additional S3 object store with other on-premises or public-cloud object stores. The IBM Storage Ceph object storage interface, the Ceph Object Gateway, is compatible with a large subset of the Amazon S3 RESTful API. See Figure 1-5.\n",
            "\n",
            "Figure 1-5 Ceph Object Gateway architecture\n",
            "\n",
            "Multiple Red Hat OpenShift clusters can share storage from the same Ceph S3 object store.\n",
            "\n",
            "## IBM Storage Scale Erasure Code Edition Active File Management\n",
            "\n",
            "IBM Fusion HCI uses IBM Storage Scale Erasure Coded Edition (ECE) as an underlying storage platform. IBM Fusion ECE is a high-performance parallel file system that is used in High Performance Computing (HCP) and maximizes storage I/O within a clustered compute environment. This high-performance storage layer provides storage for IBM Cloud Pak for Data and IBM watsonx.data internal services and serves as a cache for storage accelerated buckets.\n",
            "\n",
            "This solution achieves storage acceleration by using Storage Scale's Active File Management (AFM) technology to connect to existing object storage buckets. These buckets reside in an on-premises object storage solution, such as IBM Storage Ceph or IBM Cloud Object Storage or in a public cloud provider such as AWS, Azure, or IBM Cloudfi. AFM is a high-speed cache for buckets it is attached to, allowing for data access that is significantly faster than I/O on the buckets directly. See Figure 1-6.\n",
            "\n",
            "Figure 1-6 Storage acceleration with Active File Management\n",
            "************************* Chunk **********************************\n",
            "*************************** 4731 *********************************\n",
            "## Multicloud Object Gateway\n",
            "\n",
            "Multicloud Object Gateway (MCG) is a lightweight object storage service for Red Hat OpenShift. Although MCG can function as an object storage provider using storage from the IBM Fusion HCI appliance, in this solution, MCG functions as a gateway between IBM watsonx.data and storage accelerated buckets provided by AFM. MCG connects directly to filesets in the Storage Scale ECE file system that map back to object buckets attached to AFM. When IBM watsonx.data reads from buckets, the reads pass through MCG to the AFM cache for the bucket. A cache hit results in a high-performance read. With a cache miss, the object is fetched from the external bucket hosting the data. See Figure 1-7.\n",
            "\n",
            "Figure 1-7 Multicloud Object Gateway and Storage Acceleration\n",
            "\n",
            "Chapter 2.\n",
            "\n",
            "## Sizing and planning\n",
            "\n",
            "This chapter describes sizing guidelines for the licensed components and highlights several planning activities that are related to the solution in this publication.\n",
            "\n",
            "## 2.1 Sizing guidelines\n",
            "\n",
            "This section provides sample sizing configurations for the licensed components.\n",
            "\n",
            "## 2.1.1 Licensing\n",
            "\n",
            "The following list highlights the software and hardware licensing for IBM Fusion HCI:\n",
            "\n",
            "watsonx is licensed by available cores not total cores:\n",
            "\n",
            "-A Fusion HCI 32 core worker node has 20 available cores. Watsonx needs 20 VPC $^{1}$of entitlement per Fusion HCI 32 core server.\n",
            "\n",
            "-A Fusion HCI 64 core worker node has 52 available cores. watsonx needs 52 VPC of entitlement per Fusion HCI 64 core server.\n",
            "\n",
            "-The other 12 cores in each server are reserved for Red Hat OpenShift and Fusion.\n",
            "\n",
            "Fusion HCI CPUs can support SMT$^{2}$=2:\n",
            "\n",
            "-A Fusion HCI 32 core worker node has 40 available vCPU.\n",
            "\n",
            "-A Fusion HCI 64 core worker node has 104 available vCPU.\n",
            "\n",
            "Fusion HCI 9155 Expert Care is required:\n",
            "\n",
            "-Expert care provides Fusion hardware support starting at beginning of year 1.\n",
            "\n",
            "-Expert care provides Fusion software support starting at beginning of year 1.\n",
            "\n",
            "Strategies for managing excess Fusion HCI hardware capacity:\n",
            "\n",
            "-License watsonx to a sub-capacity of the Fusion HCI cluster size.\n",
            "\n",
            "-Use excess cluster capacity for other Cloud Paks and workloads. If segregation of workload is required workloads can be isolated in separate namespaces.\n",
            "\n",
            "Learn more about container licensing here.\n",
            "\n",
            "## 2.1.2 IBM Fusion HCI with IBM watsonx.data\n",
            "\n",
            "This section provides guidance to plan for IBM Fusion HCI configurations using watsonx.data to meet your requirements. There are two types of configurations: Standard and Performance:\n",
            "\n",
            "Standard configuration use the following:\n",
            "\n",
            "-32 core worker nodes, each with 512 GB of memory.\n",
            "\n",
            "-Fusion HCI 32-core servers have 20 cores (40 vCPU) that are available for workloads after subtracting overhead for Red Hat OpenShift and Fusion software.\n",
            "\n",
            "Performance configurations use the following:\n",
            "\n",
            "-64 core worker nodes each with 2048 GB of memory.\n",
            "\n",
            "-Fusion HCI 64-core servers have 52 cores (104 vCPU) that are available for workloads after subtracting overhead for Red Hat OpenShift and Fusion software.\n",
            "\n",
            "## Detailed configurations follow:\n",
            "\n",
            "'Fusion HCI Standard configurations' on page 13\n",
            "\n",
            "'Fusion HCI Performance configurations' on page 14\n",
            "\n",
            "'Multi Rack Performance and Standard configurations' on page 14\n",
            "\n",
            "## Fusion HCI Standard configurations\n",
            "\n",
            "Fusion HCI Standard configurations are shown in Table 2-1.\n",
            "\n",
            "Table 2-1 Standard configuration details\n",
            "\n",
            "| Fusion HCI | Fusion HCI | E03 3 | E04 3 | E05 3 | E06 3 | E07 3 | E08 3 | E09 3 | E10 3 | E11 3 |\n",
            "|-|-|-|-|-|-|-|-|-|-|-|\n",
            "| | Control Nodes | E03 3 | E04 3 | E05 3 | E06 3 | E07 3 | E08 3 | E09 3 | E10 3 | E11 3 |\n",
            "| | Worker nodes | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |\n",
            "| | Active NVMe drives | 12 | 14 | 16 | 18 | 20 | 22 | 24 | 26 | 28 |\n",
            "| Fusion hardware | Query accelerator  nodes | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional |\n",
            "| M/T 9155 | Total worker cores | 96 | 128 | 160 | 192 | 224 | 256 | 288) | 320 | 352 |\n",
            "| | Available cores a | 60 | 80 | 100 | 120 | 140 | 160 | 180 | 200 | 220 |\n",
            "| | Available memory$^{b}$  (GB) | 1,296 | 1,728 | 2,160 | 2,592 | 3,024 | 3,456 | 3,888 | 4,320 | 4,752 |\n",
            "| | Usable NVMe$^{c}$  (TB) | 59 | 69 | 79 | 89 | 99 | 109 | 119 | 129 | 139 |\n",
            "| Fusion  software 5771-PP7 | VPCs to license | 96 | 128 | 160 | 192 | 224 | 256 | 288 | 320 | 352 |\n",
            "| watsonx D0F4SZX | VPCs$^{d}$ to license | 60 | 80 | 100 | 120 | 140 | 160 | 180 | 200 | 220 |\n",
            "\n",
            "a. SMT=1.\n",
            "\n",
            "b. This is a double memory configuration with 16 GB RAM per core. You can reduce cost by configuring with 8 GB RAM per core.\n",
            "\n",
            "c. Decimal TB. The formula is 7.68 TB x number of drives x 0.65. The 0.65 approximates 4+2p erasure coding overhead.\n",
            "\n",
            "d. watsonx.data is licensed per usable / available VPC not raw cores.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4704 *********************************\n",
            "## Fusion HCI Performance configurations\n",
            "\n",
            "Fusion HCI Performance configurations are shown in Table 2-2.\n",
            "\n",
            "Table 2-2 Performance configuration details\n",
            "\n",
            "| Fusion HCI | Fusion HCI | P03 3 | P04 3 | P05 3 | P06 3 | P07 3 | P08 3 | P09 3 | P10 3 | P11 3 |\n",
            "|-|-|-|-|-|-|-|-|-|-|-|\n",
            "| | Control Nodes | P03 3 | P04 3 | P05 3 | P06 3 | P07 3 | P08 3 | P09 3 | P10 3 | P11 3 |\n",
            "| | Worker nodes | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |\n",
            "| | Active NVMe drives | 12 | 14 | 16 | 18 | 20 | 22 | 24 | 26 | 28 |\n",
            "| Fusion hardware | Query accelerator  nodes | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional | Optional |\n",
            "| M/T 9155 | Total worker cores | 192 | 256 | 320 | 384 | 448 | 512 | 576 | 640 | 704 |\n",
            "| | Available cores a | 156 | 208 | 260 | 312 | 364 | 416 | 468 | 520 | 572 |\n",
            "| | Available memory$^{b}$  (GB) | 5.904 | 7,872 | 9,840 | 11,808 | 13,776 | 15,744 | 17,712 | 19,680 | 21,648 |\n",
            "| | Usable NVMe$^{c}$  (TB) | 59 | 69 | 79 | 89 | 99 | 109 | 119 | 129 | 139 |\n",
            "| Fusion  software 5771-PP7 | VPCs to license | 192 | 256 | 320 | 384 | 448 | 512 | 576 | 640 | 704 |\n",
            "| watsonx D0F4SZX | VPCs$^{d}$ to license | 156 | 208 | 260 | 312 | 364 | 416 | 468 | 520 | 572 |\n",
            "\n",
            "b. This is a double memory configuration with 16 GB RAM per core. You can reduce cost by configuring with 8 GB RAM per core.\n",
            "\n",
            "c. Decimal TB. The formula is 7.68 TB x number of drives x 0.65. The 0.65 approximates 4+2p erasure coding overhead.\n",
            "\n",
            "d. watsonx.data is licensed per usable / available VPC not raw cores.\n",
            "\n",
            "## Multi Rack Performance and Standard configurations\n",
            "\n",
            "Table 2-3 shows the Multi Rack Performance configurations.\n",
            "\n",
            "Table 2-3 Multi Rack Performance configurations\n",
            "\n",
            "| Size | 64-core worker  nodes | Available vCPU  (SMT=1) | Available vCPU  (SMT=2) | Total Memory in  GB |\n",
            "|-|-|-|-|-|\n",
            "| Two racks | Two racks | Two racks | Two racks | Two racks |\n",
            "| P11 + P03 a | 17 | 884 | 1768 | 33,456 |\n",
            "| P11 + P11 | 25 | 1300 | 2600 | 49,200 |\n",
            "| Three racks | Three racks | Three racks | Three racks | Three racks |\n",
            "| P11+P11+P03 a | 31 | 1612 | 3224 | 61,008 |\n",
            "| P11 + P11 + P11 | 39 | 2028 | 4056 | 76,752 |\n",
            "\n",
            "a. The 3 control nodes in rack 2 and rack 3 are converted to worker nodes.\n",
            "\n",
            "Table 2-4 shows the Multi Rack Standard configurations.\n",
            "\n",
            "## 2.2.2 AFM\n",
            "\n",
            "Table 2-4 Multi rack Standard configuration\n",
            "\n",
            "| Size | 32-core worker  nodes | Available vCPU  (SMT=1) | Available vCPU  (SMT=2) | Total Memory in  GB |\n",
            "|-|-|-|-|-|\n",
            "| Two racks | Two racks | Two racks | Two racks | Two racks |\n",
            "| E11 + E03 a | 17 | 340 | 680 | 7,344 |\n",
            "| E11 + E05 | 19 | 380 | 760 | 8,208 |\n",
            "| E11 + E07 | 21 | 420 | 840 | 9,072 |\n",
            "| E11 + E09 | 23 | 460 | 920 | 9.936 |\n",
            "| E11 + E11 | 25 | 500 | 1000 | 10,800 |\n",
            "| Three racks | Three racks | Three racks | Three racks | Three racks |\n",
            "| E11+E11+E03 a | 31 | 620 | 1240 | 13,392 |\n",
            "| E11+E11+E05 | 33 | 660 | 1320 | 14,256 |\n",
            "| E11 + E11 + E07 | 35 | 700 | 1400 | 15,120 |\n",
            "| E11 + E11 + E09 | 37 | 740 | 1480 | 15,984 |\n",
            "| E11 + E11 + E11 | 39 | 780 | 1560 | 16,848 |\n",
            "\n",
            "## 2.2 Planning\n",
            "\n",
            "Planning tasks help ensure that the IBM Fusion HCI is accurately integrated with IBM watsonx.data and configured properly for your operations.\n",
            "\n",
            "For more information, see Planning and prerequisites.\n",
            "\n",
            "## 2.2.1 Network access to object storage\n",
            "\n",
            "The IBM Fusion HCI connects to the data center network during the initial appliance setup. The appliance includes two high-speed switches that are connected to the core network through one port channel. This connection acts as the gateway between the IBM Fusion appliance and the network. It enables administration of the appliance and Red Hat OpenShift and is also used for network traffic in and out of the cluster. Network resources and applicable configuration settings are applied during this setup phase.\n",
            "\n",
            "For more information, see Network planning.\n",
            "\n",
            "AFM creates associations between clusters and the data source. It provides a single, global namespace across sites to automate the flow of data. AFM is enabled on the fileset that connects to the remote S3 endpoint to access the cache.\n",
            "\n",
            "The cache fileset is served by the AFM node, which functions as a gateway. As a gateway, the AFM node owns the fileset and communicates regarding data transfers.\n",
            "\n",
            "Make sure the IBM Fusion HCI has AFM nodes installed and configured as part of the Red Hat OpenShift cluster. AFM nodes must be installed and configured before you begin any storage acceleration operations. In addition, determine how large the tier 1 cache must be for storage acceleration. Ensure there is sufficient usable storage for that cache, in addition to the storage that is used for the Cloud Pak for Data and IBM watsonx.data PVCs.\n",
            "\n",
            "For more information, see Planning for AFM and Sharing Data.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4336 *********************************\n",
            "## 2.3 Customer use cases\n",
            "\n",
            "IBM watsonx provides the crucial data analytics and AI capabilities that all large organizations require. The strength of IBM Fusion HCI for IBM watsonx.data is the appliance-like experience where Red Hat OpenShift cluster, compute, storage, and network is all in a single box. It allows for a shorter time to value by having everything you need, so you can immediately start performing queries by using IBM watsonx.data.\n",
            "\n",
            "Consider the following key use cases for IBM watsonx.data:\n",
            "\n",
            "AI and machine learning (ML) at scale\n",
            "\n",
            "Build, train, tune, deploy, and monitor trusted AI and ML models for mission-critical workloads with governed data in IBM watsonx.data and ensure compliance with lineage and reproducibility of data used for AI.\n",
            "\n",
            "Real-time analytical and business intelligence Combine data from existing sources with new data to unlock new, faster insights without the cost and complexity of duplicating and moving data across different environments.\n",
            "\n",
            "Streamline data engineering\n",
            "\n",
            "Reduce data pipelines, simplify data transformation, and enrich data for consumption using SQL, Python, or an AI-infused conversational interface.\n",
            "\n",
            "Responsible data sharing Enable self-service access for more users to more data while ensuring security and compliance through centralized governance and local automated policy enforcement.\n",
            "\n",
            "## 2.3.1 Data sharing\n",
            "\n",
            "IBM Db2 Warehouse has the option to write and read to and from a cloud bucket using open formats such as parquet and iceberg. This allows for seamless integrating and sharing of data between IBM Db2 Warehouse and IBM watsonx.data without the need for deduplication or additional extract, transform, load operations. This might reduce your costs for storage that is used by IBM Db2 Warehouse and offload some of the workloads to IBM watsonx.data.\n",
            "\n",
            "Chapter 3.\n",
            "\n",
            "## Implementation\n",
            "\n",
            "Implementation involves the combination of IBM Fusion HCI and IBM Storage Ceph, which provide all the infrastructure for a stand-alone data lakehouse. The implementation includes installation of IBM Fusion HCI, Multicloud Object Gateway (MCG) configuration, Active File Management (AFM) configuration, performance tuning, and the installation of IBM watsonx.data.\n",
            "\n",
            "3\n",
            "\n",
            "## 3.1 IBM Fusion HCI installation\n",
            "\n",
            "If you already have an IBM Fusion HCI installation, ensure that your Red Hat OpenShift Container Platform is at Version 4.12.7.\n",
            "\n",
            "For the procedure to install IBM Fusion HCI 2.7.x, see Deploying IBM Fusion HCI.\n",
            "\n",
            "## 3.2 Installing Red Hat OpenShift Data Foundation and configuring the Multicloud Object Gateway\n",
            "\n",
            "The Multicloud Object Gateway (MCG) provides an object endpoint to which IBM watsonx.data and other workloads can connect to access multiple buckets, including Storage Acceleration buckets. The MCG is provided by the Red Hat OpenShift Data Foundation operator.\n",
            "\n",
            "Install the Red Hat OpenShift Data Foundation operator into Red Hat OpenShift Container Platform:\n",
            "\n",
            "1. Log in to your Red Hat OpenShift Container Platform.\n",
            "\n",
            "2. Go to OperatorHub and search for Data Foundation operator.\n",
            "\n",
            "3. Type Data Foundation in the Filter by keyword field to find the Data Foundation operator.\n",
            "\n",
            "4. Click Install. See Figure 3-1.\n",
            "\n",
            "5. Enter ibm-storage-fusion-cp-sc StorageClass that is configured by default in IBM Fusion HCI, and click Install.\n",
            "\n",
            "6. Click Install.\n",
            "\n",
            "Figure 3-1 Installing Red Hat OpenShift Data Foundation\n",
            "\n",
            "Create StorageSystem for Red Hat OpenShift Data Foundation:\n",
            "\n",
            "1. Ensure that you select the Installation Mode as A specific namespace on the cluster and click Install.\n",
            "\n",
            "2. Click Create StorageSystem for Red Hat OpenShift Data Foundation.\n",
            "\n",
            "3. In the Deployment type field, select MultiCloud Object Gateway. For more information about MCG deployment, see Deploy stand-alone Multicloud Object Gateway.\n",
            "\n",
            "Figure 3-2 Deployment type selection in Create StorageSystem\n",
            "\n",
            "## 3.2.1 Configuring Advanced File Management nodes\n",
            "\n",
            "Configure Storage Acceleration on IBM Fusion HCI to connect your remote object bucket to the Storage Scale Advanced File Management (AFM) accelerator and then expose an accelerated bucket through the MCG. This workflow has two steps:\n",
            "\n",
            "1. Create a Storage Scale AFM fileset that is connected to the remote S3 endpoint.\n",
            "\n",
            "2. Connect the MCG to the local Scale AFM fileset.\n",
            "\n",
            "After you complete the configuration, you can access the AFM cached remote S3 data.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5225 *********************************\n",
            "## Prerequisites\n",
            "\n",
            "This list describes the tools and environment prerequisites:\n",
            "\n",
            "Ensure that the Fusion HCI rack has AFM nodes installed, and ensure they are configured to be part of the Red Hat OpenShift cluster.\n",
            "\n",
            "The oc command is used to issue commands to the Red Hat OpenShift Container Platform cluster.\n",
            "\n",
            "Noobaa is installed as a part of MCG. It is used to access the Red Hat OpenShift Data Foundation MCG, and the AWS/S3 CLI is used to access the noobaa API endpoint. If the installation is not available, see noobaa-operator.\n",
            "\n",
            "Check whether you can log in to the Red Hat OpenShift Container Platform cluster.\n",
            "\n",
            "Retrieve the values for bucket name, access key, secret key, and S3 endpoint for the object bucket that you want to accelerate. The remote S3 can be IBM Storage Ceph for an on-premises environment. The S3 can be a cloud provider, such as IBM Cloud Object Storage (COS) or AWS S3. For more information, see IBM Storage Ceph and IBM Redpaper: IBM Storage Ceph Solutions Guide, REDP-5715.\n",
            "\n",
            "## Configuring the AFM nodes\n",
            "\n",
            "Before you configure the AFM nodes, collect the following information:\n",
            "\n",
            "Endpoint as REMOTE_S3_ENDPOINT\n",
            "\n",
            "Access Key as REMOTE_S3_ACCESS_KEY\n",
            "\n",
            "Access Secret as REMOTE_S3_ACCESS_SECRET\n",
            "\n",
            "Bucket Name as REMOTE_S3_BUCKET\n",
            "\n",
            "## Createing an AFM fileset and connecting to a remote S3 bucket\n",
            "\n",
            "1. Get the Scale core pod name on AFM node:\n",
            "\n",
            "AFM_NODE_POD_NAME=$(oc get node-l scale.spectrum.ibm.com/role=afm-o json | jq-r '.items[0].metadata.name' | awk-F '.' '{print $1}')\n",
            "\n",
            "2. Go into the Scale core pod:\n",
            "\n",
            "oc exec-it $AFM_NODE_POD_NAME bash-n ibm-spectrum-scale\n",
            "\n",
            "3. Input REMOTE_S3_ENDPOINT, REMOTE_S3_BUCKET, REMOTE_S3_ACCESS_KEY, and REMOTE_S3_ACCESS_SECRET :\n",
            "\n",
            "-REMOTE_S3_ENDPOINT=http://s3.us-south.cloud-object-storage.appdomain.cloud\n",
            "\n",
            "-REMOTE_S3_BUCKET=afm-s3test\n",
            "\n",
            "-REMOTE_S3_ACCESS_KEY=e48acxxxx750\n",
            "\n",
            "-REMOTE_S3_ACCESS_SECRET=8ffc2xxxxx1e22d\n",
            "\n",
            "4. Create an access key:\n",
            "\n",
            "mmafmcoskeys $REMOTE_S3_BUCKET set $REMOTE_S3_ACCESS_KEY $REMOTE_S3_ACCESS_SECRET\n",
            "\n",
            "5. Input the AFM fileset, endpoint, bucket, and afm_mode.\n",
            "\n",
            "6. Create an AFM fileset, endpoint, bucket, and afm_mode : For example, create an AFM fileset, and the AFM node is in mode *lu*. See Example 3-1.\n",
            "\n",
            "## Example 3-1 Creating an AFM fileset\n",
            "\n",
            "FILE_SYSTEM=ibmspectrum-fs fileset=afm-cos-s3-fileset AFM_MODE=lu mmafmcosconfig $FILE_SYSTEM $fileset--endpoint $REMOTE_S3_ENDPOINT--object-fs--bucket $REMOTE_S3_BUCKET--cleanup--debug--mode $AFM_MODE--tmpdir.noobaa% mmchfileset $FILE_SYSTEM $fileset-p afmPrefetchThreshold=100 mmafmcosctl $FILE_SYSTEM $fileset /mnt/${FILE_SYSTEM}/${fileset} download--all--metadata\n",
            "\n",
            "The following list describes the available modes and their purposes:\n",
            "\n",
            "Independent Writer (IW) is for changes made from the cache and server. This option must be configured when you are setting up both read and write cache. As you set the accelerator on top of the object bucket, the accelerator works both as a read and write cache for the object. It is the default setting.\n",
            "\n",
            "Local Update (LU) is for changes that are made on only the server. In this mode, you can use it for testing of your model. You do not want the changes you are making to go to the backend object bucket. After the test is complete, you can change to the IW mode.\n",
            "\n",
            "Single Writer (SW) is for changes made only from cache. In this mode, only the cache fileset does all the writing and the cache does not check home for file or directory updates.\n",
            "\n",
            "## Performance tuning\n",
            "\n",
            "Do configuration settings for performance tuning. For more information about tuning configuration, see 3.2.3, 'Performance tuning' on page 27.\n",
            "\n",
            "## Evicting cache data manually\n",
            "\n",
            "You can evict cache manually, or you can control eviction by defining quotas.\n",
            "\n",
            "Perform the following steps to evict cache manually:\n",
            "\n",
            "Evict all cache data manually:\n",
            "\n",
            "mmafmcosctl fs1 fileset1 /gpfs/fs1/new1 evict-all\n",
            "\n",
            "Evict all cache data and metadata: mmafmcosctl fs1 fileset1 /gpfs/fs1/new1 evict-all--metadata\n",
            "\n",
            "Manual eviction after quota limit is set. Evict data by using a criteria:\n",
            "\n",
            "mmafmctl fs1 evict-j fileset1--order LRU mmafmctl fs1 evict-j fileset1--order Size\n",
            "\n",
            "## Evicting cache by using quota enabled eviction\n",
            "\n",
            "Eviction can also be automatically controlled by using quotas. After the soft quota is exceeded, AFM automatically evicts the files based on LUR fashion. If a policy is not set for eviction, after the limit is reached, then the requests will fail with no space return error code. For more information, see Enabling quotas.\n",
            "\n",
            "In this example, set the soft quota to be 1 TB and the hard quota to be 2 TB. Adjust the values based on the PV/PVC size that you plan to create.\n",
            "\n",
            "mmsetquota $FILE_SYSTEM:$fileset--block 1000G:2000G\n",
            "\n",
            "Use either mlsquota or mmrepquota to view your quotas.\n",
            "\n",
            "## 3.2.2 Creating the static PV and PVC\n",
            "\n",
            "The steps in this section describe how to create the static PV and PVC.\n",
            "\n",
            "## Defining the variables\n",
            "\n",
            "Assign the scale cluster ID to the variable CLUSTER_ID : CLUSTER_ID=$(oc exec $AFM_NODE_POD_NAME-n ibm-spectrum-scale 'mmlscluster ' | grep 'GPFS cluster id: ' | awk '{print $4}')\n",
            "\n",
            "Assign the file system ID to the variable FILESYSTEM_ID :\n",
            "\n",
            "FILESYSTEM_ID=$(oc exec $AFM_NODE_POD_NAME-n ibm-spectrum-scale--bash-c 'mmlsfs ibmspectrum-fs--uid' | grep 'uid ' | awk '{print $2}')\n",
            "************************* Chunk **********************************\n",
            "*************************** 5192 *********************************\n",
            "## Updating and applying the YAML template\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Update the YAML template for PV name, capacity, volumeHandle and PVC name, as shown in Example 3-2.\n",
            "\n",
            "## Example 3-2 Updating the YAML template\n",
            "\n",
            "apiVersion: v1 kind: PersistentVolume metadata: name: {{PV_Name}} spec: accessModes:-ReadWriteMany capacity: storage: {{Capacity}} csi: driver: spectrumscale.csi.ibm.com volumeHandle: 0;2;{{CLUSTER_ID}};{{FILESYSTEM_ID}};;{{fileset}};/mnt/ibmspectrum-fs/{{fileset}} persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem--apiVersion: v1 kind: PersistentVolumeClaim metadata: name: {{PVC_Name}} namespace: openshift-storage spec: accessModes:-ReadWriteMany resources: requests: storage: {{Capacity}} storageClassName: ' ' volumeMode: Filesystem volumeName: {{PV_Name}}\n",
            "\n",
            "Example 3-3 is an example YAML file is an example using PV name, capacity, volumeHandle and PVC name. You will need to use your naming conventions for your organization.\n",
            "\n",
            "## Example 3-3 Example of updating the YAML template\n",
            "\n",
            "Example yaml as file pv_pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: afm-cos-s3-remote-pv spec: accessModes:-ReadWriteMany capacity:\n",
            "\n",
            "storage: 1000Gi csi: driver: spectrumscale.csi.ibm.com volumeHandle: 0;2;6734170828145876673;1180A8C0:646EEE59;;afm-cos-s3-fileset;/mnt/ibmspectrum-fs/ afm-cos-s3-fileset persistentVolumeReclaimPolicy: Retain volumeMode: Filesystem--apiVersion: v1 kind: PersistentVolumeClaim metadata: name: afm-cos-s3-remote-pvc namespace: openshift-storage spec: accessModes:-ReadWriteMany resources: requests: storage: 1000Gi storageClassName: ' ' volumeMode: Filesystem volumeName: afm-cos-s3-remote-pv\n",
            "\n",
            "2. Apply the yaml to create static PV and PVC. The following command is an example:\n",
            "\n",
            "oc apply-f pv_pvc.yaml\n",
            "\n",
            "## Configuring the MCG bucket\n",
            "\n",
            "Configure an MCG bucket that uses the PVC to access the AFM cache.\n",
            "\n",
            "As a prerequisite, install ODF operator and create an ODF cluster with MCG only mode.\n",
            "\n",
            "1. Create a NamespaceStore with the PVC, as shown in Example 3-4.\n",
            "\n",
            "## Example 3-4 Creating a NamespaceStore with the PVC\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: NamespaceStore metadata: name: {{NSS_Name}} namespace: openshift-storage spec: nsfs: fsBackend: GPFS pvcName: {{PVC_Name}} subPath: ' ' type: nsfs\n",
            "\n",
            "2. Create a bucket class with the NamespaceStore, as shown in Example 3-5.\n",
            "\n",
            "## Example 3-5 Creating a bucket class with the NamespaceStore\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: name: {{Bucket_Class_Name}} namespace: openshift-storage spec: namespacePolicy: single: resource: {{NSS_Name}} type: Single\n",
            "\n",
            "3. Create a Noobaa Account for the NamespaceStore, as shown in Example 3-6.\n",
            "\n",
            "Example 3-6 Creating a Noobaa Account for the NamespaceStore\n",
            "\n",
            "apiVersion: noobaa.io/v1alpha1 kind: NooBaaAccount metadata: name: {{Noobaa_Account_Name}} namespace: openshift-storage spec: allow_bucket_creation: true default_resource: {{NSS_Name}} nsfs_account_config: gid: 0 new_buckets_path: / nsfs_only: true uid: 0\n",
            "\n",
            "4. Create an ObjectBucketClaim with the BucketClass as shown in Example 3-7.\n",
            "\n",
            "Example 3-7 Creating an ObjectBucketClaim with the BucketClass\n",
            "\n",
            "apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: {{Object_Bucket_Claim_Name}} namespace: openshift-storage spec: additionalConfig: bucketclass: {{Bucket_Class_Name}} bucketName: {{Bucket_Name}} storageClassName: openshift-storage.noobaa.io\n",
            "\n",
            "Example 3-8 is an example of creating an ObjectBucketClaim with the BucketClass. You will need to use your organizations naming conventions.\n",
            "\n",
            "Example 3-8 Example showing YAML creating an ObjectBucketClaim with the BucketClass\n",
            "\n",
            "Example YAML: apiVersion: noobaa.io/v1alpha1 kind: NamespaceStore metadata: name: afm-cos-s3-nss\n",
            "\n",
            "namespace: openshift-storage spec: nsfs: fsBackend: GPFS pvcName: afm-cos-s3-remote-pvc subPath: data type: nsfs--apiVersion: noobaa.io/v1alpha1 kind: BucketClass metadata: name: afm-cos-s3-bc namespace: openshift-storage spec: namespacePolicy: single: resource: afm-cos-s3-nss type: Single--apiVersion: noobaa.io/v1alpha1 kind: NooBaaAccount metadata: name: afm-cos-s3-acc namespace: openshift-storage spec: allow_bucket_creation: true default_resource: afm-cos-s3-nss nsfs_account_config: gid: 0In e new_buckets_path: / nsfs_only: true uid: 0--apiVersion: objectbucket.io/v1alpha1 kind: ObjectBucketClaim metadata: name: afm-cos-s3-obc namespace: openshift-storage spec: additionalConfig: bucketclass: afm-cos-s3-bc bucketName: afm-cos-s3-nss-bc storageClassName: openshift-storage.noobaa.io\n",
            "\n",
            "5. Apply the yaml file from Example 3-8 on page 24 to create resources.\n",
            "\n",
            "## Updating the bucket access policy\n",
            "\n",
            "You need the noobaa admin account to update the bucket access policy.\n",
            "\n",
            "1. Get AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output: noobaa status--show-secrets\n",
            "\n",
            "2. Create the policy.json file with the content in Example 3-9.\n",
            "\n",
            "Example 3-9 Creating the policy.json file\n",
            "\n",
            "{ 'Version ': '2012-10-17 ', 'Statement ':[{ 'Sid ': 'id-1 ', 'Effect ': 'Allow ', 'Principal ': '* ', 'Action ':['s3:* '], 'Resource ':['arn:aws:s3:::* '] }] }\n",
            "\n",
            "3. Add a bucket policy (Example 3-10).\n",
            "\n",
            "Replace the bucket name with the bucket name that you defined in the previous steps.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4993 *********************************\n",
            "## Example 3-10 Adding a bucket policy\n",
            "\n",
            "ObjectBucketClaim.Spec.bucketName field BUCKET_NAME=afm-cos-s3-nss-bc S3_ENDPOINT=https://$(oc get route s3-n openshift-storage-o json | jq-r '.status.ingress[0].host')\n",
            "\n",
            "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY aws--endpoint-url=$S3_ENDPOINT--no-verify-ssl s3api put-bucket-policy--bucket $BUCKET_NAME--policy file://policy.json\n",
            "\n",
            "4. Verify the bucket:\n",
            "\n",
            "BUCKET_NAME=afm-cos-s3-nss-bc\n",
            "\n",
            "a. Get the noobaa account. The noobaa account name that was defined in the previous steps.\n",
            "\n",
            "b. Get the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output:\n",
            "\n",
            "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY aws--endpoint $S3_ENDPOINT\n",
            "\n",
            "NoobaaAccount.metadata.name NOOBAA_ACCOUNT_NAME=afm-cos-s3-acc noobaa account status $NOOBAA_ACCOUNT_NAME--show-secrets b. Get the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY from the output: AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID--no-verify-ssl s3api list-objects--bucket $BUCKET_NAME\n",
            "\n",
            "## 3.2.3 Performance tuning\n",
            "\n",
            "You defined a storage accelerated bucket that can be accessed by workloads such as watsonx.data. The following list describes the default parameters that can be changed or disabled to improve performance:\n",
            "\n",
            "| afmFileLookupRefreshInterval | Defines the frequency of revalidation that is triggered by a  look-up operation on a file such as  ls  or  stat , from the  IBM Fusion HCI. AFM sends a message to the external  object bucket to determine that the metadata of the file is |\n",
            "|-|-|\n",
            "| afmFileOpenRefreshInterval | IBM Fusion HCI. Defines the frequency of revalidations that are triggered by  the read and write operations on a file from the IBM Fusion  HCI. AFM sends a message to the external object bucket  to determine if the metadata of the file was modified since  it was last revalidated. |\n",
            "| afmDirLookupRefreshInterval | Defines the frequency of revalidation that is triggered by a  look-up operation such as  ls  or  stat  on a directory from  the IBM Fusion HCI. AFM sends a message to the external  object bucket to find out whether the metadata of that  directory is modified since it was last revalidated. If so, the  latest metadata information at the external object bucket is |\n",
            "| afmDirOpenRefreshInterval | Defines the frequency of revalidations that are triggered by  the read and update operations on a directory from the  IBM Fusion HCI. AFM sends a message to the external  object bucket to find whether the metadata of that directory  is modified since it was last revalidated. |\n",
            "| afmObjectFastReaddir | Improves the objects download and readdir performance,  when the  afmObjectFastReaddir  parameter value is set to  yes  at the fileset level. Extended attributes and ACLs are  not fetched from a cloud object storage when this  parameter is enabled. Also, deleted objects on a cloud  object storage system are not reflected immediately on a  cache when this parameter is enabled. You can use this |\n",
            "| afmParallelReadChunkSize | Defines the minimum chunk size of the read that needs to  be distributed among the gateway nodes during parallel  reads. A zero (0) value disables the parallel reads across  multiple gateways. The parallel reads are routed through a  single gateway node. |\n",
            "| afmObjectFastReaddir | Improves the objects download and readdir performance,  afmObjectFastReaddir  parameter value is set to  'yes' at the fileset level. Extended attributes and ACLs are |\n",
            "| | when the  not fetched from a cloud object storage when this  parameter is enabled. Also, deleted objects on a cloud  object storage system are not reflected immediately on a |\n",
            "\n",
            "## Data not changing on the server\n",
            "\n",
            "When you configure an accelerator or caching for an object bucket, there might not be any changes to the data on the object bucket.\n",
            "\n",
            "Changes to the applications go to the object bucket through the accelerator. If there is no possibility of anybody making updates to the object bucket without the accelerator, then you can disable the refresh intervals.\n",
            "\n",
            "For example, a Ceph object bucket exists, and the accelerator is placed on top of it. Applications in turn run on top of the accelerator.\n",
            "\n",
            "If no changes are happening on the object bucket outside the accelerator path, then use the following configuration parameters:\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmFileLookupRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmFileOpenRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmDirLookupRefreshInterval=disable mmchfileset $FILE_SYSTEM $fileset-p afmDirOpenRefreshInterval=disable\n",
            "\n",
            "## Data might change on the server\n",
            "\n",
            "When the data can change on the server, then you can disable the file lookup interval without disabling the directory lookup. You can you can increase the revalidation time from the default of 60 seconds.\n",
            "\n",
            "Enter the following commands to alter the frequency of revalidation:\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmObjectFastReaddir=yes mmchfileset $FILE_SYSTEM $fileset-p afmFileLookupRefreshInterval=disable\n",
            "************************* Chunk **********************************\n",
            "*************************** 4047 *********************************\n",
            "## Size of the download from each Gateway\n",
            "\n",
            "AFM can read from multiple gateways at the same time. If the amount of date that is read is greater than a defined number, then parallel reads begin. For example, if 12 MB is the chunk size, then each gateway reads 12 MB and then pass the data to the main gateway to process the data.\n",
            "\n",
            "mmchfileset $FILE_SYSTEM $fileset-p afmParallelReadChunkSize=12M\n",
            "\n",
            "## How it improves performance\n",
            "\n",
            "Parallel read data transfer improves the overall data read transfer performance of an AFM to cloud object storage fileset by using multiple gateway nodes.\n",
            "\n",
            "## Multiple gateways\n",
            "\n",
            "For filesets with mode LU or RO, you can use multiple gateways for better performance. Configure more gateways with the following commands:\n",
            "\n",
            "mmchnode-gateway-N node mm chfileset fs fileset-name-p afmGateway=all\n",
            "\n",
            "To activate afmGateway=all, stop and restart the fileset by using the following commands:\n",
            "\n",
            "mmafmctl perffs stop-j db2wh-db2u-perf-test-2-cos mmchfileset perffs db2wh-db2u-perf-test-2-cos-p afmGateway=all mmafmctl perffs start-j db2wh-db2u-perf-test-2-cos\n",
            "\n",
            "For the other modes, such as IW and SW, use the mmafmconfig command:\n",
            "\n",
            "mmafmconfig {add | update} MapName--export-map ExportServerMap [--no-server-resolution]\n",
            "\n",
            "The following command is a sample of the mmafmconfig command:\n",
            "\n",
            "mmafmconfig add mymap--export-map 169.46.118.100/fin37.ibm.com,10.242.33.16/fin38.ibm.com\n",
            "\n",
            "The 2nd IP address in each pair is the gateway, and fin37.ibm.com and fin38.ibm.com are the addresses of the gateways.\n",
            "\n",
            "## Populating the metadata cache\n",
            "\n",
            "If you want to pre-fetch your metadata cache, you have several options.\n",
            "\n",
            "The mmchfileset command retrieves the name entries the fastest from listv2 with minimum attributes. You can use the ls command to view the metadata. Use the following format for the command:\n",
            "\n",
            "mmchfileset device filesetname-p afmObjectFastReaddir=yes\n",
            "\n",
            "The mmafmcosctl command is not as fast as the mmchfileset command. The command bypasses the gateway and retrieves the names and full attributes:\n",
            "\n",
            "mmafmcosctl device filesetName path download--metadata--outband\n",
            "\n",
            "The mmafmcosctl is slower than the other commands. It uses the gateway and retrieves the names and full attributes:\n",
            "\n",
            "mmafmcosctl device filesetame path download--metadata\n",
            "\n",
            "## 3.3 Installing IBM watsonx.data on IBM Fusion HCI\n",
            "\n",
            "Complete the following steps to install IBM watsonx.data.\n",
            "\n",
            "1. Install IBM watsonx.data. For the procedure to install, see Installing wastsonx.data.\n",
            "\n",
            "2. To configure IBM watsonx.data for IBM Fusion HCI storage, create a storage class with the appropriate settings for use with IBM watsonx.data. For the actual procedure to configure, see Setting up IBM Storage Scale storage.\n",
            "\n",
            "3. Create a watsonx.data instance. The watsonx.data operator is installed one time on the cluster and shared by many instances of watsonx.data on the cluster.\n",
            "\n",
            "## Creating an accelerated bucket\n",
            "\n",
            "As a prerequisite, create an AFM fileset and attach it to a bucket. Use the following procedure to create an accelerated bucket and connect to an existing externally managed object storage (Multicloud Gateway):\n",
            "\n",
            "1. Log in to watsonx.data console.\n",
            "\n",
            "2. From the navigation menu, select Infrastructure Manager.\n",
            "\n",
            "3. To define and connect a bucket, click Add component and select Add bucket.\n",
            "\n",
            "4. In the Add bucket window, provide the following details to connect to the accelerated bucket provided by MCG:\n",
            "\n",
            "Note: Refer to the values that you set during the accelerated bucket creation for the Bucket name, endpoint, access key, and secret key.\n",
            "\n",
            "-Bucket type. Select Ceph as the value for bucket type from list.\n",
            "\n",
            "-Bucket name. Enter the name of your existing bucket.\n",
            "\n",
            "-Endpoint. Enter the endpoint URL.\n",
            "\n",
            "-Access key. Enter your access key.\n",
            "\n",
            "-Secret key. Enter your secret key.\n",
            "\n",
            "-Activation. Activate the bucket immediately or activate it later.\n",
            "\n",
            "-Catalog type. Select the catalog type from the list.\n",
            "\n",
            "-Catalog name. Enter the name of the catalog. The catalog is automatically associated with your bucket.\n",
            "\n",
            "To add a bucket-catalog pair, see Adding a bucket-catalog pair.\n",
            "\n",
            "Chapter 4.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5225 *********************************\n",
            "## Monitoring\n",
            "\n",
            "The IBM Fusion HCI is like an Red Hat OpenShift cluster in a box. With the available default Red Hat OpenShift monitoring options, you can monitor the watsonx.data project or namespace. In addition, the IBM Fusion HCI has its own monitoring and logging capabilities to view different dashboards for storage, networking, and compute. For more information, see Monitoring and logging.\n",
            "\n",
            "However, for monitoring watsonx.data specifically, use the built-in Presto engine web interface for monitoring and managing queries. This web interface is accessible by an Red Hat OpenShift route to the coordinator pod. For more information, see Exposing secure route to Presto server.\n",
            "\n",
            "After the route has been exposed, you can open a web browser to the route's URL. The main page has a list of queries and includes information such as unique query ID, query text, query state, percentage completed, username, and source from which this query originated. The currently running queries are at the top of the page, followed by the most recently completed or failed queries.\n",
            "\n",
            "Figure 4-1 shows an example of the main page.\n",
            "\n",
            "Figure 4-1 An example of the main page\n",
            "\n",
            "For more detailed information about a query, click the query ID link. The query detail page has a summary section, graphical representation of various stages of the query and a list of tasks. Each task ID can be clicked to get more information about that task. For example, when you click the task ID, you see a page similar to Figure 4-2.\n",
            "\n",
            "Figure 4-2 An example of the detailed view of an example task ID\n",
            "\n",
            "Chapter 5.\n",
            "\n",
            "## Backup and restore of IBM Cloud Pak for Data\n",
            "\n",
            "This chapter describes backing up and restoring Cloud Pak for Data with IBM watsonx.data on IBM Fusion. It covers the non-disruptive backup of Cloud Pak for Data on a Fusion HCI and restore to an alternative Fusion HCI. The process includes setting up a backup location, creating backup policies on the Fusion hub system, and applying the policies on to the Cloud Pak for Data application that is deployed.\n",
            "\n",
            "## 5.1 Considerations and requirements\n",
            "\n",
            "The example includes two IBM Fusion HCI racks running Version 2.6.1 that are connected as hub and spoke. Supported versions of Cloud Pak for Data and watsonx.data service are installed on the hub system as the source for backup.\n",
            "\n",
            "The following software and configurations are required when backing up your Cloud Pak for Data environment:\n",
            "\n",
            "IBM Fusion HCI 2.6.1 or later.\n",
            "\n",
            "Cloud Pak for Data 4.7.1 or later.\n",
            "\n",
            "Red Hat OpenShift Container Platform versions must be at the same major version on both source and target clusters. For example, IBM Fusion HCI 2.6.1 supports OpenShift Container Platform 4.10 and 4.12 and both source and target clusters must be at the same major version.\n",
            "\n",
            "Cloud Pak for Data and its services at the same release level.\n",
            "\n",
            "A supported version of Cloud Pak for Data is installed in private topology, and each Cloud Pak for Data instance includes the following components:\n",
            "\n",
            "-Two namespaces:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "cpd-instance\n",
            "\n",
            "-One shared cluster components: ibm-scheduler (optional scheduling service)\n",
            "\n",
            "With IBM Fusion 2.6.1, Backup & Restore should be configured across two Fusion HCI clusters with one of the clusters acting as the hub. The hub controls the backup and restore flow with one or more clusters that are connected to the hub as spokes. This setup allows for backups that are taken in one cluster to be restored in a different cluster.\n",
            "\n",
            "Cloud Pak for Data along with IBM watsonx.data can be installed on either the hub cluster or the spoke cluster.\n",
            "\n",
            "## 5.2 Getting the prerequisites ready\n",
            "\n",
            "Install more components before backing up the Cloud Pak for Data environment.\n",
            "\n",
            "## 5.2.1 Installing the Cloud Pak Backup and Restore service\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Install the cpdbr-oadp service in the Cloud Pak for Data operators and Cloud Pak for Data shared namespaces of the cluster components, which include Cloud Pak for Data Scheduling service (if installed). Ignore the Scheduling service namespace if it is not installed. The cpdbr-oadp service must be installed on both the Hub and Spoke clusters.\n",
            "\n",
            "To install the service, prepare your Hub and Spoke clusters to use cpd-cli. For more information, see Cloud Pak for Data command-line interface CPD.\n",
            "\n",
            "2. Install the cpdbr-oadp service in the following namespaces:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "cpd-scheduler\n",
            "\n",
            "For more information, see Installing the cpdbr service for IBM Fusion integration.\n",
            "\n",
            "3. On the source cluster, install the cpdbr-oadp service by issuing the following command:\n",
            "\n",
            "./cpd-cli oadp install--component=cpdbr-tenant--tenant-operator-namespace=<cpd-operator_ns>--cpdbr-hooks-image-prefix=quay.io/cpdsre--cpd-scheduler-namespace=cpd-scheduler--log-level=debug--verbose\n",
            "\n",
            "4. After installation is done, verify that the cpdbr pod is deployed by running the following command:\n",
            "\n",
            "oc get pods-A| grep cpdbr The following line is an example of the expected output: cpd-operator cpdbr-tenant-service-6dcc49464c-rr9jh\n",
            "\n",
            "The installation of cpdbr-oadp also installs, generates, and applies the required recipes in the respective Cloud Pak for Data cpd-operator and cpd-scheduler namespaces (if installed). To verify, issue the command oc get frcpe-A, as shown in Example 5-1.\n",
            "************************* Chunk **********************************\n",
            "*************************** 4415 *********************************\n",
            "## Example 5-1 Example output\n",
            "\n",
            "| oc get frcpe -A | oc get frcpe -A | oc get frcpe -A |\n",
            "|-|-|-|\n",
            "| NAMESPACE | NAME | AGE |\n",
            "| cpd-operator | ibmcpd-tenant | 2m6s |\n",
            "| ibm-spectrum-fusion-ns | fusion-control-plane 3d23h | |\n",
            "| ibm-spectrum-fusion-ns | fusion-cr-backup | 10d |\n",
            "\n",
            "## 5.2.2 Installing the cpdbr service on the target cluster\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Install the cpdbr-tenant service on the target cluster, as shown in Example 5-2.\n",
            "\n",
            "Example 5-2 Installing cpdbr-tenant service on the target cluster\n",
            "\n",
            "$ cpd-cli oadp install--component=cpdbr-tenant--tenant-operator-namespace=cpd-operator processing request... cpd tenant operator namespace: cpd-operator clusterrole/cpdbr-tenant-service-clusterrole created clusterrolebinding/cpdbr-tenant-service-crb created role/cpdbr-tenant-service-role created in namespace kube-public rolebinding/cpdbr-tenant-service-rb created in namespace kube-public\n",
            "\n",
            "2. After the installation is done, verify that ClusterRoleBinding was created, as shown in Example 5-3.\n",
            "\n",
            "## Example 5-3 Verifying that ClusterRoleBinding was created\n",
            "\n",
            "$ oc get clusterrolebinding cpdbr-tenant-service-crb NAME ROLE AGE cpdbr-tenant-service-crb ClusterRole/cpdbr-tenant-service-clusterrole 37s $ oc get clusterrole cpdbr-tenant-service-clusterrole NAME CREATED AT cpdbr-tenant-service-clusterrole 2023-08-25T17:45:55Z $ oc get clusterrolebinding | grep cpdbr cpdbr-tenant-service-crb\n",
            "\n",
            "## 5.2.3 Backup policies for Cloud Pak for Data applications\n",
            "\n",
            "Before you create and apply backup policies to the Cloud Pak for Data applications, you must create an S3 compliant backup location in the Fusion UI of the hub system. The hub system is used to store the backups and is the source when the data is restored.\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. To create a backup location, on IBM Fusion UI, select Backup & restore  Locations to add a backup location. In this setup, which is shown in Figure 5-1, we added an object storage from IBM Cloud as a backup location.\n",
            "\n",
            "Figure 5-1 Adding a backup location\n",
            "\n",
            "2. After creating the backup location, run the following command to list the backup locations that were created:\n",
            "\n",
            "$ oc get fbsl-n ibm-spectrum-fusion-ns NAME PROVIDER\n",
            "\n",
            "ibm-cloudd\n",
            "\n",
            "isf-backup-restore\n",
            "\n",
            "PHASE\n",
            "\n",
            "STORAGETYPE\n",
            "\n",
            "ibm\n",
            "\n",
            "Connected\n",
            "\n",
            "## 5.2.4 Creating and assigning a backup policy\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. On IBM Fusion UI, select Backup & restore  Policies to add a backup policy, as shown in Figure 5-2. A Backup Policy specifies how frequently backups are taken, where backups are stored, and how long backups are retained.\n",
            "\n",
            "Figure 5-2 Adding a backup policy\n",
            "\n",
            "To list backup policies from the CLI, run the oc get backuppolicies command:\n",
            "\n",
            "$oc get backuppolicies-n ibm-spectrum-fusion-ns NAME PROVIDER BACKUPSTORAGELOCATION SCHEDULE RETENTION RETENTIONUNIT cpd-oper-policy isf-backup-restore ibm-cloud 00 1 * * * 30 days\n",
            "\n",
            "2. Assign the backup policy to Cloud Pak for Data applications. From the IBM Fusion UI, select Backed up applications, and then open the Protect apps menu and select the cluster where Cloud Pak for Data is deployed. Select the following applications:\n",
            "\n",
            "cpd-operator\n",
            "\n",
            "ibm-scheduler (if installed)\n",
            "\n",
            "Figure 5-3 Verifying that the recipes are now associated to the corresponding policy assignments\n",
            "\n",
            "3. Examine the policies that are assigned by running the command in Example 5-4.\n",
            "\n",
            "## Example 5-4 Checking the policies that are assigned\n",
            "\n",
            "$ oc get policyassignments.data-protection.isf.ibm.com-n ibm-spectrum-fusion-ns | grep cpd-oper\n",
            "\n",
            "NAME PROVIDER APPLICATION BACKUPPOLICY RECIPE RECIPENAMESPACE PHASE LASTBACKUPTIMESTAMP CAPACITY\n",
            "\n",
            "cpd-operator-cpd-oper-policy-apps isf-backup-restorecpd-operator cpd-oper-policyibmcpd-tenant cpd-operator\n",
            "\n",
            "Assigned <no value>\n",
            "\n",
            "The recipes are not yet associated to the policy assignment. The recipes must be manually patched into the policies except for cpd-tenant, which is assigned automatically. If cpd-scheduler is assigned, the recipe must be patched. Example 5-5 shows patching cpd-scheduler.\n",
            "\n",
            "## Example 5-5 Patching the recipes into the policy assignment\n",
            "\n",
            "$ oc-n ibm-spectrum-fusion-ns patch policyassignment <cpd-scheduler-policy-assignment>--type merge-p '{ 'spec ':{ 'recipe ':{ 'name ': 'ibmcpd-scheduler ', 'namespace ': 'cpd-scheduler ', 'apiVersion ': 'spp-data-protection.isf.ibm.com/v1alpha1 '}}}'\n",
            "\n",
            "4. Check the policy assignments again. The recipes are now attached to the backup policy assignment in Example 5-6.\n",
            "************************* Chunk **********************************\n",
            "*************************** 1591 *********************************\n",
            "## Example 5-6 Checking the policy assignments again\n",
            "\n",
            "| $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper | $ oc get policyassignments.data-protection.isf.ibm.com -n ibm-spectrum-fusion-ns | grep cpd-oper |\n",
            "|-|-|-|-|-|-|\n",
            "| NAME | | Provider | APPLICATION | BACKUPPOLICY | RECIPE |\n",
            "| RECIPENAMESPACE | PHASE | LASTBACKUPTIMESTAMP | CAPAC | | |\n",
            "| cpd-operator-cpd-oper-policy-apps | cpd-operator-cpd-oper-policy-apps | isf-backup-restore | cpd-operator | cpd-oper-policy | cpd-oper-policy |\n",
            "| ibmcpd-tenant | cpd-operator | Assigned | <no value> | <no value> | <no value> |\n",
            "\n",
            "## 5.3 Backing up the source cluster\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. Take a backup on the hub by selecting Backed up applications  cpd-operator. Then, select Backup now  Backup.\n",
            "\n",
            "Important: The first backup of cpd-operator prepares follow-on backups to be valid for restore. Do not restore from the first backup. After the first backup is complete, take a second backup. The second backup, and all later backups, may be used for the restore.\n",
            "\n",
            "2. After all the backups are finished, go to the Backed up applications page to confirm that the backups finished successfully.\n",
            "************************* Chunk **********************************\n",
            "*************************** 5291 *********************************\n",
            "## 5.4 Restoring to an alternative cluster\n",
            "\n",
            "Before restoring to an alternative cluster, ensure that the target cluster is prepared for Cloud Pak for Data and watsonx.data installation. For more information, see Preparing your cluster.\n",
            "\n",
            "Complete the following steps:\n",
            "\n",
            "1. When the alternative cluster is ready, edit guardian-configmap in the ibm-backup-restore project by increasing the restoreDatamoverTimeout parameter value to 240 minutes.\n",
            "\n",
            "2. Then, select Backup & restore  Topology and verify that the hub and spoke are connected and in a healthy state.\n",
            "\n",
            "The next step is to install the certificate manager and the IBM License Service. For more information, see Installing shared cluster components for IBM Cloud Pak for Data.\n",
            "\n",
            "3. Change the logging level from default INFO to DEBUG by running the following command:\n",
            "\n",
            "oc patch cm-n ibm-backup-restore guardian-configmap-p='{ 'data ':{ 'logLevel ': 'DEBUG '}}'\n",
            "\n",
            "4. Next, restore cpd-operator by selecting Backed up applications  cpd-operator  Restore.\n",
            "\n",
            "5. Under Select a destination, click Choose a different cluster to restore the application in and then select the target cluster. Click Next.\n",
            "\n",
            "6. In the next window, select the backup that you want to use and then click Next.\n",
            "\n",
            "7. After the job is finished, verify the completion details by going to the Jobs section under Backup & restore.\n",
            "\n",
            "8. Repeat these steps for 'ibm-scheduler' (if installed). After the restore is finished for 'cpd-operator', and 'ibm-scheduler', confirm that the restore was successful by logging in to the Cloud Pak for Data user interface. Inspect the watsonx.data instance and all the previous data under the Instances page, as shown in Figure 5-4.\n",
            "\n",
            "Figure 5-4 Verification of the restored watsonx.data instance in the Cloud Pak for Data user interface\n",
            "\n",
            "Figure 5-5 shows that the watsonx.data test engine was restored successfully.\n",
            "\n",
            "Figure 5-5 The watsonx.data test engine was restored successfully\n",
            "\n",
            "Figure 5-6 shows the data that is restored from the 'Data manager' view.\n",
            "\n",
            "Figure 5-6 Data was restored from the 'Data manager' view\n",
            "\n",
            "9. Verify the restore by running oc get pods-n cpd-instance and ensuring that all pods are in a good state, as shown in Example 5-7.\n",
            "\n",
            "Example 5-7 Verifying the restore by running oc get pods-n cpd-instance\n",
            "\n",
            "| $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE | $ oc get pods -n cpd-instance READY STATUS RESTARTS AGE |\n",
            "|-|-|-|-|-|-|\n",
            "| NAME | | | | | |\n",
            "| create-secrets-job-nwt8q | | 0/1 Completed 0 | | | 8m35s |\n",
            "| ibm-lh-lakehouse-hive-metastore-696f8fb6dd-8ss85 1/1 Running 3 (16m ago) | | | | | 21m |\n",
            "| ibm-lh-lakehouse-minio-ff8f7b77f-h4n5r | | 1/1 Running 0 | | | 21m |\n",
            "| ibm-lh-lakehouse-presto-01-presto-0 | | 1/1 Running 2 (16m ago) | | | 21m |\n",
            "| ibm-lh-lakehouse-presto543-presto-0 | | 1/1 Running 2 (16m ago) | | | 21m |\n",
            "| ibm-lh-postgres-edb-2 | | 1/1 Running 0 | | | 26m |\n",
            "| ibm-lh-postgres-edb-3 | | 1/1 Running 0 | | | 25m |\n",
            "| ibm-lh-postgres-edb-4 | | 1/1 Running 0 | | | 24m |\n",
            "| ibm-lh-postgres-setup-job-8q6zf | | 0/1 Completed 0 | | | 10m |\n",
            "| ibm-nginx-6995f698fd-9s9vv | | 2/2 Running 0 | | | 19m |\n",
            "| ibm-nginx-6995f698fd-sgvq6 | | 2/2 Running 0 | | | 19m |\n",
            "| ibm-nginx-tester-55588dd7b-pnjvx | | 2/2 Running 0 | | | 21m |\n",
            "| lhconsole-api-85f77cc57d-k4wk9 | | 1/1 Running 5 (18m ago) | | | 21m |\n",
            "| lhconsole-api-85f77cc57d-vmzkv | | 1/1 Running 5 (18m ago) | | | 21m |\n",
            "| lhconsole-nodeclient-6bb7475775-7x4js | | 1/1 Running 0 | | | 21m |\n",
            "| lhconsole-ui-7c7dbb98d8-t8kpj | | 1/1 Running 0 | | | 21m |\n",
            "| usermgmt-6bf557c77c-2fsgg | | 1/1 Running 0 | | | 18m |\n",
            "| usermgmt-6bf557c77c-s5ql6 | | 1/1 Running 0 | | | 18m |\n",
            "| usermgmt-ensure-tables-job-6vpp2 | | 0/1 Completed 0 | | | 7m20s |\n",
            "| zen-audit-67944bcc74-v2445 | | 1/1 Running 0 | | | 21m |\n",
            "| zen-core-5f7786c596-bmr9n | | 2/2 Running 3 (18m ago) | | | 19m |\n",
            "| zen-core-5f7786c596-hzb8v | | 2/2 Running 3 (18m ago) | | | 19m |\n",
            "| zen-core-api-58f7f7664d-2thq8 | | 2/2 Running 0 | | | 19m |\n",
            "| zen-core-api-58f7f7664d-97fzz | | 2/2 Running 0 | | | 19m |\n",
            "| zen-core-create-tables-job-x7qb7 | | 0/1 Completed 0 | | | 6m50s |\n",
            "| zen-core-pre-requisite-job-qh5sg | | 0/1 Completed 0 | | | 5m7s |\n",
            "| zen-metastore-edb-2 | | 1/1 Running 0 | | | 26m |\n",
            "| zen-metastore-edb-3 | | 1/1 Running 0 | | | 25m |\n",
            "| zen-minio-0 | | 1/1 Running 0 | | | 21m |\n",
            "| zen-minio-1 | | 1/1 Running 0 | | | 21m |\n",
            "| zen-minio-2 | | 1/1 Running 0 | | | 21m |\n",
            "| zen-minio-create-buckets-job-ccmdv | | 0/1 Completed 0 | | | 8m44s |\n",
            "| zen-pre-requisite-job-lxghr | | 0/1 Completed 0 | | | 5m51s |\n",
            "| zen-validate-metastore-edb-connection-job-m979g 0/1 Completed 0 | | | | | 7m44s |\n",
            "| zen-watchdog-7f5dcd6789-zkcrl | | 1/1 Running 5 (13m ago) | | | 18m |\n",
            "| zen-watchdog-create-tables-job-kgz7q | | 0/1 Completed 0 | | | 6m36s 3m33s |\n",
            "| zen-watchdog-post-requisite-job-cgchv | | 0/1 Completed 0 | | | |\n",
            "| zen-watchdog-pre-requisite-job-x6pn5 | | 0/1 Completed 0 | | | 3m51s |\n",
            "\n",
            "10.Run oc get pvc-n cpd-instance to ensure that each pvc is in a good state and bound, as shown in Example 5-8.\n",
            "\n",
            "Example 5-8 Ensuring that each pvc is in a good state and bound\n",
            "************************* Chunk **********************************\n",
            "*************************** 2895 *********************************\n",
            "Example 5-8 Ensuring that each pvc is in a good state and bound\n",
            "\n",
            "| | $ oc get pvc -n cpd-instance | | STATUS VOLUME |\n",
            "|-|-|-|-|\n",
            "| NAME CAPACITY | ACCESS MODES STORAGECLASS | | AGE |\n",
            "| export-zen-minio-0 | export-zen-minio-0 Bound | | pvc-0b5e56d4-8dff-42fd-9dc9-bbf903365bfd |\n",
            "| 10Gi | RWO | | ibm-storage-fusion-cp-sc 21m |\n",
            "| export-zen-minio-1 | export-zen-minio-1 Bound | | pvc-3729cd73-6950-4fb3-ace6-d86389b50f5d |\n",
            "| 10Gi | RWO | | ibm-storage-fusion-cp-sc 21m |\n",
            "| export-zen-minio-2 | export-zen-minio-2 Bound | | pvc-b92377fb-a2d7-4973-983e-d04988fda54c |\n",
            "| 10Gi | RWO | | ibm-storage-fusion-cp-sc 21m |\n",
            "| ibm-lh-lakehouse-minio-pvc | ibm-lh-lakehouse-minio-pvc | | Bound pvc-608134b5-0c6c-41b9-9537-ff5e0878851d |\n",
            "| 488284Mi | RWO | | ibm-storage-fusion-cp-sc 43m |\n",
            "| ibm-lh-postgres-edb-2 | ibm-lh-postgres-edb-2 Bound | | pvc-adf13e39-14af-4b55-866f-27e3184a6157 |\n",
            "| 9540Mi | RWO | | ibm-storage-fusion-cp-sc 43m |\n",
            "| ibm-lh-postgres-edb-3 | Bound | | pvc-cf179bc5-38e0-49f9-bc16-b535d31b0f00 |\n",
            "| 9765625Ki | RWO | | ibm-storage-fusion-cp-sc 26m |\n",
            "| ibm-lh-postgres-edb-4 | ibm-lh-postgres-edb-4 Bound | | pvc-be3b1749-7f31-487b-b60e-aa4f733adb65 |\n",
            "| 9765625Ki | RWO | | ibm-storage-fusion-cp-sc 25m |\n",
            "| ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be | ibm-zen-objectstore-backup-pvc Bound pvc-d03edb68-67bb-4799-a386-5f3d223cd7be |\n",
            "| 20Gi | RWO | | ibm-storage-fusion-cp-sc 43m |\n",
            "| zen-metastore-edb-2 | | | Bound pvc-88f1dd8e-3534-4ee3-82b1-3ef2b5904f25 |\n",
            "| 10Gi | RWO | | ibm-storage-fusion-cp-sc 43m |\n",
            "| zen-metastore-edb-3 | | | Bound pvc-7d9498fc-3c17-456d-b45d-90576b1d8b0d |\n",
            "| 10Gi | RWO | | ibm-storage-fusion-cp-sc 26m |\n",
            "\n",
            "11.Run oc get catalogsource-n cpd-operator and oc get pods-n cpd-operator to ensure that the Cloud Pak for Data operators are in a good state, as shown in Example 5-9.\n",
            "\n",
            "Example 5-9 Ensuring that the Cloud Pak for Data operator is in a good state\n",
            "\n",
            "| | $ oc get catalogsource -n cpd-operator |\n",
            "|-|-|\n",
            "| NAME | DISPLAY |\n",
            "| TYPE PUBLISHER | AGE |\n",
            "| cloud-native-postgresql-catalog | |\n",
            "| | ibm-cloud-native-postgresql-4.14.0+20230616.111503 |\n",
            "| grpc IBM | 39m |\n",
            "| ibm-watsonx-data-catalog | |\n",
            "| ibm-watsonx-data-1.0.2+20230816.142123.1192-linux-amd64 grpc IBM | |\n",
            "| IBM | opencloud-operators ibm-cp-common-services-4.1.0 |\n",
            "| grpc | 37m |\n",
            "| $ oc get pods -n cpd-operator | |\n",
            "| NAME READY STATUS | |\n",
            "| 28347d5b35b4a7e67ebbadc34bae6a27cf624ee1ec0388b16aa215aa76mjpbk 0/1 | |\n",
            "| Completed | 0 37m |\n",
            "| 457c18b305fb59f54375ecc9faa6f530db9ca6c2f2adbf6a4c2d831673shjq5 0/1 | 36m |\n",
            "| Completed | 0 |\n",
            "| 82a84c3f4c0679f4c09e1261e671982f2405935dd8a66d50738940740dclj48 0/1 | 31m |\n",
            "| 8ee3408bcdd092e94f0be116278a44a1007c5fa49b13bcdbfbc1c6f002r7p57 0/1 | |\n",
            "| | 36m |\n",
            "| Completed | 0 |\n",
            "************************* Chunk **********************************\n",
            "*************************** 3713 *********************************\n",
            "| 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k | 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k | 94368157bcda899a2502d5cdf67291342961a91e61937aa778e80dc348skh7k | 0/1 |\n",
            "|-|-|-|-|\n",
            "| Completed 0 | | 35m | |\n",
            "| b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc | b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc | b41a5640f98ac37869ac16c0eccdee0b225cc565114472ab5a50df351ergbsc | 0/1 |\n",
            "| Completed 0 | Completed 0 | 37m | |\n",
            "| cloud-native-postgresql-catalog-nqjn4 1/1 Running | cloud-native-postgresql-catalog-nqjn4 1/1 Running | cloud-native-postgresql-catalog-nqjn4 1/1 Running | |\n",
            "| 0 | | 40m | |\n",
            "| cpd-platform-hp4w6 | cpd-platform-hp4w6 | cpd-platform-hp4w6 | 1/1 Running |\n",
            "| | | 0 39m | |\n",
            "| cpd-platform-operator-manager-6bc68dc8d-7xbjz | cpd-platform-operator-manager-6bc68dc8d-7xbjz | cpd-platform-operator-manager-6bc68dc8d-7xbjz | 1/1 Running |\n",
            "| 0 | | 15m | |\n",
            "| cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running | cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running | cpdbr-tenant-service-6dcc49464c-zph7h 1/1 Running | |\n",
            "| | | 0 41m | |\n",
            "| create-postgres-license-config-m6ljg | create-postgres-license-config-m6ljg | create-postgres-license-config-m6ljg | 0/1 |\n",
            "| Completed 0 | | 32m | |\n",
            "| | | | 0/1 |\n",
            "| create-postgres-license-config-xdc4s Completed 0 30m | create-postgres-license-config-xdc4s Completed 0 30m | create-postgres-license-config-xdc4s Completed 0 30m | |\n",
            "| e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk | e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk | e71db3df91177a0feccb558c266c053c60f349f28459e9ae7c2c55f685vzlzk | 0/1 |\n",
            "| Completed | Completed | | |\n",
            "| 0 33m ibm-common-service-operator-5f688bffdb-jzppd | 0 33m ibm-common-service-operator-5f688bffdb-jzppd | 0 33m ibm-common-service-operator-5f688bffdb-jzppd | 1/1 Running |\n",
            "| | | 0 15m | |\n",
            "| | ibm-lakehouse-controller-manager-6c54bdbb6f-c6stq | ibm-lakehouse-controller-manager-6c54bdbb6f-c6stq | 1/1 Running |\n",
            "| | | 0 15m | |\n",
            "| | ibm-namespace-scope-operator-66f4878bff-w9bt5 | ibm-namespace-scope-operator-66f4878bff-w9bt5 | 1/1 Running |\n",
            "| | | 0 37m | |\n",
            "| ibm-watsonx-data-catalog-fvl6d | ibm-watsonx-data-catalog-fvl6d | ibm-watsonx-data-catalog-fvl6d | 1/1 Running |\n",
            "| | | 0 38m | |\n",
            "| | ibm-zen-operator-5646fffdf6-bb95f | ibm-zen-operator-5646fffdf6-bb95f | 1/1 Running |\n",
            "| 0 | | 15m | |\n",
            "| meta-api-deploy-7bcbf6c896-nkwl7 | meta-api-deploy-7bcbf6c896-nkwl7 | meta-api-deploy-7bcbf6c896-nkwl7 | 1/1 Running |\n",
            "| | | 0 30m | |\n",
            "| opencloud-operators-8nw56 | opencloud-operators-8nw56 | opencloud-operators-8nw56 | 1/1 Running |\n",
            "| | | 0 37m | |\n",
            "| operand-deployment-lifecycle-manager-5f94f78-9rwln | operand-deployment-lifecycle-manager-5f94f78-9rwln | operand-deployment-lifecycle-manager-5f94f78-9rwln | 1/1 Running |\n",
            "| 0 | | 15m | |\n",
            "| postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5 | postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5 | postgresql-operator-controller-manager-1-18-5-6cb46bfd94-4v7m5 | 1/1 Running |\n",
            "| | | 0 15m | 0/1 |\n",
            "| pre-zen-operand-config-job-lbj9l | pre-zen-operand-config-job-lbj9l | pre-zen-operand-config-job-lbj9l | |\n",
            "| Completed 0 | | 29m | |\n",
            "| pre-zen-operand-config-job-n6xzr 0 | pre-zen-operand-config-job-n6xzr 0 | pre-zen-operand-config-job-n6xzr 0 | 0/1 |\n",
            "| Completed setup-job-ft4lg | | 30m | 0/1 |\n",
            "| Completed 0 15m | Completed 0 15m | Completed 0 15m | |\n",
            "\n",
            "To access the catalogs in IBM watsonx.data, the IBM watsonx.data service must be restarted by running the following command:\n",
            "\n",
            "oc rollout restart sts,deploy-l 'component in (ibm-lh-presto-coordinator,ibm-lh-presto,ibm-lh-hive-metastore)'-n cpd-instance\n",
            "\n",
            "The command and its output are shown in Example 5-10.\n",
            "************************* Chunk **********************************\n",
            "*************************** 2882 *********************************\n",
            "## Example 5-10 Restarting IBM watsonx.data service\n",
            "\n",
            "$ oc rollout restart sts,deploy-l 'component in (ibm-lh-presto-coordinator,ibm-lh-presto,ibm-lh-hive-metastore)'-n cpd-instance Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto-coordinator ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto-coordinator restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-presto-01-presto-worker ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') statefulset.apps/ibm-lh-lakehouse-presto-01-presto-worker restarted Warning: would violate PodSecurity 'restricted:v1.24 ': seccompProfile (pod or container 'ibm-lh-lakehouse-hive-metastore ' must set securityContext.seccompProfile.type to 'RuntimeDefault ' or 'Localhost ') deployment.apps/ibm-lh-lakehouse-hive-metastore restarted\n",
            "\n",
            "After the command finishes, verify that the watsonx.data service was successfully restarted by running the following command:\n",
            "\n",
            "oc get deploy,sts-n cpd-instance\n",
            "\n",
            "## Related publications\n",
            "\n",
            "The publications listed in this section are considered particularly suitable for a more detailed discussion of the topics covered in this paper.\n",
            "\n",
            "## IBM Redbooks\n",
            "\n",
            "The following IBM Redbooks publications provide additional information about the topic in this document. Note that some publications referenced in this list might be available in softcopy only. For the current online list of Fusion Redbooks select here.\n",
            "\n",
            "IBM Storage Fusion HCI System: Metro Sync Disaster Recovery Use Case, REDP-5708\n",
            "\n",
            "IBM Storage Fusion Multicloud Object Gateway, REDP-5718\n",
            "\n",
            "IBM Storage Fusion Product Guide, REDP-5688\n",
            "\n",
            "You can search for, view, download or order these documents and other Redbooks, Redpapers, web docs, drafts, and additional materials, at the following website:\n",
            "\n",
            "ibm.com /redbooks\n",
            "\n",
            "## Other publications\n",
            "\n",
            "These publications are also relevant as further information sources:\n",
            "\n",
            "## Online resources\n",
            "\n",
            "IBM Documentation for IBM Fusion 2.7.x https://www.ibm.com/docs/en/sfhs/2.7.x\n",
            "\n",
            "IBM Documentation for IBM watsonx.data https://cloud.ibm.com/docs/watsonxdata\n",
            "\n",
            "IBM Fusion\n",
            "\n",
            "https://www.ibm.com/products/storage-fusion\n",
            "\n",
            "IBM watsonx.data https://www.ibm.com/products/watsonx-data\n",
            "\n",
            "IBM watsonx.data together with IBM Storage Fusion HCI System (video)\n",
            "\n",
            "## Help from IBM\n",
            "\n",
            "IBM Support and downloads ibm.com /support IBM Global Services ibm.com /services\n",
            "\n",
            "Back cover\n",
            "\n",
            "REDP-5720-00\n",
            "\n",
            "ISBN 0738461458\n",
            "\n",
            "Printed in U.S.A.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of Supported Split Code Language by Langchain"
      ],
      "metadata": {
        "id": "vRjmU1k2Ym0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full list of supported languages\n",
        "[e.value for e in Language]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o_Q8hBbYmgJ",
        "outputId": "3ebf90b3-4de8-4322-d268-e8233d953746"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cpp',\n",
              " 'go',\n",
              " 'java',\n",
              " 'kotlin',\n",
              " 'js',\n",
              " 'ts',\n",
              " 'php',\n",
              " 'proto',\n",
              " 'python',\n",
              " 'rst',\n",
              " 'ruby',\n",
              " 'rust',\n",
              " 'scala',\n",
              " 'swift',\n",
              " 'markdown',\n",
              " 'latex',\n",
              " 'html',\n",
              " 'sol',\n",
              " 'csharp',\n",
              " 'cobol',\n",
              " 'c',\n",
              " 'lua',\n",
              " 'perl',\n",
              " 'haskell']"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attempt to classify file type with Magika @Google."
      ],
      "metadata": {
        "id": "XbEZTfgkRDZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing through a markdown file\n",
        "from magika import Magika\n",
        "\n",
        "document = \"\"\n",
        "with open(\"/content/redbook-example.md\", 'r', encoding='utf-8') as file:\n",
        "    document = file.read()\n",
        "documents = [document]\n",
        "\n",
        "m = Magika()\n",
        "document_bytes = documents[0].encode('utf-8')\n",
        "res = m.identify_bytes(document_bytes)\n",
        "print(type(res.output.ct_label))\n",
        "print(res.output.ct_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP6XDJppEk0X",
        "outputId": "c4355f17-5157-476d-bca4-3293935e2fc6"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.str_'>\n",
            "markdown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing through a markdown file\n",
        "from magika import Magika\n",
        "\n",
        "document = \"\"\n",
        "with open(\"/content/gistfile1.js\", 'r', encoding='utf-8') as file:\n",
        "    document = file.read()\n",
        "documents = [document]\n",
        "\n",
        "m = Magika()\n",
        "document_bytes = documents[0].encode('utf-8')\n",
        "res = m.identify_bytes(document_bytes)\n",
        "print(res.output.ct_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAnx5DJiYSu9",
        "outputId": "f941a681-c6c3-46be-a10a-859b6ecae6aa"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "javascript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Noteable improves on:\n",
        "1. Table contents doesn't get cut in half\n",
        "2. Section headers, especially H1, H2 does not get cut in half.\n",
        "3. Used regex to trim unnecessary white spaces and"
      ],
      "metadata": {
        "id": "oZ66KCAhUAaC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKf5v0S6mLvU"
      },
      "source": [
        "# Markdown Chunking using LLMSherpa\n",
        "\n",
        "Using LLMSherpa, a library for reading and processing documents, we can easily convert PDF content into Markdown and then divide it into chunks. LLMSherpa's LayoutPDFReader class helps extract text from PDFs accurately, and with the help of a tokenizer, we can create consistent chunk sizes. This process makes the text easier to handle for further analysis or natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfDGsHCFmLvU"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from pathlib import Path\n",
        "import traceback\n",
        "from llmsherpa.readers import LayoutPDFReader\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIWjT2C_mLvU"
      },
      "source": [
        "We use a tokenizer to break down the Markdown content into smaller tokenized chunks. This helps in creating manageable sections of text that are suitable for further processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "0f61f0e190514277bcf3b9d8b8f09c5b",
            "a005c31486be42cfb3bcf8bdac53a9c6",
            "fd4bd72750cd4103989dc3ba5ad6c2a8",
            "0d109c1b492f47b9b532fdb7badd5832",
            "08c3a676fdc147258f306b9902ed3ebc",
            "72aa167957bb447fb6fbc50b54aecda4",
            "4157f6f716294b158872c658e2e0f2dc",
            "e7f9bcef67d349e3986db0910d5293ba",
            "fcf959fd1e524fd0928d1b8893ed71d5",
            "7f12c1c59e6d4617938d6b0cd47da99a",
            "79a17b0cd1db43c790626487051c6c67",
            "313e787c171942689731eac23768a2ba",
            "59a3e4cdd0d44a8fb2aadb0c9c2e43ed",
            "7ba9e5a165d54e20abbcc157adedd495",
            "c2e9e8774f1346f28b10f026cc39e15f",
            "e5dec1063e5f44c892e6814d48b77946",
            "08bcd461974b493b94bfe4d82e72e539",
            "40f4018089aa4678a72f9976a2212212",
            "3935e000d1df43a29cd514ec1f5404d5",
            "ce011a65916f4dbb83711519e0ece972",
            "b4c02b361c7545b1a832faeb39097cf2",
            "1958cb45152f4bdda09c8b228075e770",
            "43452a5b546f49b7882168107bd4a631",
            "297c11341a594c46891d7fa0ca8a280e",
            "5e5f3d3a8d0b4bff84e0a0a962284a34",
            "e3efbd509e3e4c6998821a946440430d",
            "242f3948b71e4825855673eae33fe54f",
            "a71cdc8c27e747649c784cba75dd04d9",
            "ace1ae9557f7498486df4fcd42e3420e",
            "3e97ef8ac7544f0bb7684d5cd0ac7f3e",
            "29f59f0b04b1453baee3ecd634dd8dfd",
            "d3da8191a0454306b55d1a5104dc1d5c",
            "ba92ad5dacc2436b8f31d54a4e220366",
            "1aaa7d12d9ca4ed2a36107c1eef1ee0c",
            "d1449aabf0b6424188d2a1a6d4fc3d8e",
            "d76ef232b8c14832a760ddc7b07cebff",
            "85eba9e0f928493586aac03bbb44368f",
            "6b0187404e5144f5b6f39d79c7c18d3e",
            "7168b03604714e74b3f47854348bf23f",
            "0cc0ab97ee024d91aa6b8d761304171e",
            "34fff9bf45f3434c92461014974f563e",
            "a5aff11c95464288adba13b86ae953dc",
            "6f0b11603ed74b39baabb12acef6bc10",
            "3e279db60c1740d98b04da8fdf5fb67b",
            "e41404c55c9d4ed8a5d20ad56fb3eb69",
            "f57a7f93495e4728a84ae92573a88d60",
            "f5ac5cf1775b4491baece42023d5670c",
            "57fbfbd25b414c10893e83fc974748ea",
            "a702c28050c445ec80243475c5d8f2db",
            "eca7dc23a12b4a7298138edb3d40603f",
            "265e0225efdb4510a1ff393be90b8246",
            "9d5848a584bf472a82ea28aafc302e42",
            "e859ab73b645492ab2b0b9ef6864979e",
            "4403b596aac04215b8d5a268d74a6ee0",
            "ba4e773ace7d4406a232d3cb95a96117"
          ]
        },
        "id": "m7bb27TnmLvU",
        "outputId": "75767cd8-b775-425d-9535-fbb447876590"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f61f0e190514277bcf3b9d8b8f09c5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "313e787c171942689731eac23768a2ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43452a5b546f49b7882168107bd4a631"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1aaa7d12d9ca4ed2a36107c1eef1ee0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e41404c55c9d4ed8a5d20ad56fb3eb69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"instructlab/granite-7b-lab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErsU_2sgmLvU"
      },
      "source": [
        "This function chunk_markdown takes a document and breaks it into smaller chunks of text based on the number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "393kCRKnmLvU"
      },
      "outputs": [],
      "source": [
        "# Function to chunk Markdown content\n",
        "def chunk_markdown(doc, max_threshold_tokens=3000, min_threshold_tokens=300):\n",
        "    tokens = tokenizer.encode(doc)\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    if num_tokens < min_threshold_tokens:\n",
        "        return Dataset.from_dict({'input': [], 'num_tokens': []})\n",
        "\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < num_tokens:\n",
        "        threshold = np.random.randint(min_threshold_tokens, max_threshold_tokens)\n",
        "        end = min(i + threshold, num_tokens)\n",
        "        chunk = tokens[i:end]\n",
        "        chunks.append({'input': tokenizer.decode(chunk), 'metadata': json.dumps({'num_tokens': len(chunk)})})\n",
        "        i += threshold\n",
        "\n",
        "    return Dataset.from_list(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I527caxmLvU"
      },
      "outputs": [],
      "source": [
        "# pdf file path\n",
        "file_path = './pdfs/redbook-example.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FJtK96nmLvU"
      },
      "outputs": [],
      "source": [
        "# Initialize llmsherpa LayoutPDFReader, available in github https://github.com/nlmatics/llmsherpa\n",
        "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
        "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
        "\n",
        "# Read the PDF file\n",
        "doc = pdf_reader.read_pdf(file_path)\n",
        "\n",
        "# Convert PDF to text and then to Markdown\n",
        "markdown_content = doc.to_text()\n",
        "\n",
        "# Chunk the Markdown content\n",
        "markdown_chunks = chunk_markdown(markdown_content, min_threshold_tokens=300, max_threshold_tokens=3000)\n",
        "\n",
        "# Print the number of chunks\n",
        "print(f\"Number of chunks: {len(markdown_chunks)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KCVQin1mLvV"
      },
      "outputs": [],
      "source": [
        "# Print the Markdown chunks\n",
        "for doc in markdown_chunks:\n",
        "    print(\"------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print(doc['input'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6saQkqjDpcO"
      },
      "source": [
        "# Document Specific Chunking\n",
        "Basically a rule-based specific chunking method that applies different seperators to different kinds of contexts.\n",
        "\n",
        "Langchain.RecursiveCharacterTextSplitter supports different various kinds of splitters. Lists of document-specific splitting method they provide:\n",
        "* Split by HTML header\n",
        "* Split by HTML section\n",
        "* Split code\n",
        "* MarkdownHeaderTextSplitter\n",
        "* Recursively split JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sm9K-k_AJ37Z"
      },
      "source": [
        "## Code\n",
        "Below is demonstration on text splitting between code.\n",
        "```\n",
        "# You can also see the separators used for a given language\n",
        "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blP-2pOnGnG7"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import (\n",
        "    Language,\n",
        "    RecursiveCharacterTextSplitter,\n",
        ")\n",
        "\n",
        "print(RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9iof7P8GbFK"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/utils.py'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    PYTHON_CODE = file.read()\n",
        "\n",
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=1024, chunk_overlap=0\n",
        ")\n",
        "python_docs = python_splitter.create_documents([PYTHON_CODE])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWyrysPyHokk"
      },
      "source": [
        "### Output - Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJKrPj73Hp5g"
      },
      "outputs": [],
      "source": [
        "for doc in python_docs:\n",
        "    print(\"------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print(doc.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-sB0pnFHvWP"
      },
      "source": [
        "## HTML, Markdown, and JSON\n",
        "HTML, Markdown and JSON all represents similar traits where they could be chunked by character-level seperators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBQ7nl-nKbR2"
      },
      "source": [
        "## PDF\n",
        "For PDFs, there are two parts: texts in PDF and images. Potential chunking methods are:\n",
        "\n",
        "**Method 1 (Rule-based)**\n",
        "1. Use OCR models to extract texts from PDF (all pdf extractors use this underneeth)\n",
        "2. Filter out JSON/Markdown/readable texts with document-specific rules.\n",
        "3. Tweak these rules and construct chunks.\n",
        "\n",
        "**Method 2 (Multimodal Embedding)**\n",
        "1. Using multimodal models and embed texts, images, and everythig.\n",
        "2. Group by context similarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KET79ia7KFDn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cl8c4Q8bNJeD"
      },
      "outputs": [],
      "source": [
        "filename = \"SalesforceFinancial.pdf\"\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "    filename=filename,\n",
        "\n",
        "    # Unstructured Helpers\n",
        "    strategy=\"hi_res\",\n",
        "    infer_table_structure=True,\n",
        "    model_name=\"yolox\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzQWjQQFPYfr"
      },
      "outputs": [],
      "source": [
        "elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ge3KwzyPoAP"
      },
      "outputs": [],
      "source": [
        "print(type(elements[-5]))\n",
        "print(type(elements[-5].metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAovSVTgPaoH"
      },
      "outputs": [],
      "source": [
        "elements[-5].metadata.text_as_html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8lEnxbyw0h6"
      },
      "source": [
        "# Semantic Chunker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Li40yXb_22S"
      },
      "source": [
        "## Concepts\n",
        "Position-based chunking do not necessarily share the same contexts. Meaning and contexts of a chunk should be taken consideration to make our chunk. After applying embeddings,chunks with more similiar meanings/contexts should have smaller distances.\n",
        "\n",
        "Analogy: Grouping books not by shelf-size, instead, by \"genre\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT6BLHfBw2Ue"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\"/content/utils.py\"]).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95F_8IG4w4-E"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import Settings\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# This is setting global default, not recommended\n",
        "bge_small = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
        ")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"instructlab/granite-7b-lab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHrbHfU1WTf"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import (\n",
        "    SentenceSplitter,\n",
        "    SemanticSplitterNodeParser,\n",
        ")\n",
        "\n",
        "splitter = SemanticSplitterNodeParser(\n",
        "    buffer_size=1, breakpoint_percentile_threshold=90, embed_model=bge_small\n",
        ")\n",
        "\n",
        "# also baseline splitter\n",
        "base_splitter = SentenceSplitter(chunk_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YUFxy0c8ENX"
      },
      "outputs": [],
      "source": [
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# def base_splitter_wrapper(text):\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(\n",
        "#         separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
        "#         chunk_size=num_chars_from_tokens(no_tokens_per_doc),\n",
        "#         chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "#     )\n",
        "#     return base_splitter.split_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3f0rRKv5iP8"
      },
      "source": [
        "## Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3npXpKi5RZP"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "print(len(nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Loi7m_mx5gym"
      },
      "outputs": [],
      "source": [
        "for node in nodes:\n",
        "    print(len(node.get_content()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgD99Hjl77Zd"
      },
      "outputs": [],
      "source": [
        "print(nodes[5].get_content())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhvvhpdQ7iMg"
      },
      "outputs": [],
      "source": [
        "for node in nodes:\n",
        "    print(node.get_content())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekzoBHz57sv1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A93MiGpWRE_X"
      },
      "source": [
        "# Summary\n",
        "## Conclusions\n",
        "1. Using LLMSherpa to convert from PDF to markdown then chunking from markdown is the current method we use.\n",
        "2. Document Specific Chunking may be the best methods for our current scenarios.\n",
        "3. Semantic Chunking could be applied to natural language, not sure about code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H6V4sa8F4PO"
      },
      "source": [
        "# Split by Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-JI9DhFwxcz"
      },
      "source": [
        "# Semantic Double Merging Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqhzbP3quMm1"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import (\n",
        "    SemanticDoubleMergingSplitterNodeParser,\n",
        "    LanguageConfig,\n",
        ")\n",
        "from llama_index.core import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRKXZTx4uO3a"
      },
      "outputs": [],
      "source": [
        "with open('/content/pg_essay.txt', 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "    char_length = len(content)\n",
        "\n",
        "print(\"Number of characters in the file:\", char_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO1dIH0Vv7Qg"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"]).load_data()\n",
        "\n",
        "config = LanguageConfig(language=\"english\", spacy_model=\"en_core_web_md\")\n",
        "splitter = SemanticDoubleMergingSplitterNodeParser(\n",
        "    language_config=config,\n",
        "    initial_threshold=0.4,\n",
        "    appending_threshold=0.5,\n",
        "    merging_threshold=0.5,\n",
        "    max_chunk_size=5000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbFNrnkYwA3W"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nzHgHVAwc7G"
      },
      "outputs": [],
      "source": [
        "print(nodes[0].get_content())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7-JI9DhFwxcz"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f61f0e190514277bcf3b9d8b8f09c5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a005c31486be42cfb3bcf8bdac53a9c6",
              "IPY_MODEL_fd4bd72750cd4103989dc3ba5ad6c2a8",
              "IPY_MODEL_0d109c1b492f47b9b532fdb7badd5832"
            ],
            "layout": "IPY_MODEL_08c3a676fdc147258f306b9902ed3ebc"
          }
        },
        "a005c31486be42cfb3bcf8bdac53a9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72aa167957bb447fb6fbc50b54aecda4",
            "placeholder": "",
            "style": "IPY_MODEL_4157f6f716294b158872c658e2e0f2dc",
            "value": "tokenizer_config.json:100%"
          }
        },
        "fd4bd72750cd4103989dc3ba5ad6c2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7f9bcef67d349e3986db0910d5293ba",
            "max": 2268,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcf959fd1e524fd0928d1b8893ed71d5",
            "value": 2268
          }
        },
        "0d109c1b492f47b9b532fdb7badd5832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f12c1c59e6d4617938d6b0cd47da99a",
            "placeholder": "",
            "style": "IPY_MODEL_79a17b0cd1db43c790626487051c6c67",
            "value": "2.27k/2.27k[00:00&lt;00:00,191kB/s]"
          }
        },
        "08c3a676fdc147258f306b9902ed3ebc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72aa167957bb447fb6fbc50b54aecda4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4157f6f716294b158872c658e2e0f2dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7f9bcef67d349e3986db0910d5293ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf959fd1e524fd0928d1b8893ed71d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f12c1c59e6d4617938d6b0cd47da99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a17b0cd1db43c790626487051c6c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "313e787c171942689731eac23768a2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59a3e4cdd0d44a8fb2aadb0c9c2e43ed",
              "IPY_MODEL_7ba9e5a165d54e20abbcc157adedd495",
              "IPY_MODEL_c2e9e8774f1346f28b10f026cc39e15f"
            ],
            "layout": "IPY_MODEL_e5dec1063e5f44c892e6814d48b77946"
          }
        },
        "59a3e4cdd0d44a8fb2aadb0c9c2e43ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08bcd461974b493b94bfe4d82e72e539",
            "placeholder": "",
            "style": "IPY_MODEL_40f4018089aa4678a72f9976a2212212",
            "value": "tokenizer.model:100%"
          }
        },
        "7ba9e5a165d54e20abbcc157adedd495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3935e000d1df43a29cd514ec1f5404d5",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce011a65916f4dbb83711519e0ece972",
            "value": 499723
          }
        },
        "c2e9e8774f1346f28b10f026cc39e15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c02b361c7545b1a832faeb39097cf2",
            "placeholder": "",
            "style": "IPY_MODEL_1958cb45152f4bdda09c8b228075e770",
            "value": "500k/500k[00:00&lt;00:00,953kB/s]"
          }
        },
        "e5dec1063e5f44c892e6814d48b77946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08bcd461974b493b94bfe4d82e72e539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f4018089aa4678a72f9976a2212212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3935e000d1df43a29cd514ec1f5404d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce011a65916f4dbb83711519e0ece972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4c02b361c7545b1a832faeb39097cf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1958cb45152f4bdda09c8b228075e770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43452a5b546f49b7882168107bd4a631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_297c11341a594c46891d7fa0ca8a280e",
              "IPY_MODEL_5e5f3d3a8d0b4bff84e0a0a962284a34",
              "IPY_MODEL_e3efbd509e3e4c6998821a946440430d"
            ],
            "layout": "IPY_MODEL_242f3948b71e4825855673eae33fe54f"
          }
        },
        "297c11341a594c46891d7fa0ca8a280e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71cdc8c27e747649c784cba75dd04d9",
            "placeholder": "",
            "style": "IPY_MODEL_ace1ae9557f7498486df4fcd42e3420e",
            "value": "tokenizer.json:100%"
          }
        },
        "5e5f3d3a8d0b4bff84e0a0a962284a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e97ef8ac7544f0bb7684d5cd0ac7f3e",
            "max": 1843294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29f59f0b04b1453baee3ecd634dd8dfd",
            "value": 1843294
          }
        },
        "e3efbd509e3e4c6998821a946440430d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3da8191a0454306b55d1a5104dc1d5c",
            "placeholder": "",
            "style": "IPY_MODEL_ba92ad5dacc2436b8f31d54a4e220366",
            "value": "1.84M/1.84M[00:00&lt;00:00,2.16MB/s]"
          }
        },
        "242f3948b71e4825855673eae33fe54f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71cdc8c27e747649c784cba75dd04d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ace1ae9557f7498486df4fcd42e3420e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e97ef8ac7544f0bb7684d5cd0ac7f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f59f0b04b1453baee3ecd634dd8dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3da8191a0454306b55d1a5104dc1d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba92ad5dacc2436b8f31d54a4e220366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1aaa7d12d9ca4ed2a36107c1eef1ee0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1449aabf0b6424188d2a1a6d4fc3d8e",
              "IPY_MODEL_d76ef232b8c14832a760ddc7b07cebff",
              "IPY_MODEL_85eba9e0f928493586aac03bbb44368f"
            ],
            "layout": "IPY_MODEL_6b0187404e5144f5b6f39d79c7c18d3e"
          }
        },
        "d1449aabf0b6424188d2a1a6d4fc3d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7168b03604714e74b3f47854348bf23f",
            "placeholder": "",
            "style": "IPY_MODEL_0cc0ab97ee024d91aa6b8d761304171e",
            "value": "added_tokens.json:100%"
          }
        },
        "d76ef232b8c14832a760ddc7b07cebff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34fff9bf45f3434c92461014974f563e",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5aff11c95464288adba13b86ae953dc",
            "value": 119
          }
        },
        "85eba9e0f928493586aac03bbb44368f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f0b11603ed74b39baabb12acef6bc10",
            "placeholder": "",
            "style": "IPY_MODEL_3e279db60c1740d98b04da8fdf5fb67b",
            "value": "119/119[00:00&lt;00:00,9.09kB/s]"
          }
        },
        "6b0187404e5144f5b6f39d79c7c18d3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7168b03604714e74b3f47854348bf23f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc0ab97ee024d91aa6b8d761304171e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34fff9bf45f3434c92461014974f563e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5aff11c95464288adba13b86ae953dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f0b11603ed74b39baabb12acef6bc10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e279db60c1740d98b04da8fdf5fb67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e41404c55c9d4ed8a5d20ad56fb3eb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f57a7f93495e4728a84ae92573a88d60",
              "IPY_MODEL_f5ac5cf1775b4491baece42023d5670c",
              "IPY_MODEL_57fbfbd25b414c10893e83fc974748ea"
            ],
            "layout": "IPY_MODEL_a702c28050c445ec80243475c5d8f2db"
          }
        },
        "f57a7f93495e4728a84ae92573a88d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eca7dc23a12b4a7298138edb3d40603f",
            "placeholder": "",
            "style": "IPY_MODEL_265e0225efdb4510a1ff393be90b8246",
            "value": "special_tokens_map.json:100%"
          }
        },
        "f5ac5cf1775b4491baece42023d5670c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d5848a584bf472a82ea28aafc302e42",
            "max": 655,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e859ab73b645492ab2b0b9ef6864979e",
            "value": 655
          }
        },
        "57fbfbd25b414c10893e83fc974748ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4403b596aac04215b8d5a268d74a6ee0",
            "placeholder": "",
            "style": "IPY_MODEL_ba4e773ace7d4406a232d3cb95a96117",
            "value": "655/655[00:00&lt;00:00,54.2kB/s]"
          }
        },
        "a702c28050c445ec80243475c5d8f2db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca7dc23a12b4a7298138edb3d40603f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "265e0225efdb4510a1ff393be90b8246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d5848a584bf472a82ea28aafc302e42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e859ab73b645492ab2b0b9ef6864979e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4403b596aac04215b8d5a268d74a6ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba4e773ace7d4406a232d3cb95a96117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}