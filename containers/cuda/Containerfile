# SPDX-License-Identifier: Apache-2.0

FROM nvcr.io/nvidia/cuda:12.3.2-devel-ubi9
RUN dnf install -y python3.11 && dnf install -y openssh && dnf install -y git && dnf install -y python3-pip && dnf install -y make automake gcc gcc-c++
RUN ssh-keyscan github.com > ~/.ssh/known_hosts
WORKDIR /instructlab
RUN python3.11 -m ensurepip
RUN dnf install -y gcc
RUN rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm
RUN dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo && dnf repolist && dnf config-manager --set-enabled cuda-rhel9-x86_64 && dnf config-manager --set-enabled cuda && dnf config-manager --set-enabled epel && dnf update -y
RUN --mount=type=ssh,id=default python3.11 -m pip install --force-reinstall nvidia-cuda-nvcc-cu12 
RUN export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64" \
    && export CUDA_HOME=/usr/local/cuda \
    && export PATH="/usr/local/cuda/bin:$PATH" \
    && export XLA_TARGET=cuda120 \
    && export XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda
RUN --mount=type=ssh,id=default CMAKE_ARGS="-DLLAMA_CUBLAS=on" python3.11 -m pip install --force-reinstall --no-cache-dir llama-cpp-python 
RUN --mount=type=ssh,id=default python3.11 -m pip install git+https://github.com/instructlab/instructlab@stable
CMD ["/bin/bash"]

