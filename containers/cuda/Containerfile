# SPDX-License-Identifier: Apache-2.0
# CUDA container for InstructLab
# with flash-attn and BitsAndBytes packages

ARG CUDA_VERSION="12.4.1"

FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-devel-ubi9 AS builder
ARG PKG_CACHE=on
ARG PYTHON=python3.11

ENV PYTHON="${PYTHON}" \
    APP_ROOT="/opt/app-root"
ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_COMPILE=1 \
    PS1="(app-root) \w\$ " \
    PKG_CACHE=${PKG_CACHE} \
    VIRTUAL_ENV="${APP_ROOT}" \
    PATH="${APP_ROOT}/bin:${PATH}" \
    XLA_TARGET="cuda120" \
    XLA_FLAGS="--xla_gpu_cuda_data_dir=/usr/local/cuda"

RUN --mount=type=cache,sharing=locked,id=dnf-ubi9,target=/var/cache/dnf \
    dnf install -y --nodocs --setopt=keepcache=True \
        ${PYTHON} ${PYTHON}-devel ${PYTHON}-pip make gcc gcc-c++ git-core && \
        if [ "${PKG_CACHE}" == "off" ]; then dnf clean all; fi

RUN ${PYTHON} -m venv --upgrade-deps ${VIRTUAL_ENV} && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf && \
    chown -R 1001:0 ${VIRTUAL_ENV}
COPY --chown=1001:0 containers/sitecustomize.py ${VIRTUAL_ENV}/lib/${PYTHON}/site-packages/
COPY --chown=1001:0 containers/bin/debug-* ${VIRTUAL_ENV}/bin/

# -DLLAMA_NATIVE=off: work around a build problem with llama-cpp-python and gcc 11.
# flash-attn is compiled from source, bitsandbytes has a manylinux wheel
COPY requirements.txt /tmp
RUN --mount=type=cache,sharing=locked,id=pipcache,target=/root/.cache/pip,mode=775 \
    sed 's/\[.*\]//' /tmp/requirements.txt >/tmp/constraints.txt && \
    if [ "${PKG_CACHE}" == "off" ]; then \
        echo "pip cache off"; \
        export PIP_NO_CACHE_DIR=off; \
    else \
        echo "pip cache on"; \
        export PIP_NO_CACHE_DIR=; \
        export PIP_CACHE_DIR=/root/.cache/pip; \
        pip cache remove llama_cpp_python; \
        pip cache remove flash_attn; \
    fi && \
    ${VIRTUAL_ENV}/bin/pip install wheel && \
    CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major -DLLAMA_NATIVE=off" \
        FORCE_CMAKE=1 \
        ${VIRTUAL_ENV}/bin/pip install --no-binary llama_cpp_python -c /tmp/constraints.txt llama_cpp_python && \
    ${VIRTUAL_ENV}/bin/pip install -r /tmp/requirements.txt && \
    ${VIRTUAL_ENV}/bin/pip install --no-binary flash-attn -c /tmp/constraints.txt flash-attn && \
    ${VIRTUAL_ENV}/bin/pip install -c /tmp/constraints.txt bitsandbytes && \
    rm /tmp/constraints.txt && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf && \
    chown -R 1001:0 ${VIRTUAL_ENV}

COPY . /tmp/instructlab
RUN ${VIRTUAL_ENV}/bin/pip install --no-deps /tmp/instructlab && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf && \
    chown -R 1001:0 ${VIRTUAL_ENV}


FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-runtime-ubi9 AS final

# APP_ROOT, VIRTUAL_ENV, and user mimick ubi9/python-311 container
ARG PYTHON=python3.11
ENV PYTHON="${PYTHON}" \
    APP_ROOT="/opt/app-root"
ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_COMPILE=1 \
    PS1="(app-root) \w\$ " \
    VIRTUAL_ENV="${APP_ROOT}" \
    PATH="${APP_ROOT}/bin:${PATH}"

# include compiler and python devel for torch compile
RUN --mount=type=cache,sharing=locked,id=dnf-ubi9,target=/var/cache/dnf \
    export CUDA_DASHED_VERSION=$(echo ${CUDA_VERSION} | awk -F '.' '{ print $1"-"$2; }') && \
    dnf upgrade -y --nodocs --setopt=keepcache=True && \
    dnf install -y --nodocs --setopt=keepcache=True \
        ${PYTHON} ${PYTHON}-devel ${PYTHON}-pip git-core gcc \
        cuda-cupti-${CUDA_DASHED_VERSION} nvidia-driver-cuda-libs && \
    if [ "${PKG_CACHE}" == "off" ]; then dnf clean all; fi

COPY --from=builder ${VIRTUAL_ENV} ${VIRTUAL_ENV}

RUN mkdir -m775 ${VIRTUAL_ENV}/src && \
    chown -R 1001:0 ${VIRTUAL_ENV}/src && \
    useradd -u 1001 -g 0 -c "Default Application User" -d /opt/app-root/src -s /sbin/nologin default

ENV HOME="${VIRTUAL_ENV}/src"
WORKDIR "${HOME}"
VOLUME ["/opt/app-root/src"]
# reset NVIDIA's entry point
ENTRYPOINT []
CMD ["/bin/bash"]

LABEL com.github.instructlab.instructlab.target="cuda" \
      name="instructlab-cuda-${CUDA_VERSION}" \
      summary="PyTorch, llama.cpp, and InstructLab NVIDIA CUDA" \
      usage="podman run -it --device nvidia.com/gpu=0 --volume $HOME/.config/instructlab:/opt/app-root/src:z ..."
