# SPDX-License-Identifier: Apache-2.0
# CUDA container for InstructLab
# with flash-attn and BitsAndBytes packages

ARG CUDA_VERSION="12.4.1"

FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-devel-ubi9 AS builder

ARG PYTHON=python3.11

ENV PYTHON="${PYTHON}" \
    CUDA_HOME="/usr/local/cuda" \
    APP_ROOT="/opt/app-root"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${CUDA_HOME}/compat" \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_COMPILE=1 \
    PIP_CACHE_DIR=/root/.cache/pip \
    PS1="(app-root) \w\$ " \
    VIRTUAL_ENV="${APP_ROOT}" \
    PATH="${APP_ROOT}/bin:${CUDA_HOME}/bin:${PATH}" \
    XLA_TARGET="cuda120" \
    XLA_FLAGS="--xla_gpu_cuda_data_dir=${CUDA_HOME}"

RUN --mount=type=cache,sharing=locked,id=dnf-ubi9,target=/var/cache/dnf \
    dnf install -y --nodocs --setopt=keepcache=True \
        python3.11 python3.11-devel python3.11-pip make gcc gcc-c++ git-core

RUN python3.11 -m venv ${VIRTUAL_ENV} && \
    find ${VIRTUAL_ENV} -name __pycache__ | xargs rm -rf
COPY containers/sitecustomize.py ${VIRTUAL_ENV}/lib/${PYTHON}/site-packages
COPY requirements.txt /tmp

# -mno-avx: work around a build problem with llama-cpp-python and gcc.
# flash-attn is compiled from source, bitsandbytes has a manylinux wheel
RUN --mount=type=cache,sharing=locked,id=pipcache,target=${PIP_CACHE_DIR},mode=775 \
    sed 's/\[.*\]//' /tmp/requirements.txt >/tmp/constraints.txt && \
    ${VIRTUAL_ENV}/bin/pip install wheel && \
    ${VIRTUAL_ENV}/bin/pip cache remove llama_cpp_python && \
    CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major" \
        CFLAGS="-mno-avx" \
        FORCE_CMAKE=1 \
        ${VIRTUAL_ENV}/bin/pip install -c /tmp/constraints.txt llama-cpp-python && \
    ${VIRTUAL_ENV}/bin/pip install -r /tmp/requirements.txt && \
    ${VIRTUAL_ENV}/bin/pip cache remove flash_attn && \
    ${VIRTUAL_ENV}/bin/pip install -c /tmp/constraints.txt flash-attn bitsandbytes && \
    rm /tmp/constraints.txt

COPY . /tmp/instructlab
RUN ${VIRTUAL_ENV}/bin/pip install --no-deps /tmp/instructlab


FROM nvcr.io/nvidia/cuda:${CUDA_VERSION}-runtime-ubi9 AS final

# APP_ROOT, VIRTUAL_ENV, and user mimick ubi9/python-311 container
ARG PYTHON=python3.11
ENV PYTHON="${PYTHON}" \
    CUDA_HOME="/usr/local/cuda" \
    APP_ROOT="/opt/app-root"
ENV LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${CUDA_HOME}/compat" \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_COMPILE=1 \
    PS1="(app-root) \w\$ " \
    VIRTUAL_ENV="${APP_ROOT}" \
    PATH="${APP_ROOT}/bin:${CUDA_HOME}/bin:${PATH}" \
    XLA_TARGET="cuda120" \
    XLA_FLAGS="--xla_gpu_cuda_data_dir=${CUDA_HOME}"

# include compiler and python devel for torch compile
RUN --mount=type=cache,sharing=locked,id=dnf-ubi9,target=/var/cache/dnf \
    export CUDA_DASHED_VERSION=$(echo ${CUDA_VERSION} | awk -F '.' '{ print $1"-"$2; }') && \
    dnf upgrade -y --nodocs --setopt=keepcache=True && \
    dnf install -y --nodocs --setopt=keepcache=True \
        python3.11 python3.11-devel python3.11-pip git-core gcc \
        cuda-cupti-${CUDA_DASHED_VERSION} nvidia-driver-cuda-libs

COPY --from=builder ${VIRTUAL_ENV} ${VIRTUAL_ENV}

RUN mkdir -m755 ${VIRTUAL_ENV}/src

ENV HOME="${VIRTUAL_ENV}/src"
WORKDIR "${HOME}"
VOLUME ["/opt/app-root/src"]
# reset NVIDIA's entry point
ENTRYPOINT []
CMD ["/bin/bash"]

LABEL com.github.instructlab.instructlab.target="cuda" \
      name="instructlab-cuda-${CUDA_VERSION}" \
      summary="PyTorch, llama.cpp, and InstructLab NVIDIA CUDA" \
      usage="podman run -it --device nvidia.com/gpu=0 --volume $HOME/.config/instructlab:/opt/app-root/src:z ..."
