# SPDX-License-Identifier: Apache-2.0

torch<2.7.0
vllm<0.9.0

# flash-attn 2.8.0+ is broken for torch 2.6.x.
# See: https://github.com/Dao-AILab/flash-attention/issues/1717
flash-attn<2.8.0
